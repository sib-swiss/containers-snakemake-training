{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course website","text":""},{"location":"#teachers","title":"Teachers","text":"<ul> <li>Geert van Geest  </li> <li>Antonin Thi\u00e9baut  </li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Geert van Geest  </li> <li>Antonin Thi\u00e9baut  </li> <li>Patricia Palagi  </li> </ul>"},{"location":"#attribution","title":"Attribution","text":"<p>This course is partly inspired by the Carpentries Docker course, the official Snakemake tutorial and the Introduction to Snakemake workshop from SIB-Days 2022.</p>"},{"location":"#material","title":"Material","text":"<ul> <li>This website</li> <li>Google doc (through mail)</li> </ul>"},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>After this course, you will be able to:</p> <ul> <li>Understand the basic concepts and terminology associated with virtualization with containers</li> <li>Customise, store, manage and share containerised environments with Docker</li> <li>Use Apptainer to run containers on a shared computer environment (e.g. a HPC cluster)</li> <li>Understand the basic concepts and terminology associated with workflow management systems</li> <li>Create a computational workflow that uses containers and package managers with Snakemake</li> </ul>"},{"location":"#learning-outcomes-explained","title":"Learning outcomes explained","text":"<p>To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter (found at Course material) starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.</p>"},{"location":"#exercises","title":"Exercises","text":"<p>Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.</p>"},{"location":"#asking-questions","title":"Asking questions","text":"<p>During lectures, you are encouraged to raise your hand if you have questions.</p> <p>A main source of communication will be our slack channel. Ask background questions that interest you personally at #background. During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a.  This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option:</p> <p>The teachers will review the answers, and add/modify if necessary. If you are really stuck and need specific tutor support, write the teachers or helpers personally.</p> <p>To summarise:</p> <ul> <li>During lectures: raise hand</li> <li>Personal interest questions: #background on slack</li> <li>During exercises: raise hand/#q-and-a on slack</li> </ul>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1-containers","title":"Day 1 - Containers","text":"Block Start End subject Block 1 9:15 AM 10:45 AM Introduction to containers 10:45 AM 11:15 AM BREAK Block 2 11:15 AM 12:45 PM Managing containers and images 12:45 PM 1:45 PM BREAK Block 3 1:45 PM 3:15 PM Working with <code>dockerfiles</code> 3:15 PM 3:45 PM BREAK Block 4 3:45 PM 5:15 PM Running containers with apptainer"},{"location":"course_schedule/#day-2-snakemake","title":"Day 2 - Snakemake","text":"Block Start End subject Block 1 9:15 AM 10:45 AM Introduction to Snakemake 10:45 AM 11:15 AM BREAK Block 2 11:15 AM 12:45 PM Generalising a Snakemake workflow 12:45 PM 1:45 PM BREAK Block 3 1:45 PM 3:15 PM Decorating a Snakemake workflow 3:15 PM 3:45 PM BREAK Block 4 3:45 PM 5:00 PM Snakemake, package managers and containers 5:00 PM 5:15 PM Wrap-up &amp; Open Q&amp;A"},{"location":"precourse/","title":"Precourse preparations","text":""},{"location":"precourse/#unix","title":"UNIX","text":"<p>As is stated in the course prerequisites at the announcement web page. We expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here. If you don\u2019t have experience with UNIX command line, or if you are unsure whether you meet the prerequisites, follow our online UNIX tutorial.</p>"},{"location":"precourse/#software","title":"Software","text":"<p>Install Docker on your local computer and create an account on dockerhub. You can find instructions here. Note that you need admin rights to install and use Docker, and if you are installing Docker on Windows, you need a recent Windows version. You should also have a modern code editor installed. We support VScode.</p> If working on Windows <p>During the course exercises you will be mainly interacting with docker through the command line. Although windows powershell is suitable for that, it might cause some issues with bind mounting directories. Hence, it is easier to follow the exercises if you have a UNIX or \u2018UNIX-like\u2019 terminal. You can get one by using WSL2. With VScode, you can also add the WSL extension. Make sure you install the latest versions before installing docker.</p> If installing Docker is a problem <p>During the course, we can give only limited support for installation issues. If you do not manage to install Docker before the course, you can still do almost all the exercises on Play with Docker. A Docker login is required.</p> <p>In addition to your local computer, we will be working on an Amazon Web Services (AWS) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through <code>ssh</code> with a username, key and IP address. All participants will be granted access to a personal home directory. You will be guided through setting up an ssh connection with VScode during the course. If you are not using VScode and if you are not familiar with <code>ssh</code>, you can check the Heidelberg University tutorial for information on how to set it up and use it.</p>"},{"location":"course_material/day1/apptainer/","title":"Running containers with apptainer","text":""},{"location":"course_material/day1/apptainer/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Login to a remote machine with <code>ssh</code></li> <li>Use <code>apptainer pull</code> to convert an image from dockerhub to the \u2018apptainer image format\u2019 (<code>.sif</code>)</li> <li>Execute a apptainer container</li> <li>Explain the difference in default mounting behaviour between <code>docker</code> and <code>apptainer</code></li> <li>Use <code>apptainer shell</code> to generate an interactive shell inside a <code>.sif</code> image</li> <li>Search and use images with both <code>docker</code> and <code>apptainer</code> from bioconda</li> </ul>"},{"location":"course_material/day1/apptainer/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Apptainer documentation</li> <li>Apptainer hub</li> <li>An article on Docker vs Apptainer</li> <li>Using conda and containers with snakemake</li> </ul>"},{"location":"course_material/day1/apptainer/#exercises","title":"Exercises","text":""},{"location":"course_material/day1/apptainer/#login-to-remote","title":"Login to remote","text":"<p>If you are enrolled in the course, you have received an e-mail with an IP, username, private key and password. To do the Apptainer exercises we will login to a remote server. Below you can find instructions on how to login.</p> <p>VScode is a code editor that can be used to edit files and run commands locally, but also on a remote server. In this subchapter we will set up VScode to work remotely.</p> <p>If not working with VScode</p> <p>If you are not working with VScode, you can login to the remote server with the following command:</p> <pre><code>ssh -i key_username.pem\n</code></pre> <p>If you want to edit files directly on the server, you can mount a directory with <code>sshfs</code>. </p> <p>Required installations</p> <p>For this exercise it is easiest if you use VScode. In addition you would need to have followed the instructions to set up remote-ssh:</p> <ul> <li>OpenSSH compatible client. This is usually pre-installed on your OS. You can check whether the command <code>ssh</code> exists. </li> <li>The Remote-SSH extension. To install, open VSCode and click on the extensions icon (four squares) on the left side of the window. Search for <code>Remote-SSH</code> and click on <code>Install</code>.</li> </ul> Windowsmac OS/Linux <p>Open a PowerShell and <code>cd</code> to the directory where you have stored your private key. After that, move it to <code>~\\.ssh</code>:</p> <pre><code>mv .\\key_username.pem ~\\.ssh\n</code></pre> <p>Open a terminal, and <code>cd</code> to the directory where you have stored your private key. After that, change the file permissions of the key and move it to <code>~/.ssh</code>:</p> <pre><code>chmod 400 key_username.pem\nmv key_username.pem ~/.ssh\n</code></pre> <p>Open VScode and click on the green or blue button in the bottom left corner. Select <code>Connect to Host...</code>, and then on <code>Configure SSH Host...</code>. Specify a the location for the config file. Use the same directory as where your keys are stored (so <code>~/.ssh</code>). A skeleton config file will be provided. Edit it, so it looks like this (replace <code>username</code> with your username, and specify the correct IP at <code>HostName</code>):</p> WindowsMacOS/Linux <pre><code>Host sib_course_remote\n    User username\n    HostName 123.456.789.123\n    IdentityFile ~\\.ssh\\key_username.pem\n</code></pre> <pre><code>Host sib_course_remote\n    User username\n    HostName 123.456.789.123\n    IdentityFile ~/.ssh/key_username.pem\n</code></pre> <p>Save and close the config file. Now click again the green or blue button in the bottom left corner. Select <code>Connect to Host...</code>, and then on <code>sib_course_remote</code>. You will be asked which operating system is used on the remote. Specify \u2018Linux\u2019. </p>"},{"location":"course_material/day1/apptainer/#pulling-an-image","title":"Pulling an image","text":"<p>Apptainer can take several image formats (e.g. a <code>docker</code> image), and convert them into it\u2019s own <code>.sif</code> format. Unlike <code>docker</code> this image doesn\u2019t live in a local image cache, but it\u2019s stored as an actual file.</p> <p>Exercise: On the remote server, pull the docker image that has the adjusted default <code>CMD</code> that we have pushed to dockerhub in this exercise (<code>ubuntu-figlet-df:v3</code>) with <code>apptainer pull</code>. The syntax is:</p> <pre><code>apptainer pull docker://[USER NAME]/[IMAGE NAME]:[TAG]\n</code></pre> Answer <p><pre><code>apptainer pull docker://[USER NAME]/ubuntu-figlet:v3\n</code></pre> This will result in a file called <code>ubuntu-figlet_v3.sif</code></p> <p>Note</p> <p>If you weren\u2019t able to push the image in the previous exercises to your docker hub, you can use <code>geertvangeest</code> as username to pull the image.</p>"},{"location":"course_material/day1/apptainer/#executing-an-image","title":"Executing an image","text":"<p>These <code>.sif</code> files can be run as standalone executables:</p> <pre><code>./ubuntu-figlet_v3.sif\n</code></pre> <p>Note</p> <p>This is shorthand for:</p> <pre><code>apptainer run ubuntu-figlet_v3.sif\n</code></pre> <p>And you can overwrite the default command like this:</p> <pre><code>apptainer run [IMAGE NAME].sif [COMMAND]\n</code></pre> <p>Note</p> <p>In this case, you can also use</p> <pre><code>./[IMAGE NAME].sif [COMMAND]\n</code></pre> <p>However, most applications require <code>apptainer run</code>. Especially if you want to provide options like <code>--bind</code> (for mounting directories). </p> <p>Exercise: Run the <code>.sif</code> file without a command, and with a command that runs <code>figlet</code>. Do you get expected output? Do the same for the python or R image you\u2019ve created in the previous chapter.</p> <p>Entrypoint and apptainer</p> <p>The <code>daterange</code> image has an entrypoint set, and <code>apptainer run</code> does not overwrite it. In order to ignore both the entrypoint and cmd use <code>apptainer exec</code>.  </p> Answer <p>Running it without a command (<code>./ubuntu-figlet_v3.sif</code>) should give:</p> <p><pre><code>__  __         _                                                 _        _\n|  \\/  |_   _  (_)_ __ ___   __ _  __ _  ___  __      _____  _ __| | _____| |\n| |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| |\n| |  | | |_| | | | | | | | | (_| | (_| |  __/  \\ V  V / (_) | |  |   &lt;\\__ \\_|\n|_|  |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___|   \\_/\\_/ \\___/|_|  |_|\\_\\___(_)\n       |___/                     |___/\n</code></pre> Which is the default command that we changed in the <code>Dockerfile</code>.</p> <p>Running with a another <code>figlet</code> command:</p> <pre><code>./ubuntu-figlet_v3.sif figlet 'Something else'\n</code></pre> <p>Should give:</p> <pre><code>____                       _   _     _                    _\n/ ___|  ___  _ __ ___   ___| |_| |__ (_)_ __   __ _    ___| |___  ___\n\\___ \\ / _ \\| '_ ` _ \\ / _ \\ __| '_ \\| | '_ \\ / _` |  / _ \\ / __|/ _ \\\n___) | (_) | | | | | |  __/ |_| | | | | | | | (_| | |  __/ \\__ \\  __/\n|____/ \\___/|_| |_| |_|\\___|\\__|_| |_|_|_| |_|\\__, |  \\___|_|___/\\___|\n                                             |___/\n</code></pre> RPython <p>Pulling the <code>search_biomart_datasets</code> image:</p> <pre><code>apptainer pull docker://[USER NAME]/search_biomart_datasets:v1\n</code></pre> <p>Running it without command:</p> <pre><code>./search_biomart_datasets_v1.sif\n</code></pre> <p>Running with a command:</p> <pre><code>./search_biomart_datasets_v1.sif --pattern sapiens\n</code></pre> <p>To overwrite both entrypoint and the command:</p> <pre><code>apptainer exec search_biomart_datasets_v1.sif search_biomart_datasets.R --pattern \"(R|r)at\"\n</code></pre> <p>Pulling the <code>daterange.py</code> image:</p> <pre><code>apptainer pull docker://[USER NAME]/daterange:v1\n</code></pre> <p>Running it without command:</p> <pre><code>./daterange_v1.sif\n</code></pre> <p>Running with a command:</p> <pre><code>./daterange_v1.sif --date 20221005\n</code></pre> <p>To overwrite both entrypoint and the command:</p> <pre><code>apptainer exec daterange_v1.sif daterange.py --date 20221005\n</code></pre>"},{"location":"course_material/day1/apptainer/#mounting-with-apptainer","title":"Mounting with Apptainer","text":"<p>Apptainer is also different from Docker in the way it handles mounting. By default, Apptainer binds your home directory and a number of paths in the root directory to the container. This results in behaviour that is almost like if you are working on the directory structure of the host.  </p> <p>If your directory is not mounted by default</p> <p>It depends on the apptainer settings whether most directories are mounted by default to the container. If your directory is not mounted, you can do that with the <code>--bind</code> option of <code>apptainer exec</code>:</p> <pre><code>apptainer exec --bind /my/dir/to/mount/ [IMAGE NAME].sif [COMMAND]\n</code></pre> <p>Running the command <code>pwd</code> (full name of current working directory) will therefore result in a path on the host machine:</p> <pre><code>./ubuntu-figlet_v3.sif pwd\n</code></pre> <p>Exercise: Run the above command. What is the output? How would the output look like if you would run a similar command with Docker?</p> <p>Hint</p> <p>A similar Docker command would look like (run this on your local computer):</p> <pre><code>docker run --rm ubuntu-figlet:v3 pwd\n</code></pre> Answer <p>The output of <code>./ubuntu-figlet_v3.sif pwd</code> is the current directory on the host: i.e. <code>/home/username</code> if you have it in your home directory. The output of <code>docker run --rm ubuntu-figlet:v3 pwd</code> (on the local host) would be <code>/</code>, which is the default workdir (root directory) of the container. As we did not mount any host directory, this directory exists only within the container (i.e. separated from the host).</p>"},{"location":"course_material/day1/apptainer/#interactive-shell","title":"Interactive shell","text":"<p>If you want to debug or inspect an image, it can be helpful to have a shell inside the container. You can do that with <code>apptainer shell</code>:</p> <pre><code>apptainer shell ubuntu-figlet_v3.sif\n</code></pre> <p>Note</p> <p>To exit the shell type <code>exit</code>.</p> <p>Exercise: Can you run <code>figlet</code> inside this shell?</p> Answer <p>Yes: <pre><code>Apptainer&gt; figlet test\n _            _\n| |_ ___  ___| |_\n| __/ _ \\/ __| __|\n| ||  __/\\__ \\ |_\n \\__\\___||___/\\__|\n</code></pre></p> <p>During the lecture you have learned that apptainer takes over the user privileges of the user on the host. You can get user information with command like <code>whoami</code>, <code>id</code>, <code>groups</code> etc.</p> <p>Exercise: Run the <code>figlet</code> container interactively. Do you have the same user privileges as if you were on the host? How is that with <code>docker</code>?</p> Answer <p>A command like <code>whoami</code> will result in your username printed at stdout:</p> <pre><code>Apptainer&gt; whoami\nmyusername\nApptainer&gt; id\nuid=1030(myusername) gid=1031(myusername) groups=1031(myusername),1001(condausers)\nApptainer&gt; groups\nmyusername condausers\n</code></pre> <p>With apptainer, you have the same privileges inside the apptainer container as on the host. If you do this in the docker container (based on the same image), you\u2019ll get output like this:</p> <pre><code>root@a3d6e59dc19d:/# whoami\nroot\nroot@a3d6e59dc19d:/# groups\nroot\nroot@a3d6e59dc19d:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre>"},{"location":"course_material/day1/apptainer/#a-bioinformatics-example-extra","title":"A bioinformatics example (extra)","text":"<p>All bioconda packages also have a pre-built container. Have a look at the bioconda website, and search for <code>fastqc</code>. In the search results, click on the appropriate record (i.e. package \u2018fastqc\u2019). Now, scroll down and find the namespace for the latest fastqc image. </p> <p>Exercise: Check out the container image at quay.io, by following quay.io/biocontainers/fastqc. Choose a tag, and pull it with <code>apptainer pull</code>.</p> Answer <p>It\u2019s up to you which tag you choose. The tag with the latest version is <code>0.12.1--hdfd78af_0</code>.</p> <pre><code>apptainer pull docker://quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\n</code></pre> <p>Apptainer images at galaxy.org</p> <p>Most (if not all) biocontainer images are available as apptainer (singularity) image at https://depot.galaxyproject.org/singularity/. You can simply download them with <code>wget</code> or <code>curl</code>, e.g.:</p> <pre><code>wget https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0\n</code></pre> <p>Let\u2019s test the image. Download some sample reads first:</p> <pre><code>mkdir reads\ncd reads\nwget https://introduction-containers.s3.eu-central-1.amazonaws.com/ecoli_reads.tar.gz\ntar -xzvf ecoli_reads.tar.gz\nrm ecoli_reads.tar.gz\n</code></pre> <p>Now you can simply run the image as an executable preceding the commands you would like to run within the container. E.g. running <code>fastqc</code> would look like:</p> <pre><code>cd\n./fastqc_0.12.1--hdfd78af_0.sif fastqc ./reads/ecoli_*.fastq.gz\n</code></pre> <p>This will result in <code>html</code> files in the directory <code>./reads</code>. These are quality reports for the sequence reads. If you\u2019d like to view them, you can download them with <code>scp</code> or e.g. FileZilla, and view them with your local browser.</p>"},{"location":"course_material/day1/dockerfiles/","title":"Working with dockerfiles","text":""},{"location":"course_material/day1/dockerfiles/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Build an image based on a dockerfile</li> <li>Use the basic dockerfile syntax</li> <li>Change the default command of an image and validate the change</li> <li>Map ports to a container to display interactive content through a browser</li> </ul>"},{"location":"course_material/day1/dockerfiles/#material","title":"Material","text":"<ul> <li>Official <code>Dockerfile</code> reference</li> <li>Ten simple rules for writing dockerfiles</li> </ul>"},{"location":"course_material/day1/dockerfiles/#exercises","title":"Exercises","text":"<p>To make your images shareable and adjustable, it\u2019s good practice to work with a <code>Dockerfile</code>. This is a script with a set of instructions to build your image from an existing image.</p>"},{"location":"course_material/day1/dockerfiles/#basic-dockerfile","title":"Basic <code>Dockerfile</code>","text":"<p>You can generate an image from a <code>Dockerfile</code> using the command <code>docker build</code>. A <code>Dockerfile</code> has its own syntax for giving instructions. Luckily, they are rather simple. The script always contains a line starting with <code>FROM</code> that takes the image name from which the new image will be built. After that you usually want to run some commands to e.g. configure and/or install software. The instruction to run these commands during building starts with <code>RUN</code>.  In our <code>figlet</code> example that would be:</p> <pre><code>FROM ubuntu:jammy-20240427\nRUN apt-get update\nRUN apt-get install figlet\n</code></pre> <p>On writing reproducible <code>Dockerfiles</code></p> <p>At the <code>FROM</code> statement in the the above <code>Dockerfile</code> you see that we have added a specific tag to the image (i.e. <code>jammy-20240427</code>). We could also have written:</p> <pre><code>FROM ubuntu\nRUN apt-get update\nRUN apt-get install figlet\n</code></pre> <p>This will automatically pull the image with the tag <code>latest</code>. However, if the maintainer of the <code>ubuntu</code> images decides to tag another <code>ubuntu</code> version as <code>latest</code>, rebuilding with the above <code>Dockerfile</code> will not give you the same result. Therefore it\u2019s always good practice to add the (stable) tag to the image in a <code>Dockerfile</code>. More rules on making your <code>Dockerfiles</code> more reproducible here.</p> <p>Exercise: Create a file on your computer called <code>Dockerfile</code>, and paste the above instruction lines in that file. Make the directory containing the <code>Dockerfile</code> your current directory. Build a new image based on that <code>Dockerfile</code> with:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build .\n</code></pre> <pre><code>export DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build .\n</code></pre> <p>If using an Apple M chip (newer Macs)</p> <p>If you are using a computer with an Apple M chip, you have the less common ARM system architecture, which can limit transferability of images to (more common) <code>x86_64/AMD64</code> machines. When building images on a Mac with an M chip (especially if you have sharing in mind), it\u2019s best to set the <code>DOCKER_DEFAULT_PLATFORM</code> to <code>linux/amd64</code> with <code>export DOCKER_DEFAULT_PLATFORM=linux/amd64</code>. </p> <p>The argument of <code>docker build</code></p> <p>The command <code>docker build</code> takes a directory as input (providing <code>.</code> means the current directory). This directory should contain the <code>Dockerfile</code>, but it can also contain more of the build context, e.g. (python, R, shell) scripts that are required to build the image.</p> <p>What has happened? What is the name of the build image?</p> Answer <p>A new image was created based on the <code>Dockerfile</code>. You can check it with: <code>docker image ls</code>, which gives something like:</p> <pre><code>REPOSITORY                        TAG       IMAGE ID       CREATED             SIZE\n&lt;none&gt;                            &lt;none&gt;    92c980b09aad   7 seconds ago       101MB\nubuntu-figlet                     latest    e08b999c7978   About an hour ago   101MB\nubuntu                            latest    f63181f19b2f   30 hours ago        72.9MB\n</code></pre> <p>It has created an image without a name or tag. That\u2019s a bit inconvenient.</p> <p>Exercise: Build a new image with a specific name. You can do that with adding the option <code>-t</code> to <code>docker build</code>. Before that, remove the nameless image.</p> <p>Hint</p> <p>An image without a name is usually a \u201cdangling image\u201d. You can remove those with <code>docker image prune</code>.</p> Answer <p>Remove the nameless image with <code>docker image prune</code>.</p> <p>After that, rebuild an image with a name:</p> x86_64 / AMD64ARM (MacOS M1 chip) <pre><code>docker build -t ubuntu-figlet:v2 .\n</code></pre> <pre><code>DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build -t ubuntu-figlet:v2 .\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#using-cmd","title":"Using <code>CMD</code>","text":"<p>As you might remember the second positional argument of <code>docker run</code> is a command (i.e. <code>docker run IMAGE [CMD]</code>). If you leave it empty, it uses the default command. You can change the default command in the <code>Dockerfile</code> with an instruction starting with <code>CMD</code>. For example:</p> <pre><code>FROM ubuntu:jammy-20240427\nRUN apt-get update\nRUN apt-get install figlet\nCMD figlet My image works!\n</code></pre> <p>Exercise: Build a new image based on the above <code>Dockerfile</code>. Can you validate the change using <code>docker image inspect</code>? Can you overwrite this default with <code>docker run</code>?</p> Answer <p>Copy the new line to your <code>Dockerfile</code>, and build the new image like this:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t ubuntu-figlet:v3 .\n</code></pre> <pre><code>DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build ubuntu-figlet:v3 .\n</code></pre> <p>The command <code>docker inspect ubuntu-figlet:v3</code> will give:</p> <pre><code>\"Cmd\": [\n    \"/bin/sh\",\n    \"-c\",\n    \"figlet My image works!\"\n]\n</code></pre> <p>So the default command (<code>/bin/bash</code>) has changed to <code>figlet My image works!</code></p> <p>Running the image (with clean-up (<code>--rm</code>)):</p> <pre><code>docker run --rm ubuntu-figlet:v3\n</code></pre> <p>Will result in:</p> <pre><code>__  __         _                                                 _        _\n|  \\/  |_   _  (_)_ __ ___   __ _  __ _  ___  __      _____  _ __| | _____| |\n| |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| |\n| |  | | |_| | | | | | | | | (_| | (_| |  __/  \\ V  V / (_) | |  |   &lt;\\__ \\_|\n|_|  |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___|   \\_/\\_/ \\___/|_|  |_|\\_\\___(_)\n       |___/                     |___/\n</code></pre> <p>And of course you can overwrite the default command:</p> <pre><code>docker run --rm ubuntu-figlet:v3 figlet another text\n</code></pre> <p>Resulting in:</p> <pre><code>_   _                 _            _\n__ _ _ __   ___ | |_| |__   ___ _ __  | |_ _____  _| |_\n/ _` | '_ \\ / _ \\| __| '_ \\ / _ \\ '__| | __/ _ \\ \\/ / __|\n| (_| | | | | (_) | |_| | | |  __/ |    | ||  __/&gt;  &lt;| |_\n\\__,_|_| |_|\\___/ \\__|_| |_|\\___|_|     \\__\\___/_/\\_\\\\__|\n</code></pre> <p>Two flavours of <code>CMD</code></p> <p>You have seen in the output of <code>docker inspect</code> that docker translates the command (i.e. <code>figlet \"my image works!\"</code>) into this: <code>[\"/bin/sh\", \"-c\", \"figlet 'My image works!'\"]</code>. The notation we used in the <code>Dockerfile</code> is the shell notation while the notation with the square brackets (<code>[]</code>) is the exec-notation. You can use both notations in your <code>Dockerfile</code>. Altough the shell notation is more readable, the exec notation is directly used by the image, and therefore less ambiguous.</p> <p>A <code>Dockerfile</code> with shell notation:</p> <pre><code>FROM ubuntu:jammy-20240427\nRUN apt-get update\nRUN apt-get install figlet\nCMD figlet My image works!\n</code></pre> <p>A <code>Dockerfile</code> with exec notation:</p> <pre><code>FROM ubuntu:jammy-20240427\nRUN apt-get update\nRUN apt-get install figlet\nCMD [\"/bin/sh\", \"-c\", \"figlet My image works!\"]\n</code></pre> <p>Exercise: Now push our created image (with a version tag) to docker hub. We will use it later for the <code>apptainer</code> exercises.</p> Answer <pre><code>docker tag ubuntu-figlet:v3 [USER NAME]/ubuntu-figlet:v3\ndocker push [USER NAME]/ubuntu-figlet:v3\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#build-an-image-for-your-own-script","title":"Build an image for your own script","text":"<p>Often containers are built for a specific purpose. For example, you can use a container to ship all dependencies together with your developed set of scripts/programs. For that you will need to add your scripts to the container. That is quite easily done with the instruction <code>COPY</code>. However, in order to make your container more user-friendly, there are several additional instructions that can come in useful. We will treat the most frequently used ones below. Depending on your preference, either choose R or Python below. </p> RPython <p>In the exercises will use a simple script called <code>test_deseq2.R</code>. You can download it here, or copy-paste it:</p> test_deseq2.R<pre><code>#!/usr/bin/env Rscript\n\n# load packages required for this script\nwrite(\"Loading packages required for this script\", stderr())\nsuppressPackageStartupMessages({\n    library(DESeq2)\n    library(optparse)\n})\n\n# load dependency packages for testing installations\nwrite(\"Loading dependency packages for testing installations\", stderr())\nsuppressPackageStartupMessages({\n    library(apeglm)\n    library(IHW)\n    library(limma)\n    library(data.table)\n    library(ggplot2)\n    library(ggrepel)\n    library(pheatmap)\n    library(RColorBrewer)\n    library(scales)\n    library(stringr)\n})\n\n# parse options with optparse\noption_list &lt;- list(\n    make_option(c(\"--rows\"),\n        type = \"integer\",\n        help = \"Number of rows in dummy matrix [default = %default]\",\n        default = 100\n    )\n)\n\nopt_parser &lt;- OptionParser(\n    option_list = option_list,\n    description = \"Runs DESeq2 on dummy data\"\n)\nopt &lt;- parse_args(opt_parser)\n\n# create a random dummy count matrix\ncnts &lt;- matrix(rnbinom(n = opt$row * 10, mu = 100, size = 1 / 0.5), ncol = 10)\ncond &lt;- factor(rep(1:2, each = 5))\n\n# object construction\ndds &lt;- DESeqDataSetFromMatrix(cnts, DataFrame(cond), ~cond)\n\n# standard analysis\ndds &lt;- DESeq(dds)\nres &lt;- results(dds)\n\n# print results to stdout\nprint(res)\n</code></pre> <p>After you have downloaded it, make sure to set the permissions to executable:</p> <p><pre><code>chmod +x test_deseq2.R\n</code></pre> It is a relatively simple script that runs DESeq2 on a dummy dataset. An example for execution would be:</p> <pre><code>./test_deseq2.R --rows 75\n</code></pre> <p>Giving a list of results from DESeq2 on a dummy dataset with 75 rows.</p> <p>Here, <code>--rows</code> is a optional arguments that specifies the number of rows generated in the input count matrix. When running the script, it will return a bunch of messages and at the end an overview of differential gene expression analysis results:</p> <pre><code>    baseMean log2FoldChange     lfcSE         stat    pvalue      padj\n    &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\n1     66.1249       0.281757  0.727668     0.387206  0.698604  0.989804\n2     76.9682       0.305763  0.619209     0.493796  0.621451  0.989804\n3     64.7843      -0.694525  0.479445    -1.448603  0.147448  0.931561\n4    123.0252       0.631247  0.688564     0.916758  0.359269  0.931561\n5     93.2002      -0.453430  0.686043    -0.660936  0.508653  0.941951\n...       ...            ...       ...          ...       ...       ...\n96    64.0177    0.757585137  0.682683  1.109718054  0.267121  0.931561\n97   114.3689   -0.580010850  0.640313 -0.905823841  0.365029  0.931561\n98    79.9620    0.000100617  0.612442  0.000164288  0.999869  0.999869\n99    92.6614    0.563514308  0.716109  0.786910869  0.431334  0.939106\n100   96.4410   -0.155268696  0.534400 -0.290547708  0.771397  0.989804\n</code></pre> <p>From the script you can see it has <code>DESeq2</code> and <code>optparse</code> as dependencies. If we want to run the script inside a container, we would have to install them. We do this in the <code>Dockerfile</code> below. We give it the following instructions:</p> <ul> <li>use the r2u base image version jammy</li> <li>install the package <code>DESeq2</code>, <code>optparse</code> and some additional packages we will need later on. We perform the installations with <code>install2.r</code>, which is a helper command that is present inside most rocker images. More info here. </li> <li>copy the script <code>test_deseq2.R</code> to <code>/opt</code> inside the container:</li> </ul> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nCOPY test_deseq2.R /opt \n</code></pre> <p>Note</p> <p>In order to use <code>COPY</code>, the file that needs to be copied needs to be in the same directory as the <code>Dockerfile</code> or one of its subdirectories.</p> <p>R image stack</p> <p>The most used R image stack is from the rocker project. It contains many different base images (e.g. with shiny, Rstudio, tidyverse etc.). It depends on the type of image whether installations with <code>apt-get</code> or <code>install2.r</code> are possible. To understand more about how to install R packages in different containers, check it this cheat sheet, or visit rocker-project.org.</p> <p>Exercise: Download the <code>test_deseq2.R</code> and build the image with <code>docker build</code>. Name the image <code>deseq2</code>. After that, start an interactive session and execute the script inside the container. </p> <p>Hint</p> <p>Make an interactive session with the options <code>-i</code> and <code>-t</code> and use <code>/bin/bash</code> as the command. </p> Answer <p>Build the container:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t deseq2 .\n</code></pre> <pre><code>DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build -t deseq2 .\n</code></pre> <p>Run the container:</p> <pre><code>docker run -it --rm deseq2 /bin/bash\n</code></pre> <p>Inside the container we look up the script:</p> <pre><code>cd /opt\nls\n</code></pre> <p>This should return <code>test_deseq2.R</code>. </p> <p>Now you can execute it from inside the container:</p> <pre><code>./test_deseq2.R --rows 100\n</code></pre> <p>That\u2019s kind of nice. We can ship our R script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add <code>/opt</code> to the global <code>$PATH</code> variable with <code>ENV</code>. </p> <p>The <code>$PATH</code> variable</p> <p>The path variable is a special variable that consists of a list of path seperated by colons (<code>:</code>). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia. </p> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nCOPY test_deseq2.R /opt \n\nENV PATH=/opt:$PATH\n</code></pre> <p>Note</p> <p>The <code>ENV</code> instruction can be used to set any variable. </p> <p>Exercise: Rebuild the image and start an interactive bash session inside the new image. Is the path variable updated? (i.e. can we execute <code>test_deseq2.R</code> from anywhere?)</p> Answer <p>After re-building we start an interactive session:</p> <pre><code>docker run -it --rm deseq2 /bin/bash\n</code></pre> <p>The path is upated, <code>/opt</code> is appended to the beginning of the variable:</p> <pre><code>echo $PATH\n</code></pre> <p>returns:</p> <pre><code>/opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n</code></pre> <p>Now you can try to execute it from the root directory (or any other):</p> <pre><code>test_deseq2.R\n</code></pre> <p>Instead of starting an interactive session with <code>/bin/bash</code> we can now more easily run the script non-interactively:</p> <pre><code>docker run --rm deseq2 test_deseq2.R --rows 100\n</code></pre> <p>Now it will directly print the output of <code>test_deseq2.R</code> to stdout. </p> <p>In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use <code>ENTRYPOINT</code>. <code>ENTRYPOINT</code> is similar to <code>CMD</code>, but has two important differences:</p> <ul> <li><code>ENTRYPOINT</code> can not be overwritten by the positional arguments (i.e. <code>docker run image [CMD]</code>), but has to be overwritten by <code>--entrypoint</code>. </li> <li>The positional arguments (or <code>CMD</code>) are pasted to the <code>ENTRYPOINT</code> command. This means that you can use <code>ENTRYPOINT</code> as the executable and the positional arguments (or <code>CMD</code>) as the options. </li> </ul> <p>Let\u2019s try it out:</p> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nCOPY test_deseq2.R /opt \n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"test_deseq2.R\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--rows\", \"100\"]\n</code></pre> <p>Exercise: Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different number of rows to <code>--rows</code>. How do the commands look?</p> Answer <p>Just running the container non-interactively would be:</p> <pre><code>docker run --rm deseq2\n</code></pre> <p>Passing a different argument (i.e. overwriting <code>CMD</code>) would be:</p> <pre><code>docker run --rm deseq2 --rows 200\n</code></pre> <p>Here, the container behaves as the executable itself to which you can pass arguments. </p> <p>Note</p> <p>You can overwrite <code>ENTRYPOINT</code> with <code>--entrypoint</code> as an argument to <code>docker run</code>. </p> <p>Exercise: Push the image to dockerhub, so we can use it later with the apptainer exercises.</p> Answer <p>Pushing it to dockerhub: </p> <pre><code>docker tag deseq2 [USER NAME]/deseq2:v1\ndocker push [USER NAME]/deseq2:v1\n</code></pre> <p>In the exercises will use a simple script called <code>daterange.py</code>. You can download it here. Or copy-paste it from here:</p> daterange.py<pre><code>#!/usr/bin/env python3\n\nimport pandas as pd\nimport argparse\n\nparser = argparse.ArgumentParser(description = \"Get a daterange\")\n\nparser.add_argument('-d', '--date', type=str, required=True, \n                    help='Date. Format: [YYYYMMDD]')\n\nargs = parser.parse_args()\n\ndates = pd.date_range(args.date, periods=7)\n\nfor d in dates:\n    print(d)\n</code></pre> <p>After you have downloaded it, make sure to set the permissions to executable:</p> <pre><code>chmod +x daterange.py\n</code></pre> <p>Have a look at <code>daterange.py</code>. It is a simple script that uses <code>pandas</code> and <code>argparse</code>. It takes a date (in the format <code>YYYYMMDD</code>) as provided by the option <code>--date</code>, and returns a list of all dates in the week starting from that date. An example for execution would be:</p> <pre><code>./daterange.py --date 20220226\n</code></pre> <p>Giving a list of dates starting from 26-FEB-2022:</p> <pre><code>2022-02-26 00:00:00\n2022-02-27 00:00:00\n2022-02-28 00:00:00\n2022-03-01 00:00:00\n2022-03-02 00:00:00\n2022-03-03 00:00:00\n2022-03-04 00:00:00\n</code></pre> <p>From the script, you can see it has the dependecy <code>pandas</code>, which is not a built-in module. In the <code>Dockerfile</code> below we give the instruction to install <code>pandas</code> with <code>pip</code> and copy <code>daterange.py</code> to <code>/opt</code> inside the container:</p> <pre><code>FROM python:3.9.16\n\nRUN pip install pandas \n\nCOPY daterange.py /opt \n</code></pre> <p>Note</p> <p>In order to use <code>COPY</code>, the file that needs to be copied needs to be in the same directory as the <code>Dockerfile</code> or one of its subdirectories.</p> <p>Exercise: Download the <code>daterange.py</code> and build the image with <code>docker build</code>. After that, execute the script inside the container. </p> <p>Hint</p> <p>Make an interactive session with the options <code>-i</code> and <code>-t</code> and use <code>/bin/bash</code> as the command. </p> Answer <p>Build the container:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t daterange .\n</code></pre> <pre><code>export DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build -t daterange .\n</code></pre> <p>Run the container:</p> <pre><code>docker run -it --rm daterange /bin/bash\n</code></pre> <p>Inside the container we look up the script:</p> <pre><code>cd /opt\nls\n</code></pre> <p>This should return <code>daterange.py</code>. </p> <p>Now you can execute it from inside the container:</p> <pre><code>./daterange.py --date 20220226\n</code></pre> <p>That\u2019s kind of nice. We can ship our python script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add <code>/opt</code> to the global <code>$PATH</code> variable with <code>ENV</code>. </p> <p>The <code>$PATH</code> variable</p> <p>The path variable is a special variable that consists of a list of path seperated by colons (<code>:</code>). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia. </p> <pre><code>FROM python:3.9.16\n\nRUN pip install pandas \n\nCOPY daterange.py /opt \n\nENV PATH=/opt:$PATH\n</code></pre> <p>Note</p> <p>The <code>ENV</code> instruction can be used to set any variable. </p> <p>Exercise: Start an interactive bash session inside the new container. Is the path variable updated? (i.e. can we execute <code>daterange.py</code> from anywhere?)</p> Answer <p>After re-building we start an interactive session:</p> <pre><code>docker run -it --rm daterange /bin/bash\n</code></pre> <p>The path is upated, <code>/opt</code> is appended to the beginning of the variable:</p> <pre><code>echo $PATH\n</code></pre> <p>returns:</p> <pre><code>/opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n</code></pre> <p>Now you can try to execute it from the root directory (or any other):</p> <pre><code>daterange.py --date 20220226\n</code></pre> <p>Instead of starting an interactive session with <code>/bin/bash</code> we can now more easily run the script non-interactively:</p> <pre><code>docker run --rm daterange daterange.py --date 20220226\n</code></pre> <p>Now it will directly print the output of <code>daterange.py</code> to stdout. </p> <p>In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use <code>ENTRYPOINT</code>. <code>ENTRYPOINT</code> is similar to <code>CMD</code>, but has two important differences:</p> <ul> <li><code>ENTRYPOINT</code> can not be overwritten by the positional arguments (i.e. <code>docker run image [CMD]</code>), but has to be overwritten by <code>--entrypoint</code>. </li> <li>The positional arguments (or <code>CMD</code>) are pasted to the <code>ENTRYPOINT</code> command. This means that you can use <code>ENTRYPOINT</code> as the executable and the positional arguments (or <code>CMD</code>) as the options. </li> </ul> <p>Let\u2019s try it out:</p> <pre><code>FROM python:3.9.16\n\nRUN pip install pandas \n\nCOPY daterange.py /opt \n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"daterange.py\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--date\", \"20220226\"]\n</code></pre> <p>Exercise: Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different date to <code>--date</code>. How do the commands look?</p> Answer <p>Just running the container non-interactively would be:</p> <pre><code>docker run --rm daterange\n</code></pre> <p>Passing a different argument (i.e. overwriting <code>CMD</code>) would be:</p> <pre><code>docker run --rm daterange --date 20210330\n</code></pre> <p>Here, the container behaves as the executable itself to which you can pass arguments. </p> <p>Note</p> <p>You can overwrite <code>ENTRYPOINT</code> with <code>--entrypoint</code> as an argument to <code>docker run</code>. </p> <p>Exercise: Push the image to dockerhub, so we can use it later with the apptainer exercises.</p> Answer <p>Pushing it to dockerhub: </p> <pre><code>docker tag daterage [USER NAME]/daterange:v1\ndocker push [USER NAME]/daterange:v1\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#extra-get-information-on-your-image-with-docker-inspect","title":"Extra: Get information on your image with <code>docker inspect</code>","text":"<p>We have used <code>docker inspect</code> already in the previous chapter to find the default <code>Cmd</code> of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the <code>Config</code> record from the output of <code>docker inspect</code>. For our image this looks like:</p> <pre><code>\"Config\": {\n    \"Hostname\": \"\",\n    \"Domainname\": \"\",\n    \"User\": \"\",\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"AttachStderr\": false,\n    \"Tty\": false,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Env\": [\n        \"PATH=/opt:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n        \"LC_ALL=en_US.UTF-8\",\n        \"LANG=en_US.UTF-8\",\n        \"DEBIAN_FRONTEND=noninteractive\",\n        \"TZ=UTC\"\n    ],\n    \"Cmd\": [\n        \"--rows\",\n        \"100\"\n    ],\n    \"ArgsEscaped\": true,\n    \"Image\": \"\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"/opt\",\n    \"Entrypoint\": [\n        \"test_deseq2.R\"\n    ],\n    \"OnBuild\": null,\n    \"Labels\": {\n        \"maintainer\": \"Dirk Eddelbuettel &lt;edd@debian.org&gt;\",\n        \"org.label-schema.license\": \"GPL-2.0\",\n        \"org.label-schema.vcs-url\": \"https://github.com/rocker-org/\",\n        \"org.label-schema.vendor\": \"Rocker Project\"\n    }\n}\n</code></pre> <p>You can annotate your <code>Dockerfile</code> and the image by using the instruction <code>LABEL</code>. You can give it any key and value with <code>&lt;key&gt;=&lt;value&gt;</code>. However, it is recommended to use the Open Container Initiative (OCI) keys.</p> <p>Exercise: Annotate our <code>Dockerfile</code> with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with <code>docker inspect</code>. </p> <p>Note</p> <p>You can type <code>LABEL</code> for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.:</p> <pre><code>LABEL keyx=\"valuex\" keyy=\"valuey\"\n</code></pre> Answer <p>The <code>Dockerfile</code> would look like:</p> <pre><code>FROM rocker/r2u:jammy\n\nLABEL org.opencontainers.image.created=\"2023-04-12\" \\\n        org.opencontainers.image.authors=\"Geert van Geest\" \\\n        org.opencontainers.image.description=\"Container with DESeq2 and friends\"\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nWORKDIR /opt\n\nCOPY test_deseq2.R .\n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"test_deseq2.R\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--rows\", \"100\"]\n</code></pre> <p>The <code>Config</code> record in the output of <code>docker inspect</code> was updated with:</p> <pre><code>    \"Labels\": {\n        \"org.opencontainers.image.authors\": \"Geert van Geest\",\n        \"org.opencontainers.image.created\": \"2023-04-12\",\n        \"org.opencontainers.image.description\": \"Container with DESeq2 and friends\",\n        \"org.opencontainers.image.licenses\": \"GPL-2.0-or-later\",\n        \"org.opencontainers.image.source\": \"https://github.com/rocker-org/rocker\",\n        \"org.opencontainers.image.vendor\": \"Rocker Project\"\n    }\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#extra-building-an-image-with-a-browser-interface","title":"Extra: Building an image with a browser interface","text":"<p>In this exercise, we will use a different base image (<code>rocker/rstudio:4</code>), and we\u2019ll install the same packages. Rstudio server is a nice browser interface that you can use for a.o. programming in R. With the image we are creating we will be able to run Rstudio server inside a container.  Check out the <code>Dockerfile</code>:</p> <pre><code>FROM rocker/rstudio:4\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y libz-dev\n\nRUN install2.r \\\n    optparse \\\n    BiocManager\n\nRUN R -q -e 'BiocManager::install(\"biomaRt\")'\n</code></pre> <p>This will create an image from the existing <code>rstudio</code> image. It will also install <code>libz-dev</code> with <code>apt-get</code>, <code>BiocManager</code> with <code>install2.r</code> and <code>biomaRt</code> with an R command. Despiste we\u2019re installing the same packages, the installation steps need to be different from the <code>r-base</code> image. This is because in the <code>rocker/rstudio</code> images R is installed from source, and therefore you can\u2019t install packages with <code>apt-get</code>. More information on how to install R packages in R containers in this cheat sheet, or visit rocker-project.org.</p> <p>Exercise: Build an image based on this <code>Dockerfile</code> and give it a meaningful name.</p> Answer x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t rstudio-server .\n</code></pre> <pre><code>export DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build -t rstudio-server .\n</code></pre> <p>You can now run a container from the image. However, you will have to tell docker where to publish port 8787 from the docker container with <code>-p [HOSTPORT:CONTAINERPORT]</code>. We choose to publish it to the same port number:</p> <pre><code>docker run --rm -it -p 8787:8787 rstudio-server\n</code></pre> <p>Networking</p> <p>More info on docker container networking here</p> <p>By running the above command, a container will be started exposing rstudio server at port 8787 at localhost. You can approach the instance of jupyterhub by typing <code>localhost:8787</code> in your browser. You will be asked for a password. You can find this password in the terminal from which you have started the container.</p> <p>We can make this even more interesting by mounting a local directory to the container running the jupyter-lab image:</p> <pre><code>docker run \\\n-it \\\n--rm \\\n-p 8787:8787 \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/home/rstudio/working_dir \\\nrstudio-server\n</code></pre> <p>By doing this you have a completely isolated and shareable R environment running Rstudio server, but with your local files available to it. Pretty neat right? </p>"},{"location":"course_material/day1/dockerfiles/#extra-get-information-on-your-image-with-docker-inspect_1","title":"Extra: Get information on your image with <code>docker inspect</code>","text":"<p>We have used <code>docker inspect</code> already in the previous chapter to find the default <code>Cmd</code> of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the <code>Config</code> record from the output of <code>docker inspect</code>. For our image this looks like:</p> <pre><code>\"Config\": {\n        \"Hostname\": \"\",\n        \"Domainname\": \"\",\n        \"User\": \"\",\n        \"AttachStdin\": false,\n        \"AttachStdout\": false,\n        \"AttachStderr\": false,\n        \"Tty\": false,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Env\": [\n            \"PATH=/opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n            \"LANG=C.UTF-8\",\n            \"GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568\",\n            \"PYTHON_VERSION=3.9.4\",\n            \"PYTHON_PIP_VERSION=21.1.1\",\n            \"PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/1954f15b3f102ace496a34a013ea76b061535bd2/public/get-pip.py\",\n            \"PYTHON_GET_PIP_SHA256=f499d76e0149a673fb8246d88e116db589afbd291739bd84f2cd9a7bca7b6993\"\n        ],\n        \"Cmd\": [\n            \"--date\",\n            \"20220226\"\n        ],\n        \"ArgsEscaped\": true,\n        \"Image\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"/opt\",\n        \"Entrypoint\": [\n            \"daterange.py\"\n        ],\n        \"OnBuild\": null,\n        \"Labels\": null\n    },\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#extra-adding-metadata-to-your-image","title":"Extra: Adding metadata to your image","text":"<p>You can annotate your <code>Dockerfile</code> and the image by using the instruction <code>LABEL</code>. You can give it any key and value with <code>&lt;key&gt;=&lt;value&gt;</code>. However, it is recommended to use the Open Container Initiative (OCI) keys.</p> <p>Exercise: Annotate our <code>Dockerfile</code> with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with <code>docker inspect</code>. </p> <p>Note</p> <p>You can type <code>LABEL</code> for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.:</p> <pre><code>LABEL keyx=\"valuex\" keyy=\"valuey\"\n</code></pre> Answer <p>The <code>Dockerfile</code> would look like:</p> <pre><code>FROM python:3.9.16\n\nLABEL org.opencontainers.image.created=\"2022-04-12\" \\\n    org.opencontainers.image.authors=\"Geert van Geest\" \\\n    org.opencontainers.image.description=\"Great container for getting all dates in a week! \\\n    You will never use a calender again\"\n\nRUN pip install pandas \n\nWORKDIR /opt\n\nCOPY daterange.py .\n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"daterange.py\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--date\", \"20220226\"]\n</code></pre> <p>The <code>Config</code> record in the output of <code>docker inspect</code> was updated with:</p> <pre><code>\"Labels\": {\n            \"org.opencontainers.image.authors\": \"Geert van Geest\",\n            \"org.opencontainers.image.created\": \"2022-04-12\",\n            \"org.opencontainers.image.description\": \"Great container for getting all dates in a week!     You will never use a calender again\"\n        }\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#extra-building-an-image-with-a-browser-interface_1","title":"Extra: Building an image with a browser interface","text":"<p>In this exercise, we will use a different base image from the jupyter docker image stack. JupyterLab is a nice browser interface that you can use for a.o. programming in python. With the image we are creating we will be able to run jupyter lab inside a container.  Check out the <code>Dockerfile</code>:</p> <pre><code>FROM jupyter/base-notebook:python-3.9\n\nRUN pip install pandas\n</code></pre> <p>This will create an image from the existing <code>python</code> image. It will also install <code>jupyterlab</code> with <code>pip</code>. As a default command it starts a jupyter notebook at port 8888.</p> <p>Ports</p> <p>We have specified here that jupyter lab should use port 8888. However, this inside the container. We can not connect to it yet with our browser.</p> <p>Exercise: Build an image based on this <code>Dockerfile</code> and give it a meaningful name.</p> Answer x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t jupyter-lab .\n</code></pre> <pre><code>export DOCKER_DEFAULT_PLATFORM=linux/amd64\ndocker build -t jupyter-lab .\n</code></pre> <p>You can now run a container from the image. However, you will have to tell docker where to publish port 8888 from the docker container with <code>-p [HOSTPORT:CONTAINERPORT]</code>. We choose to publish it to the same port number:</p> <pre><code>docker run --rm -it -p 8888:8888 jupyter-lab\n</code></pre> <p>Networking</p> <p>More info on docker container networking here</p> <p>By running the above command, a container will be started exposing jupyterhub at port 8888 at localhost. You can approach the instance of jupyterhub by typing <code>localhost:8888</code> in your browser. You will be asked for a token. You can find this token in the terminal from which you have started the container.</p> <p>We can make this even more interesting by mounting a local directory to the container running the jupyter-lab image:</p> <pre><code>docker run \\\n-it \\\n--rm \\\n-p 8888:8888 \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/working_dir/ \\\njupyter-lab\n</code></pre> <p>By doing this you have a completely isolated and shareable python environment running jupyter lab, but with your local files available to it. Pretty neat right? </p> <p>Note</p> <p>Jupyter has a wide range of pre-built images available here.</p>"},{"location":"course_material/day1/introduction_containers/","title":"Introduction to containers","text":""},{"location":"course_material/day1/introduction_containers/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Discriminate between an image and a container</li> <li>Run a docker container from dockerhub interactively</li> <li>Validate the available containers and their status</li> </ul>"},{"location":"course_material/day1/introduction_containers/#material","title":"Material","text":"<p>General introduction:</p> <p> Download the presentation</p> <p>Introduction to containers:</p> <p> Download the presentation</p>"},{"location":"course_material/day1/introduction_containers/#exercises","title":"Exercises","text":"<p>We recommend using a code editor like VScode or Sublime text. If you don\u2019t know which one to chose, take VScode as we can provide most support for this editor. </p> <p>If working on Windows</p> <p>If you are working on Windows, it is easiest to work with WSL2. With VScode use the WSL extension. Make sure you install the latest versions before you install docker. In principle, you can also use a native shell like PowerShell, but this might result into some issues with bind mounting directories.</p> <p>Work in projects</p> <p>We recommend to work in a project folder. This will make it easier to find your files and to share them with others. You can create a project folder anywhere on your computer. For example, you can create a folder <code>projects</code> in your home directory and then create a subfolder <code>container-course</code> in it. You can then open this folder in VScode.</p> <p>Let\u2019s create our first container from an existing image. We do this with the image <code>ubuntu</code>, generating an environment with a minimal installation of ubuntu.  </p> <pre><code>docker run -it ubuntu\n</code></pre> <p>This will give you an interactive shell into the created container (this interactivity was invoked by the options <code>-i</code> and <code>-t</code>) .</p> <p>Exercise: Check out the operating system of the container by typing <code>cat /etc/os-release</code> in the container\u2019s shell. Are we really in an ubuntu environment?</p> Answer <p>Yes:</p> <pre><code>root@33bd068de5e2:/# cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.04 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n</code></pre> <p>Where does the image come from?</p> <p>If the image <code>ubuntu</code> was not on your computer yet, <code>docker</code> will search and try to get them from dockerhub, and download it.</p> <p>Exercise: Run the command <code>whoami</code> in the docker container. Who are you?</p> Answer <p>The command <code>whoami</code> returns the current user. In the container <code>whoami</code> will return <code>root</code>. This means you are the <code>root</code> user i.e. within the container you are admin and can basically change anything.  </p> <p>Check out the container panel at the Docker dashboard (the Docker gui) or open another host terminal and type:</p> <pre><code>docker container ls -a\n</code></pre> <p>Exercise: What is the container status?</p> Answer <p>In Docker dashboard you can see that the shell is running:</p> <p> </p> <p>The output of <code>docker container ls -a</code> is:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES\n27f7d11608de   ubuntu    \"/bin/bash\"   7 minutes ago   Up 6 minutes             great_moser\n</code></pre> <p>Also showing you that the <code>STATUS</code> is <code>Up</code>.</p> <p>Now let\u2019s install some software in our <code>ubuntu</code> environment. We\u2019ll install some simple software called <code>figlet</code>. Type into the container shell:</p> <pre><code>apt-get update\napt-get install figlet\n</code></pre> <p>This will give some warnings</p> <p>This installation will give some warnings. It\u2019s safe to ignore them.</p> <p>Now let\u2019s try it out. Type into the container shell:</p> <pre><code>figlet 'SIB courses are great!'\n</code></pre> <p>Now you have installed and used software <code>figlet</code> in an <code>ubuntu</code> environment (almost) completely separated from your host computer. This already gives you an idea of the power of containerization.</p> <p>Exit the shell by typing <code>exit</code>. Check out the container panel of Docker dashboard or type:</p> <pre><code>docker container ls -a\n</code></pre> <p>Exercise: What is the container status?</p> Answer <p><code>docker container ls -a</code> gives:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND       CREATED          STATUS                     PORTS     NAMES\n27f7d11608de   ubuntu    \"/bin/bash\"   15 minutes ago   Exited (0) 8 seconds ago             great_moser\n</code></pre> <p>Showing that the container has exited, meaning it\u2019s not running.</p>"},{"location":"course_material/day1/managing_docker/","title":"Managing containers and images","text":""},{"location":"course_material/day1/managing_docker/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Explain the concept of layers in the context of docker containers and images</li> <li>Use the command line to restart and re-attach to an exited container</li> <li>Create a new image with <code>docker commit</code></li> <li>List locally available images with <code>docker image ls</code></li> <li>Run a command inside a container non-interactively</li> <li>Use <code>docker image inspect</code> to get more information on an image</li> <li>Use the command line to prune dangling images and stopped containers</li> <li>Rename and tag a docker image</li> <li>Push a newly created image to dockerhub</li> <li>Use the option <code>--mount</code> to bind mount a host directory to a container</li> </ul>"},{"location":"course_material/day1/managing_docker/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Overview of how docker works</li> <li>More on bind mounts</li> <li>Docker volumes in general</li> </ul>"},{"location":"course_material/day1/managing_docker/#exercises","title":"Exercises","text":""},{"location":"course_material/day1/managing_docker/#restarting-an-exited-container","title":"Restarting an exited container","text":"<p>If you would like to go back to your container with the <code>figlet</code> installation, you could try to run again:</p> <pre><code>docker run -it ubuntu\n</code></pre> <p>Exercise: Run the above command. Is your <code>figlet</code> installation still there? Why?</p> <p>Hint</p> <p>Check the status of your containers:</p> <pre><code>docker container ls -a\n</code></pre> Answer <p>No, the installation is gone. Another container was created from the same ubuntu image, without the <code>figlet</code> installation. Running the command <code>docker container ls -a</code> results in:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND       CREATED              STATUS                     PORTS     NAMES\n8d7c4c611b70   ubuntu    \"/bin/bash\"   About a minute ago   Up About a minute                    kind_mendel\n27f7d11608de   ubuntu    \"/bin/bash\"   27 minutes ago       Exited (0) 2 minutes ago             great_moser\n</code></pre> <p>In this case the container <code>great_moser</code> contains the <code>figlet</code> installation. But we have exited that container. We created a new container (<code>kind_mendel</code> in this case) with a fresh environment created from the original <code>ubuntu</code> image.</p> <p>To restart your first created container, you\u2019ll have to look up its name. You can find it in the Docker dashboard, or with <code>docker container ls -a</code>.</p> <p>Container names</p> <p>The container name is the funny combination of two words separated by <code>_</code>, e.g.: <code>nifty_sinoussi</code>. Alternatively you can use the container ID (the first column of the output of <code>docker container ls</code>)</p> <p>To restart a container you can use:</p> <pre><code>docker start [CONTAINER NAME]\n</code></pre> <p>And after that to re-attach to the shell:</p> <pre><code>docker attach [CONTAINER NAME]\n</code></pre> <p>And you\u2019re back in the container shell.</p> <p>Exercise: Run the <code>docker start</code> and <code>docker attach</code> commands for the container that is supposed to contain the <code>figlet</code> installation. Is the installation of <code>figlet</code> still there?</p> Answer <p>yes:</p> <pre><code>figlet 'try some more text!'\n</code></pre> <p>Should give you output.</p> <p><code>docker attach</code> and <code>docker exec</code></p> <p>In addition to <code>docker attach</code>, you can also \u201cre-attach\u201d a container with <code>docker exec</code>. However, these two are quite different. While <code>docker attach</code> gets you back to your stopped shell process, <code>docker exec</code> creates a new one (more information on stackoverflow). The command <code>docker exec</code> enables you therefore to have multiple shells open in the same container. That can be convenient if you have one shell open with a program running in the foreground, and another one for e.g. monitoring. An example for using <code>docker exec</code> on a running container:</p> <pre><code>docker exec -it [CONTAINER NAME] /bin/bash\n</code></pre> <p>Note that  <code>docker exec</code> requires a CMD, it doesn\u2019t use the default.</p>"},{"location":"course_material/day1/managing_docker/#creating-a-new-image","title":"Creating a new image","text":"<p>You can store your changes and create a new image based on the <code>ubuntu</code> image like this:</p> <pre><code>docker commit [CONTAINER NAME] ubuntu-figlet\n</code></pre> <p>Exercise: Exit the container shell and run the above command  in your local terminal (replace <code>[CONTAINER NAME]</code> with the name of the container containing the <code>figlet</code> installation). Check out <code>docker image ls</code>. What have we just created?</p> Answer <p>A new image called <code>ubuntu-figlet</code> based on the status of the container. The output of <code>docker image ls</code> should look like:</p> <pre><code>REPOSITORY                        TAG       IMAGE ID       CREATED         SIZE\nubuntu-figlet                     latest    e08b999c7978   4 seconds ago   101MB\nubuntu                            latest    f63181f19b2f   29 hours ago    72.9MB\n</code></pre> <p>Now you can generate a new container based on the new image:</p> <pre><code>docker run -it ubuntu-figlet\n</code></pre> <p>Exercise: Run the above command. Is the <code>figlet</code> installation in the created container?</p> Answer <p>yes</p>"},{"location":"course_material/day1/managing_docker/#commands","title":"Commands","text":"<p>The second positional argument of <code>docker run</code> can be a command followed by its arguments. So, we could run a container non-interactively (without <code>-it</code>), and just let it run a single command:</p> <pre><code>docker run ubuntu-figlet figlet 'non-interactive run'\n</code></pre> <p>Resulting in just the output of the <code>figlet</code> command.</p> <p>In the previous exercises we have run containers without a command as positional argument. This doesn\u2019t mean that no command has been run, because the container would do nothing without a command. The default command is stored in the image, and you can find it by <code>docker image inspect [IMAGE NAME]</code>.  </p> <p>Exercise: Have a look at the output of <code>docker image inspect</code>, particularly at <code>\"Config\"</code> (ignore <code>\"ContainerConfig\"</code> for now). What is the default command (<code>CMD</code>) of the ubuntu image?</p> Answer <p>Running <code>docker image inspect ubuntu</code> gives (amongst other information):</p> <pre><code>\"Cmd\": [\n    \"/bin/bash\"\n],\n</code></pre> <p>In the case of the ubuntu the default command is <code>bash</code>, returning a shell in <code>bash</code> (i.e. Bourne again shell). Adding the options <code>-i</code> and <code>-t</code> (<code>-it</code>) to your <code>docker run</code> command will therefore result in an interactive <code>bash</code> shell. You can modify this default behaviour. More on that later, when we will work on Dockerfiles.</p> <p>The difference between <code>Config</code> and <code>ContainerConfig</code></p> <p>The configuration at <code>Config</code> represents the image, the configuration at <code>ContainerConfig</code> the last step during the build of the image, i.e. the last layer. More info e.g. at this post at stackoverflow.</p>"},{"location":"course_material/day1/managing_docker/#removing-containers","title":"Removing containers","text":"<p>In the meantime, with every call of <code>docker run</code> we have created a new container (check your containers with <code>docker container ls -a</code>). You probably don\u2019t want to remove those one-by-one. These two commands are very useful to clean up your Docker cache:</p> <ul> <li><code>docker container prune</code>: removes stopped containers</li> <li><code>docker image prune</code>: removes dangling images (i.e. images without a name)</li> </ul> <p>So, remove your stopped containers with:</p> <pre><code>docker container prune\n</code></pre> <p>Unless you\u2019re developing further on a container, or you\u2019re using it for an analysis, you probably want to get rid of it once you have exited the container. You can do this with adding <code>--rm</code> to your <code>docker run</code> command, e.g.:</p> <pre><code>docker run --rm ubuntu-figlet figlet 'non-interactive run'\n</code></pre>"},{"location":"course_material/day1/managing_docker/#pushing-to-dockerhub","title":"Pushing to dockerhub","text":"<p>Now that we have created our first own docker image, we can store it and share it with the world on docker hub. Before we get there, we first have to (re)name and tag it.</p> <p>Before pushing an image to dockerhub, <code>docker</code> has to know to which user and which repository the image should be added. That information should be in the name of the image, like this: <code>user/imagename</code>. We can rename an image with <code>docker tag</code> (which is a bit of misleading name for the command). So we could push to dockerhub like this:</p> <pre><code>docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet\ndocker push [USER NAME]/ubuntu-figlet\n</code></pre> <p>If on Linux</p> <p>If you are on Linux and haven\u2019t connected to docker hub before, you will have login first. To do that, run:</p> <pre><code>docker login\n</code></pre> <p>How docker makes money</p> <p>All images pushed to dockerhub are open to the world. With a free account you can have one image on dockerhub that is private. Paid accounts can have more private images, and are therefore popular for commercial organisations. As an alternative to dockerhub, you can store images locally with <code>docker save</code>.</p> <p>We didn\u2019t specify the tag for our new image. That\u2019s why <code>docker tag</code> gave it the default tag called <code>latest</code>. Pushing an image without a tag will overwrite the current image with the tag <code>latest</code> (more on (not) using <code>latest</code> here). If you want to maintain multiple versions of your image, you will have to add a tag, and push the image with that tag to dockerhub:</p> <pre><code>docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet:v1\ndocker push [USER NAME]/ubuntu-figlet:v1\n</code></pre>"},{"location":"course_material/day1/managing_docker/#mounting-a-directory","title":"Mounting a directory","text":"<p>For many analyses you do calculations with files or scripts that are on your host (local) computer. But how do you make them available to a docker container? You can do that in several ways, but here we will use bind-mount. You can bind-mount a directory with <code>-v</code> (<code>--volume</code>) or <code>--mount</code>. Most old-school <code>docker</code> users will use <code>-v</code>, but <code>--mount</code> syntax is easier to understand and now recommended, so we will use the latter here:</p> <pre><code>docker run \\\n--mount type=bind,source=/host/source/path,target=/path/in/container \\\n[IMAGE]\n</code></pre> <p>The target directory will be created if it does not yet exist. The source directory should exist.</p> <p>MobaXterm users</p> <p>You can specify your local path with the Windows syntax (e.g. <code>C:\\Users\\myusername</code>). However, you will have to use forward slashes (<code>/</code>) instead of backward slashes (<code>\\</code>). Therefore, mounting a directory would look like:</p> <pre><code>docker run \\\n--mount type=bind,source=C:/Users/myusername,target=/path/in/container \\\n[IMAGE]\n</code></pre> <p>Do not use autocompletion or variable substitution (e.g. <code>$PWD</code>) in MobaXterm, since these point to \u2018emulated\u2019 paths, and are not passed properly to the docker command.</p> <p>Using docker from Windows PowerShell</p> <p>Most of the syntax for <code>docker</code> is the same for both PowerShell and UNIX-based systems. However, there are some differences, e.g. in Windows, directories in file paths are separated by <code>\\</code> instead of <code>/</code>. Also, line breaks are not escaped by <code>\\</code> but by `.</p> <p>Exercise: Mount a host (local) directory to a target directory <code>/working_dir</code> in a container created from the <code>ubuntu-figlet</code> image and run it interactively. Check whether the target directory has been created.</p> Answer <p>e.g. on Mac OS this would be:</p> <pre><code>docker run \\\n-it \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/working_dir/ \\\nubuntu-figlet\n</code></pre> <p>This creates a directory called <code>working_dir</code> in the root directory (<code>/</code>):</p> <pre><code>root@8d80a8698865:/# ls\nbin   dev  home  lib32  libx32  mnt  proc  run   srv  tmp  var\nboot  etc  lib   lib64  media   opt  root  sbin  sys  usr  working_dir\n</code></pre> <p>This mounted directory is both available for the host (locally) and for the container. You can therefore e.g. copy files in there, and write output generated by the container.</p> <p>Exercise: Write the output of <code>figlet \"testing mounted dir\"</code> to a file in <code>/working_dir</code>. Check whether it is available on the host (locally) in the source directory.</p> <p>Hint</p> <p>You can write the output of <code>figlet</code> to a file like this: <pre><code>figlet 'some string' &gt; file.txt\n</code></pre></p> Answer <pre><code>root@8d80a8698865:/# figlet 'testing mounted dir' &gt; /working_dir/figlet_output.txt\n</code></pre> <p>This should create a file in both your host (local) source directory and the target directory in the container called <code>figlet_output.txt</code>.</p> <p>Using files on the host</p> <p>This of course also works the other way around. If you would have a file on the host with e.g. a text, you can copy it into your mounted directory, and it will be available to the container.</p>"},{"location":"course_material/day1/managing_docker/#extra-managing-permissions","title":"Extra: Managing permissions","text":"<p>Depending on your system, the user ID and group ID will be taken over from the user inside the container. If the user inside the container is root, this will be root. That\u2019s a bit inconvenient if you just want to run the container as a regular user (for example in certain circumstances your container could write in <code>/</code>). To do that, use the <code>-u</code> option, and specify the group ID and user ID like this:</p> <pre><code>docker run -u [uid]:[gid]\n</code></pre> <p>So, e.g.:</p> <pre><code>docker run \\\n-it \\\n-u 1000:1000 \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/working_dir/ \\\nubuntu-figlet\n</code></pre> <p>If you want docker to take over your current uid and gid, you can use:</p> <pre><code>docker run -u \"$(id -u):$(id -g)\"\n</code></pre> <p>This behaviour is different on MacOS and MobaXterm</p> <p>On MacOS and in the local shell of MobaXterm the uid and gid are taken over from the user running the container (even if you set <code>-u</code> as 0:0), i.e. your current ID. More info on stackoverflow.</p> <p>Exercise: Start an interactive container based on the <code>ubuntu-figlet</code> image, bind-mount a local directory and take over your current <code>uid</code> and <code>gid</code>. Write the output of a <code>figlet</code> command to a file in the mounted directory. Who and which group owns the file inside the container? And outside the container? Answer the same question but now run the container without setting <code>-u</code>.</p> Answer LinuxMacOSMobaXterm <p>Running <code>ubuntu-figlet</code> interactively while taking over <code>uid</code> and <code>gid</code> and mounting my current directory:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data -u \"$(id -u):$(id -g)\" ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>I have no name!@e808d7c36e7c:/$ id\nuid=1000 gid=1000 groups=1000\n</code></pre> <p>So, I have taken over uid 1000 and gid 1000.</p> <pre><code>I have no name!@e808d7c36e7c:/$ cd /data\nI have no name!@e808d7c36e7c:/data$ figlet 'uid set' &gt; uid_set.txt\nI have no name!@e808d7c36e7c:/data$ ls -lh\n-rw-r--r-- 1 1000 1000 0 Mar  400 13:37 uid_set.txt\n</code></pre> <p>So the file belongs to user 1000, and group 1000.</p> <p>Outside container:</p> <pre><code>ubuntu@ip-172-31-33-21:~$ ls -lh\n-rw-r--r-- 1 ubuntu ubuntu 400 Mar  5 13:37 uid_set.txt\n</code></pre> <p>Which makes sense:</p> <pre><code>ubuntu@ip-172-31-33-21:~$ id\nuid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu)\n</code></pre> <p>Running <code>ubuntu-figlet</code> interactively without taking over <code>uid</code> and <code>gid</code>:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>root@fface8afb220:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre> <p>So, uid and gid are <code>root</code>.</p> <pre><code>root@fface8afb220:/# cd /data\nroot@fface8afb220:/data# figlet 'uid unset' &gt; uid_unset.txt\nroot@fface8afb220:/data# ls -lh\n-rw-r--r-- 1 1000 1000 400 Mar  5 13:37 uid_set.txt\n-rw-r--r-- 1 root root 400 Mar  5 13:40 uid_unset.txt\n</code></pre> <p>Outside container:</p> <pre><code>ubuntu@ip-172-31-33-21:~$ ls -lh\n-rw-r--r-- 1 ubuntu ubuntu 0 Mar  5 13:37 uid_set.txt\n-rw-r--r-- 1 root   root   0 Mar  5 13:40 uid_unset.txt\n</code></pre> <p>So, the uid and gid 0 (root:root) are taken over.</p> <p>Running <code>ubuntu-figlet</code> interactively while taking over <code>uid</code> and <code>gid</code> and mounting my current directory:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data -u \"$(id -u):$(id -g)\" ubuntu-figlet\n</code></pre> Inside container:</p> <p><pre><code>I have no name!@e808d7c36e7c:/$ id\nuid=503 gid=20(dialout) groups=20(dialout)\n</code></pre> So, the container has taken over uid 503 and group 20</p> <pre><code>I have no name!@e808d7c36e7c:/$ cd /data\nI have no name!@e808d7c36e7c:/data$ figlet 'uid set' &gt; uid_set.txt\nI have no name!@e808d7c36e7c:/data$ ls -lh\n-rw-r--r--  1 503 dialout    400 Mar  5 13:11 uid_set.txt\n</code></pre> <p>So the file belongs to user 503, and the group <code>dialout</code>.</p> <p>Outside container:</p> <pre><code>mac-34392:~ geertvangeest$ ls -lh\n-rw-r--r--   1 geertvangeest  staff     400B Mar  5 14:11 uid_set.txt\n</code></pre> <p>Which are the same as inside the container:</p> <pre><code>mac-34392:~ geertvangeest$ echo \"$(id -u):$(id -g)\"\n503:20\n</code></pre> <p>The <code>uid</code> 503 was nameless in the docker container. However the group 20 already existed in the ubuntu container, and was named <code>dialout</code>.</p> <p>Running <code>ubuntu-figlet</code> interactively without taking over <code>uid</code> and <code>gid</code>:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>root@fface8afb220:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre> <p>So, inside the container I am <code>root</code>. Creating new files will lead to ownership of <code>root</code> inside the container:</p> <pre><code>root@fface8afb220:/# cd /data\nroot@fface8afb220:/data# figlet 'uid unset' &gt; uid_unset.txt\nroot@fface8afb220:/data# ls -lh\n-rw-r--r--  1 503 dialout    400 Mar  5 13:11 uid_set.txt\n-rw-r--r--  1 root root    400 Mar  5 13:25 uid_unset.txt\n</code></pre> <p>Outside container:</p> <pre><code>mac-34392:~ geertvangeest$ ls -lh\n-rw-r--r--   1 geertvangeest  staff     400B Mar  5 14:11 uid_set.txt\n-rw-r--r--   1 geertvangeest  staff     400B Mar  5 14:15 uid_unset.txt\n</code></pre> <p>So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.</p> <p>Running <code>ubuntu-figlet</code> interactively while taking over <code>uid</code> and <code>gid</code> and mounting to a  specfied directory:</p> <p><pre><code>docker run -it --mount type=bind,source=C:/Users/geert/data,target=/data -u \"$(id -u):$(id -g)\" ubuntu-figlet\n</code></pre> Inside container:</p> <p><pre><code>I have no name!@e808d7c36e7c:/$ id\nuid=1003 gid=513 groups=513\n</code></pre> So, the container has taken over uid 1003 and group 513</p> <pre><code>I have no name!@e808d7c36e7c:/$ cd /data\nI have no name!@e808d7c36e7c:/data$ figlet 'uid set' &gt; uid_set.txt\nI have no name!@e808d7c36e7c:/data$ ls -lh\n-rw-r--r--  1 1003 513    400 Mar  5 13:11 uid_set.txt\n</code></pre> <p>So the file belongs to user 1003, and the group 513.</p> <p>Outside container:</p> <pre><code>/home/mobaxterm/data$ ls -lh\n-rwx------   1 geert  UserGrp     400 Mar  5 14:11 uid_set.txt\n</code></pre> <p>Which are the same as inside the container:</p> <pre><code>/home/mobaxterm/data$ echo \"$(id -u):$(id -g)\"\n1003:513\n</code></pre> <p>Running <code>ubuntu-figlet</code> interactively without taking over <code>uid</code> and <code>gid</code>:</p> <p><pre><code>docker run -it --mount type=bind,source=C:/Users/geert/data,target=/data ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>root@fface8afb220:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre> <p>So, inside the container I am <code>root</code>. Creating new files will lead to ownership of <code>root</code> inside the container:</p> <pre><code>root@fface8afb220:/# cd /data\nroot@fface8afb220:/data# figlet 'uid unset' &gt; uid_unset.txt\nroot@fface8afb220:/data# ls -lh\n-rw-r--r--  1 1003 503    400 Mar  5 13:11 uid_set.txt\n-rw-r--r--  1 root root    400 Mar  5 13:25 uid_unset.txt\n</code></pre> <p>Outside container:</p> <pre><code>/home/mobaxterm/data$ ls -lh\n-rwx------   1 geert  UserGrp     400 Mar  5 14:11 uid_set.txt\n-rwx------   1 geert  UserGrp     400 Mar  5 14:15 uid_unset.txt\n</code></pre> <p>So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.</p>"},{"location":"course_material/day2/1_guidelines/","title":"General guidelines","text":""},{"location":"course_material/day2/1_guidelines/#course-goal","title":"Course goal","text":"<p>Throughout the course, you will implement and improve a workflow to trim bulk RNAseq reads, align them to a genome, perform some quality checks (QC), count mapped reads, and identify Differentially Expressed Genes (DEG). After the last series of exercises, you will have implemented a simple workflow with commonly used Snakemake features. You will be able to use this workflow as a reference to implement your own workflows in the future.</p>"},{"location":"course_material/day2/1_guidelines/#software","title":"Software","text":"<p>All the software needed in this workflow is either:</p> <ul> <li>Already installed in the <code>snake_course</code> conda environment</li> <li>Already installed in a Docker container</li> <li>Will be installed via a conda environment during today\u2019s exercises</li> </ul> <p>All information of this course is based on the official documentation for Snakemake version <code>8.20.5</code>.</p>"},{"location":"course_material/day2/1_guidelines/#website-colour-code-explanation","title":"Website colour code explanation","text":"<p>We tried to use a colour code throughout the website to make the different pieces of information easily distinguishable. Here\u2019s a quick summary about the colour blocks you will encounter:</p> <p>This is a supplementary piece of information</p> <p>This is a tip to help you solve an exercise</p> <p>This is the answer to an exercise</p> <p>This is a warning about a potential problem</p> <p>This is an explanation about a common bug/error</p>"},{"location":"course_material/day2/1_guidelines/#exercises","title":"Exercises","text":"<p>Each series of exercises is divided into multiple questions. Each question provides a background explanation, a description of the task at hand and additional details when required.</p> <p>Hints for challenging questions</p> <p>For the most challenging questions, hints will be provided. However, you should first try to solve the problem without them!</p>"},{"location":"course_material/day2/1_guidelines/#answers","title":"Answers","text":"<p>Do not hesitate to modify and overwrite your code from previous answers as difficulty is incremental. The questions are designed to incite you to build your answers upon the previous ones.</p> <p>Restarting from a clean Snakefile</p> <ul> <li>If you feel that you drifted too far apart from the solution, you can always restart from files provided in the solutions folder of the course repository</li> <li>At the start of sessions 3 and 4, you will also find a short note with a command to download the complete Snakefile from the previous session</li> </ul> <p>If something is not clear at any point, please call us and we will do our best to answer your questions! You can also check the official Snakemake documentation for more information.</p>"},{"location":"course_material/day2/1_guidelines/#computing-environment","title":"Computing environment","text":"<p>Development and computation</p> <p>You can develop and write your scripts in a distant folder (using an <code>ssh</code> connection via VS code) or locally (if you do so, you will need to copy them on the server (with <code>scp</code>) before running them), but remember that all the computations should be performed on the server, so don\u2019t forget to log in with <code>ssh -i key_username.pem username@18.195.170.182</code>.</p> <p><code>Error: Command not found</code></p> <p>If you try to run a command and get an error such as <code>Command 'snakemake' not found</code>, you are probably in the wrong conda environment:</p> <ul> <li>To list available conda environments, use <code>conda env list</code></li> <li>To activate an environment, use <code>conda activate &lt;env_name&gt;</code></li> <li>To deactivate an environment, use <code>conda deactivate</code></li> <li>To list packages installed in an environment, activate it and use <code>conda list</code>. The computing environment on the server is called <code>snake_course</code>.</li> </ul>"},{"location":"course_material/day2/2_introduction_snakemake/","title":"Introduction to Snakemake","text":""},{"location":"course_material/day2/2_introduction_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand the structure of a Snakemake workflow</li> <li>Write rules and Snakefiles to produce the desired outputs</li> <li>Chain rules together</li> <li>Run a Snakemake workflow</li> </ul>"},{"location":"course_material/day2/2_introduction_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/2_introduction_snakemake/#structuring-a-workflow","title":"Structuring a workflow","text":"<p>It is advised to implement your code in a directory called <code>workflow</code> (you will learn more about workflow structure in the next series of exercises). Filenames and locations are up to you, but we recommend that you at least group all workflow outputs in a <code>results</code> folder.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#exercises","title":"Exercises","text":"<p>This series of exercises will bear no biological meaning, on purpose: it is designed to explain the fundamentals of Snakemake.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#creating-a-basic-rule","title":"Creating a basic rule","text":"<p>A rule is the smallest block of code with which you can build a workflow. It is a set of instructions to create one or more output(s) from zero or more input(s). When a rule is executed (in other words, applied to specific input/output file(s)), it is called a job. The definition of a rule always starts with the keyword <code>rule</code>. Similarly to Python classes and their attributes, rules have directives, which contain information about their properties.</p> <p>To create the simplest rule possible, you need at least two directives:</p> <ul> <li><code>output</code>: path of the output file</li> <li><code>shell</code>: shell commands that will create the output when they are executed</li> </ul> <p>Other directives will be explained throughout the course.</p> <p>Exercise: The following example shows the minimal syntax to implement a rule. What do you think it does? Does it create a file? If so, how is it called?</p> <pre><code>rule hello_world:\n    output:\n        'results/hello.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/hello.txt'\n</code></pre> Answer <p>This rule uses the <code>echo</code> shell command to print <code>Hello world!</code> in an output file called <code>hello.txt</code>, located in the <code>results</code> folder.</p> <p>Rules are defined and written in a file called Snakefile (note the capital <code>S</code> and the absence of extension in the filename). This file should be located at the workflow root directory (here, <code>workflow/Snakefile</code>).</p>"},{"location":"course_material/day2/2_introduction_snakemake/#executing-a-workflow-with-a-specific-output","title":"Executing a workflow with a specific output","text":"<p>It is now time to execute your first workflow! To do this, you need to tell Snakemake what is your target, i.e. what is the specific output that you want to generate. A target can be any output from any rule in the workflow.</p> <p>Exercise: Create a Snakefile and copy the previous rule in it. Then, execute the workflow with <code>snakemake -c 1 &lt;target&gt;</code>. What value should you use for <code>&lt;target&gt;</code>? Once Snakemake execution is finished, can you locate the output file?</p> What does <code>-c/--cores</code> do? <p>The <code>-c/--cores N</code> parameter controls the maximum number of CPU cores used in parallel. If N is omitted or \u2018all\u2019, Snakemake will use all available CPU cores, which is useful but can also be dangerous on a cluster or a local machine. In case of cluster/cloud execution, this argument sets the maximum number of cores requested from the cluster or cloud scheduler.</p> Code indentation in Snakemake <p>As Snakemake is built on top of Python, proper code indentation is crucial. Wrong indentation often results in cryptic errors. We recommend using indents of 4 spaces, but here are two rules that should be followed at all times:</p> <ul> <li>Do not mix space and tab indents in a file</li> <li>Always use the same indent length</li> </ul> Answer <ul> <li>The target value is the file you want to generate, here <code>results/hello.txt</code>. The command to execute the workflow is: <pre><code>snakemake -c 1 results/hello.txt\n</code></pre></li> <li>The output is located in the <code>results</code> folder. You can check the folder content with <code>ls -alh results/</code></li> <li>You can check the output content with <code>cat results/hello.txt</code></li> </ul> <p>During the workflow execution, Snakemake automatically created the missing folder of the output path, <code>results/</code>. If several nested folders are missing (for example, <code>test1/test2/test3/hello.txt</code>), Snakemake will create the entire folder structure (<code>test1/test2/test3/</code>).</p> <p>Exercise: Re-run the exact same command. What happens?</p> Answer <p>Nothing! You get a message saying that Snakemake did not run anything:</p> <p><pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre> This is normal, because the desired output is already present and accounted for!</p> Snakemake re-run policy <p>By default, Snakemake runs a job if:</p> <ul> <li>A target file explicitly requested in the <code>snakemake</code> command is missing or an intermediate file is missing and is required to produce a target file</li> <li>It detects input files that have been modified more recently than output files, based on their modification dates. In this case, Snakemake will re-generate existing outputs</li> <li>Code (including <code>params</code> directive, see here for more information) has changed since last workflow execution</li> <li>Computing environment has changed since last workflow execution</li> </ul> <p>Snakemake re-runs can be forced:</p> <ul> <li>For a specific rule using the <code>-R/--forcerun</code> parameter: <code>snakemake -c 1 -R &lt;rule_name&gt;</code></li> <li>For a specific target using the <code>-f/--force</code> parameter: <code>snakemake -c 1 -f &lt;target&gt;</code></li> <li>For all workflow outputs using the <code>-F/--forceall</code> parameter: <code>snakemake -c 1 -F</code></li> </ul> <p>In practice, Snakemake re-run policy can be altered, but we will not cover this topic in the course (see \u2013rerun-triggers parameter in Snakemake CLI help and this git issue for more information).</p> <p>In the previous rule, the values of the two directives are strings. In the <code>shell</code> directive (other types of values will be seen later in the course), long strings (which includes software commands) can be written on multiple lines for clarity by encasing each line in quotes:</p> <pre><code>rule long_message:\n    output:\n        'results/long_message.txt'\n    shell:\n        'echo \"I want to print a very very very very very '\n        'very very very long string in my output\" &gt; results/long_message.txt'\n</code></pre> <p>Here, Snakemake will concatenate the two lines (i.e. paste the two lines together) and execute the resulting command: <pre><code>echo \"I want to print a very very very very very very very very very very long string in my output\" &gt; results/long_message.txt\n</code></pre></p>"},{"location":"course_material/day2/2_introduction_snakemake/#understanding-the-input-directive","title":"Understanding the input directive","text":"<p>Another directive used by most rules is <code>input</code>. It usually indicates a path to a file required by the rule to create the output. In the following example, we wrote a rule that uses the file <code>results/hello.txt</code> as an input, and copies its content to <code>results/copied_file.txt</code>:</p> <pre><code>rule copy_file:\n    input:\n        'results/hello.txt'\n    output:\n        'results/copied_file.txt'\n    shell:\n        'cp results/hello.txt results/copied_file.txt'\n</code></pre> <p>You will use the <code>input</code> directive in the next exercises.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#creating-a-workflow-with-several-rules","title":"Creating a workflow with several rules","text":"<p>As you may have guessed from the previous rule, the <code>input</code> and <code>output</code> directives allow us to create links (also called dependencies) between rules and files. Here, the <code>input</code> of rule <code>copy_file</code> requires the <code>output</code> of rule <code>hello_world</code>. In other terms, this is\u2026 a workflow! Let\u2019s build one with two rules and run it!</p>"},{"location":"course_material/day2/2_introduction_snakemake/#rule-order-matters","title":"Rule order matters!","text":"<p>Exercise: Add the rule <code>copy_file</code> to your Snakefile, after rule <code>hello_world</code>. Then, run the workflow without specifying an output with <code>snakemake -c 1</code>. What happens?</p> Your Snakefile should look like this <pre><code>rule hello_world:\n    output:\n        'results/hello.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/hello.txt'\n\nrule copy_file:\n    input:\n        'results/hello.txt'\n    output:\n        'results/copied_file.txt'\n    shell:\n        'cp results/hello.txt results/copied_file.txt'\n</code></pre> Answer <p>Nothing! You get the same message as before, saying that Snakemake did not run anything:</p> <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre> <p>When you do not specify a target, the one selected by default is the output of the first rule in the Snakefile, here <code>results/hello.txt</code> of rule <code>hello_world</code>. While this behaviour may seem weird, it will prove very useful later! In this case, <code>results/hello.txt</code> already exists from your previous runs, so Snakemake doesn\u2019t recompute anything.</p> <p>Let\u2019s try to better understand how rule dependencies work in Snakemake.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#chaining-rules","title":"Chaining rules","text":"<p>The execution principle behind Snakemake is to create a Directed Acyclic Graph (DAG) that defines dependencies between all inputs and outputs of the workflow. Starting from jobs generating the final desired outputs, Snakemake checks whether required inputs exist. If they do not, it looks for a rule that can generate these inputs and so on until all dependencies are resolved. This is why Snakemake is said to have a \u2018bottom-up\u2019 approach: it starts from last outputs and go back to first inputs.</p> <code>MissingInputException</code> <p><code>MissingInputException</code> is a common error in Snakemake. It means that Snakemake couldn\u2019t find a way to generate targets during DAG computation because an input file is missing. This is a case of broken dependency between rules. This error is often caused by typos in input or output paths (for example, output of rule <code>hello_world</code> not matching input of rule <code>copy_file</code>), so make sure to double-check them!</p> <p>Exercise: With this in mind, identify the target you need to use to trigger the execution of rule <code>copy_file</code>. Add the <code>-F</code> parameter to the <code>snakemake</code> command and execute the workflow. What do you see?</p> What do we use <code>-F/--forceall</code> here? <p>The <code>-F/--forceall</code> parameter forces the re-creation of all workflow outputs. It is used here to avoid manually removing files, but it should be used carefully, especially with large workflows which contains a lot of outputs.</p> Answer <ul> <li>To trigger the execution of the second rule, you need to use <code>results/copied_file.txt</code> as target. The command is: <pre><code>snakemake -c 1 -F results/copied_file.txt\n</code></pre></li> <li>You should now see Snakemake execute two rules and produce both targets/outputs: to generate output <code>results/copied_file.txt</code>, Snakemake requires input <code>results/hello.txt</code>. Before the workflow is executed, this file does not exist, therefore, Snakemake looks for a rule that generates <code>results/hello.txt</code>, here rule <code>hello_world</code>. The process is then repeated for <code>hello_world</code>. In this case, the rule does not require any input, so all dependencies are resolved, and Snakemake can generate the DAG</li> </ul> <p>While it is possible to pass a space-separated list of targets in a Snakemake command, writing all the intermediary outputs does not look like a good idea: it is very time-consuming, error-prone\u2026 and annoying! Imagine what would happen with a workflow generating hundreds of files?! Using rule dependencies effectively solve this problem: you only need to ask Snakemake for the final outputs, and it will create the necessary intermediary outputs by itself!</p>"},{"location":"course_material/day2/2_introduction_snakemake/#rule-dependencies-can-be-easier-to-write","title":"Rule dependencies can be easier to write","text":"<p>Creating rule dependencies using long file paths can be cumbersome, especially when you are dealing with a large number of files/rules. But there is a dedicated Snakemake syntax that makes this process easier to set-up: it is possible (and recommended!) to refer to the output of a rule in another rule with the following syntax: <code>rules.&lt;rule_name&gt;.output</code>. It has several advantages, among which:</p> <ul> <li>It limits the risk of error because you do not have to write filenames in several locations</li> <li>Changes in the output name are automatically propagated to rules that use it, which means that you only need to change the name once, in the rule that defines it</li> <li>It makes the code much clearer and easier to understand: with this syntax, you instantly know the object type (a <code>rule</code>), how/where it is created (<code>hello_world</code>), and what it is (an <code>output</code>)</li> </ul> Rules must produce unique outputs <p>Because of rule dependency, it is mandatory that an output be generated by a single rule. Rules generating the same output are called ambiguous. When Snakemake encounters ambiguous rules, it is not able to decide -at least by itself- which rule to use to generate this output, so it stops the execution. In reality, there are solutions to deal with ambiguous rules, but they should be avoided as much as possible, so we will not cover them in this course. See the official documentation for more information).</p> To quote or not to quote? <p>As opposed to strings, like <code>'results/hello.txt'</code>, quotes are not required around <code>rules.&lt;rule_name&gt;.output</code> statements, because they are Snakemake objects.</p> <p>The following example implements this syntax for the two rules defined above:</p> <pre><code>rule hello_world:\n    output:\n        'results/hello.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/hello.txt'\n\nrule copy_file:\n    input:\n        rules.hello_world.output  # Dependency syntax\n    output:\n        'results/copied_file.txt'\n    shell:\n        'cp results/hello.txt results/copied_file.txt'\n</code></pre> <p>Try to use this syntax as much as possible in the next series of exercises!</p>"},{"location":"course_material/day2/3_generalising_snakemake/","title":"Making a more general-purpose Snakemake workflow","text":""},{"location":"course_material/day2/3_generalising_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Create rules with multiple inputs and outputs</li> <li>Make the code shorter and more general by using placeholders and <code>wildcards</code></li> <li>Visualise a workflow DAG</li> <li>(Check a workflow\u2019s behaviour)</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/3_generalising_snakemake/#advice-and-reminders","title":"Advice and reminders","text":"<p>In each rule, you should try (as much as possible) to:</p> <ul> <li>Choose meaningful rule names</li> <li>Use placeholders and <code>wildcards</code><ul> <li>Choose meaningful wildcard names</li> <li>You can use the same wildcard names in multiple rules for consistency and readability, but remember that Snakemake will treat them as independent <code>wildcards</code> and their values will not be shared: rules are self-contained and <code>wildcards</code> are local to each rule (see this very nice summary on <code>wildcards</code>)</li> </ul> </li> <li>Use multiple (named) inputs/outputs when needed/possible</li> <li>Use rules dependency, with the syntax <code>rules.&lt;rule_name&gt;.output</code><ul> <li>If you use named outputs (recommended), the syntax becomes <code>rules.&lt;rule_name&gt;.output.&lt;output_name&gt;</code></li> <li>If you use numbered outputs (don\u2019t), the syntax becomes <code>rules.&lt;rule_name&gt;.output[n]</code>, with <code>n</code> starting at 0 (Python indexing)</li> </ul> </li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#testing-your-workflows-logic","title":"Testing your workflow\u2019s logic","text":"<ul> <li>If you have a doubt, do not hesitate to test your workflow logic with a dry-run (<code>-n/--dry-run/--dryrun</code> parameter): <code>snakemake -c 1 -n &lt;target&gt;</code>. Snakemake will then show all the jobs needed to generate <code>&lt;target&gt;</code> as well as a reason field explaining why each job is required</li> <li>To visualise the exact commands executed by each job (with placeholders and <code>wildcards</code> replaced by their values), run snakemake with the <code>-p/--printshellcmds</code> parameter: <code>snakemake -c 1 -p &lt;target&gt;</code></li> <li>These two parameters are often used together to check an entire workflow: <code>snakemake -c 1 -n -p &lt;target&gt;</code></li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#data-origin","title":"Data origin","text":"<p>The data you will use during the exercises was produced in this work. Briefly, the team studied the transcriptional response of a strain of baker\u2019s yeast, Saccharomyces cerevisiae, facing environments with different concentrations of CO<sub>2</sub>. To this end, they performed 150 bp paired-end sequencing of mRNA-enriched samples. Detailed information on all the samples are available here, but just know that for the purpose of the course, we selected 6 samples (3 replicates per condition, low and high CO<sub>2</sub>) and down-sampled them to 1 million read pairs each to reduce computation time.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#exercises","title":"Exercises","text":"<p>One of the aims of today\u2019s course is to develop a simple, yet efficient, workflow to analyse bulk RNAseq data. This workflow takes reads coming from RNA sequencing as inputs and produces a list of genes that are differentially expressed between two conditions. The files containing reads are in FASTQ format and the final output will be a tab-separated file containing a list of genes with expression changes, results of statistical tests\u2026</p> <p>In this series of exercises, you will create the workflow \u2018backbone\u2019, i.e. rules that are the most computationally expensive, namely:</p> <ul> <li>A rule to trim poor-quality reads</li> <li>A rule to map trimmed reads on a reference genome</li> <li>A rule to convert and sort files from SAM format to BAM format</li> <li>A rule to count reads mapping on each gene</li> </ul> Designing and debugging a workflow <p>If you have problems designing your Snakemake workflow or debugging it, you can find some help here.</p> <p>At the end of this series of exercises, your workflow should look like this:</p> <p> </p> Workflow rulegraph at the end of the session"},{"location":"course_material/day2/3_generalising_snakemake/#downloading-data-and-setting-up-folder-structure","title":"Downloading data and setting up folder structure","text":"<p>In this part, you will download the data and start building the directory structure of your workflow according to the official recommendations. You already starting doing so in the previous series of exercises and at the end of the course, it should resemble this: <pre><code>\u2502\u2500\u2500 .gitignore\n\u2502\u2500\u2500 README.md\n\u2502\u2500\u2500 LICENSE.md\n\u2502\u2500\u2500 benchmarks\n\u2502   \u2502\u2500\u2500 sample1.txt\n\u2502   \u2514\u2500\u2500 sample2.txt\n\u2502\u2500\u2500 config\n\u2502   \u2502\u2500\u2500 config.yaml\n\u2502   \u2514\u2500\u2500 some-sheet.tsv\n\u2502\u2500\u2500 data\n\u2502   \u2502\u2500\u2500 sample1.fastq\n\u2502   \u2514\u2500\u2500 sample2.fastq\n\u2502\u2500\u2500 images\n\u2502   \u2502\u2500\u2500 dag.png\n\u2502   \u2502\u2500\u2500 filegraph.png\n\u2502   \u2514\u2500\u2500 rulegraph.png\n\u2502\u2500\u2500 logs\n\u2502   \u2502\u2500\u2500 sample1.log\n\u2502   \u2514\u2500\u2500 sample2.log\n\u2502\u2500\u2500 results\n\u2502   \u2502\u2500\u2500 DEG_list.tsv\n\u2502   \u2502\u2500\u2500 sample1\n\u2502   \u2502   \u2514\u2500\u2500 sample1.bam\n\u2502   \u2514\u2500\u2500 sample2\n\u2502       \u2514\u2500\u2500 sample2.bam\n\u2502\u2500\u2500 resources\n\u2502   \u2502\u2500\u2500 Scerevisiae.fasta\n\u2502   \u2502\u2500\u2500 Scerevisiae.gtf\n\u2502   \u2514\u2500\u2500 genome_indices\n|       \u2502\u2500\u2500 Scerevisiae_index.1.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.2.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.3.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.4.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.5.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.6.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.7.ht2\n|       \u2514\u2500\u2500 Scerevisiae_index.8.ht2\n\u2514\u2500\u2500 workflow\n    \u2502\u2500\u2500 Snakefile\n    \u2502\u2500\u2500 envs\n    \u2502   \u2502\u2500\u2500 tool1.yaml\n    \u2502   \u2514\u2500\u2500 tool2.yaml\n    \u2502\u2500\u2500 rules\n    \u2502   \u2502\u2500\u2500 module1.smk\n    \u2502   \u2514\u2500\u2500 module2.smk\n    \u2514\u2500\u2500 scripts\n        \u2502\u2500\u2500 script1.py\n        \u2514\u2500\u2500 script2.R\n</code></pre></p> <p>For now, the main thing to remember is that code should go into the <code>workflow</code> subfolder  and the rest is mostly input/output files. The only exception is the <code>config</code> subfolder, but it will be explained later. All output files generated in the workflow should be stored under <code>results/</code>.</p> <p>Let\u2019s download the data, uncompress them and build the first part of the directory structure.</p> <pre><code>ssh -i ~/.ssh/key_username.pem username@18.195.170.182  # Connect to server; don't forget to change key_username and username\nwget https://containers-snakemake-training.s3.eu-central-1.amazonaws.com/snakemake_rnaseq.tar.gz  # Download data\ntar -xvf snakemake_rnaseq.tar.gz  # Uncompress archive\nrm snakemake_rnaseq.tar.gz  # Delete archive\ncd snakemake_rnaseq/  # Start developing in new folder\n</code></pre> <p>In <code>snakemake_rnaseq/</code>, you should see two subfolders:</p> <ul> <li><code>data/</code>, which contains data to analyse</li> <li><code>resources/</code>, which contains retrieved resources, here assembly, genome indices and annotation file of S. cerevisiae. It may also contain small resources delivered along with the workflow</li> </ul> <p>We also need to create the other missing subfolders and the Snakefile:</p> <pre><code>mkdir -p config/ images/ workflow/envs workflow/rules workflow/scripts  # Create subfolder structure\ntouch workflow/Snakefile  # Create empty Snakefile\n</code></pre> What does <code>-p</code> do? <p>The <code>-p</code> parameter of <code>mkdir</code> make parent directories as needed and does not return an error if the directory already exists.</p> <p>Snakefile marks the workflow entry point. It will be automatically discovered when running Snakemake from the root folder, here <code>snakemake_rnaseq/</code>.</p> Using a Snakefile from a non-default location <p>Snakemake can use a Snakefile from a non-default location thanks to the <code>-s/--snakefile</code> parameter: <pre><code>snakemake -c 1 -s &lt;Snakefile_path&gt; &lt;target&gt;\n</code></pre></p> <p>However, it is highly discouraged as it hampers reproducibility.</p> Relative paths in Snakemake <p>All paths in a Snakefile are relative to the working directory in which the <code>snakemake</code> command is executed:</p> <ul> <li>If you execute Snakemake in <code>snakemake_rnaseq/</code>, the relative path to the input files in the rule is <code>data/&lt;sample&gt;.fastq</code></li> <li>If you execute Snakemake in <code>snakemake_rnaseq/workflow/</code>, the relative path to the input files in the rule is <code>../data/&lt;sample&gt;.fastq</code></li> </ul> <p>If you followed the advice at the top of this page, Snakemake should create all the other missing folders by itself, so it is time to create the rules mentioned earlier. If needed, you can check here for a few pieces of advice on workflow design.</p> \u2018bottom-up\u2019 or \u2018top-down\u2019 development? <p>Even if it is often easier to start from final outputs and work backwards to first inputs, the next exercises are presented in the opposite direction (first inputs to last outputs) to make the session easier to understand.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#important-do-not-process-all-the-samples","title":"Important: do not process all the samples!","text":"<p>Do not try to process all the samples yet, even if we asked you to use wildcards. For now, choose only one sample (which means two .fastq files because reads are paired-end). You will see an efficient way to process a list of files in the next series of exercises.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-trim-reads","title":"Creating a rule to trim reads","text":"<p>Usually, when dealing with sequencing data, the first step is to improve read quality by removing low quality bases, stretches of As and Ns and reads that are too short.</p> Trimming sequencing adapters <p>In theory, trimming should also remove sequencing adapters, but you will not do it here to keep computation time low and avoid parsing other files to extract adapter sequences.</p> <p>You will use atropos to trim reads. The first part of the trimming command is: <pre><code>atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\"\n</code></pre></p> Explanation of atropos parameters <ul> <li><code>-q 20,20</code>: trim low-quality bases from 5\u2019 and 3\u2019 ends of each read before adapter removal</li> <li><code>--minimum-length 25</code>: discard trimmed reads that are shorter than 25 bp</li> <li><code>--trim-n</code>: trim Ns at the ends of reads</li> <li><code>--preserve-order</code>: preserve order of reads in input files</li> <li><code>--max-n 10</code>: discard reads with more than 10 Ns</li> <li><code>--no-cache-adapters</code>: do not cache adapters list as \u2018.adapters\u2019 in the working directory</li> <li><code>-a \"A{{20}}\" -A \"A{{20}}\"</code>: remove series of 20 As in adapter sequences (<code>-a</code> for first read of the pair, <code>-A</code> for the second one)<ul> <li>The usual command-line syntax is <code>-a \"A{20}\"</code>. Here, brackets were doubled to prevent Snakemake from interpreting <code>{20}</code> as a wildcard</li> </ul> </li> </ul> <p>Now, a few questions might come to mind when you need to use specific software in a workflow:</p> <ul> <li>Is the software already installed in the machine I am working on?</li> <li>If not, how do I install it quickly and easily?</li> <li>How can I make sure that everyone using my workflow has it installed? With the exact same version?!</li> </ul> <p>To solve this problem, Snakemake can use package managers (more on this later) or container managers, like <code>docker</code> and <code>apptainer</code>, to deploy rule-specific environments. The latter is done with the <code>container</code> directive and its value should be the location of the image: it can be either a local path or a remote URL. Allowed URLs are everything supported by <code>apptainer</code>, including <code>shub://</code> and <code>docker://</code>.</p> <p>Exercise:</p> <ul> <li>Complete the atropos command given above with parameters to specify inputs (files to trim) and outputs (trimmed files)<ul> <li>You can find information on how to use <code>atropos</code> and its parameters with <code>atropos trim -h</code> or you can look at the tip below</li> </ul> </li> <li>Implement a rule containing your command to trim reads contained in a .fastq files<ul> <li>You will need a rule name, and the <code>input</code>, <code>output</code>, <code>container</code> and <code>shell</code> directives</li> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2</code></li> </ul> </li> </ul> atropos inputs and outputs <ul> <li>.fastq files to trim are located in <code>data/</code></li> <li>Paths of files to trim (i.e. input files, in FASTQ format) are specified with the parameters <code>-pe1</code> (first read) and <code>-pe2</code> (second read)</li> <li>Paths of trimmed files (i.e. output files, also in FASTQ format) are specified with the parameters <code>-o</code> (first read) and <code>-p</code> (second read)</li> </ul> Answer <p>This is one way of writing this rule, but definitely not the only way (this is true for all the rules presented in these exercises): <pre><code>rule fastq_trim:\n    \"\"\"\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    \"\"\"\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    container:\n        'https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2'\n    shell:\n        '''\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2}\n        '''\n</code></pre></p> <p>There are three interesting things happening here:</p> <ol> <li>We added a comment between double quotes under the rule name (L2-7). In Python, it is called a docstring. While it is not mandatory, it is good practice (and very recommended) to write docstrings to explain what a rule does, what are its inputs, outputs, parameters\u2026</li> <li>We used the <code>{sample}</code> wildcards twice in output paths (L12-13). This is because we prefer to have all files from a single sample in the same directory</li> <li>We used a backslash <code>\\</code> at the end of L18-19 to split a very long command in smaller lines. This is purely \u2018cosmetic\u2019 but it avoids very long lines that are painful to read, understand and debug\u2026</li> </ol> <p>Exercise: If you had to run the workflow by specifying only one output, what command would you use?</p> Answer <p>To process the sample <code>highCO2_sample1</code>, for example, you would use: <pre><code>snakemake -c 1 -p --sdm=apptainer results/highCO2_sample1/highCO2_sample1_atropos_trimmed_1.fastq\n</code></pre></p> <ul> <li>You don\u2019t need to ask for the two outputs of the rule: asking only for one will still trigger execution, but the workflow will complete without errors if and only if both outputs are present. Like with intermediary files, this property also helps reducing the number of targets to write in the snakemake command used to execute the workflow!</li> <li>Do not forget to add <code>--sdm=apptainer</code>, otherwise Snakemake will not pull the image and the command will be executed in the default environment (which will most likely lead to a crash). Don\u2019t worry if the first execution is somewhat slow: Snakemake has to download the image. The next ones will be much faster as the images are cached</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-map-trimmed-reads-onto-a-reference-genome","title":"Creating a rule to map trimmed reads onto a reference genome","text":"<p>Once the reads are trimmed, the next step is to map those reads onto the species genome (S. cerevisiae strain S288C) to eventually obtain read counts. The reference assembly used in this exercise is RefSeq GCF_000146045.2 and was retrieved via the NCBI website. You will use HISAT2 to map reads.</p> HISAT2 genome index <p>To align reads to a genome, HISAT2 relies on a graph-based index. To save some time, we built the genome index for you, using: <pre><code>hisat2-build -p 24 -f resources/Scerevisiae.fasta resources/genome_indices/Scerevisiae_index\n</code></pre> The parameters are:</p> <ul> <li><code>-p</code>: number of threads to use</li> <li><code>-f</code>: genomic sequence in FASTA format</li> <li><code>Scerevisiae_index</code>: global name shared by all the index files</li> </ul> <p>Genome indices can be found in <code>resources/genome_indices/</code>.</p> <p>The first part of the mapping command is: <pre><code>hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal\n</code></pre></p> Explanation of HISAT2 parameters <ul> <li><code>--dta</code>: report alignments tailored for transcript assemblers</li> <li><code>--fr</code>: set alignment of -1, -2 mates to forward/reverse (position of reads in a pair relatively to each other)</li> <li><code>--no-mixed</code>: remove unpaired alignments for paired reads</li> <li><code>--no-discordant</code>: remove discordant alignments for paired reads</li> <li><code>--time</code>: print wall-clock time taken by search phases</li> <li><code>--new-summary</code>: print alignment summary in a new style</li> <li><code>--no-unal</code>: suppress SAM records for reads that failed to align</li> </ul> <p>Exercise:</p> <ul> <li>Complete the HISAT2 command given above with parameters to specify inputs and outputs<ul> <li>You will need 2 inputs, 2 outputs (in 2 different formats) and the genome indices mentioned above (should they be considered as inputs?)<ul> <li>You can find more information on how to use HISAT2 and its parameters with <code>hisat2 -h</code> or you can look at the tip below</li> </ul> </li> </ul> </li> <li>Implement a rule containing your command to map trimmed reads contained in .fastq files<ul> <li>You will need a rule name, and the <code>input</code>, <code>output</code>, <code>container</code> and <code>shell</code> directives</li> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6</code></li> </ul> </li> </ul> HISAT2 inputs and outputs <ul> <li>Paths of trimmed files (i.e. input files) are specified with the parameters <code>-1</code> (first read) and <code>-2</code> (second read)</li> <li>Basename of genome indices (binary format) is specified with the parameter <code>-x</code>. The files have a shared name of <code>resources/genome_indices/Scerevisiae_index</code>, which is the value you need to use for <code>-x</code></li> <li>Path of mapped reads files (i.e. output file, in SAM format) is specified with the parameter <code>-S</code> (do not forget the .sam extension at the end of the filename)</li> <li>Path of mapping report (i.e. output file, in text format) is specified with the parameter <code>--summary-file</code></li> </ul> Answer <pre><code>rule read_mapping:\n    \"\"\"\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    \"\"\"\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x resources/genome_indices/Scerevisiae_index \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report}\n        '''\n</code></pre> <p>Exercise: What do you think about the value of <code>-x</code>?</p> Answer <p>Something very interesting is happening here: to run, HISAT2 requires several genome index files. As such, they could (should?) be considered as inputs\u2026 and this is a problem:</p> <ul> <li>On one hand, the <code>-x</code> parameter only accepts strings containing a file path and common name. This means that if you were to manually add all the indices as inputs, HISAT2 would not recognize them\u2026 and crash!</li> <li>On the other hand, if you were add the value of <code>-x</code> as input, Snakemake would look for a file called <code>resources/genome_indices/Scerevisiae_index</code>\u2026 and crash because this file doesn\u2019t exist!</li> </ul> <p>This highlights the fact that inputs must be files and this is why we directly added the value of <code>-x</code> in the command. However, this is not very convenient: you will see later a better way to deal with this problem.</p> <p>Using the same sample as before (<code>highCO2_sample1</code>), the workflow can be run with: <pre><code>snakemake -c 1 --sdm=apptainer -p results/highCO2_sample1/highCO2_sample1_mapped_reads.sam\n</code></pre></p> <p>That being said, we recommend not to run it for the moment, because this step is the longest of the workflow (with current settings, it will take ~6 min to complete). Still, if you want to run it now, you can, but you should launch it and start working on the next rules while it finishes.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-convert-and-sort-sam-files-to-bam-format","title":"Creating a rule to convert and sort .sam files to BAM format","text":"<p>HISAT2 only outputs mapped reads in SAM format. However, most downstream analysis tools use BAM format, which is the compressed binary version of SAM format and, as such, is much smaller, easier to manipulate and transfer and allows a faster data retrieval. Additionally, many analyses require .bam files to be sorted by genomic coordinates and indexed because sorted .bam files can be processed much more easily and quickly than unsorted ones. Operations on .sam and .bam files are usually performed with Samtools.</p> What are alignment formats? <p>More information on alignment formats can be found on samtools github repository.</p> <p>We wrote a single rule performing all these operations for you:</p> <pre><code>rule sam_to_bam:\n    \"\"\"\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    \"\"\"\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    container:\n        'https://depot.galaxyproject.org/singularity/samtools%3A1.21--h50ea8bc_0'\n    shell:\n        '''\n        samtools view {input.sam} -b -o {output.bam}\n        samtools sort {output.bam} -O bam -o {output.bam_sorted}\n        samtools index -b {output.bam_sorted} -o {output.index}\n        '''\n</code></pre> Explanation of Samtools parameters <ul> <li>You can find information on how to use Samtools and its parameters with <code>samtools --help</code></li> <li><code>samtools view</code>:<ul> <li><code>-b</code>: create an output in BAM format</li> <li><code>-o</code>: path of output file</li> </ul> </li> <li><code>samtools sort</code>:<ul> <li><code>-O bam</code>: create an output in BAM format</li> <li><code>-o</code>: path of output file</li> </ul> </li> <li><code>samtools index</code>:<ul> <li><code>-b</code>: create an index in BAI format</li> </ul> </li> </ul> <p>Exercise: Copy this rule in your Snakefile. Don\u2019t forget to update the output values to match the ones you used in your previous rules. Do you notice anything surprising in this rule?</p> Answer <p>Let\u2019s start with a quick breakdown of the <code>shell</code> directive:</p> <ul> <li>L15: <code>samtools view</code> converts a file in SAM format to BAM format</li> <li>L16: <code>samtools sort</code> sorts a .bam file by genomic coordinates</li> <li>L17: <code>samtools index</code> indexes a sorted .bam file. The index must have the exact same basename as its associated .bam file; the only difference is that it finishes with the extension <code>.bam.bai</code> instead of <code>.bam</code></li> </ul> <p>The interesting thing is that so far, all the rules had only one command in the <code>shell</code> directive. In this rule, there are three commands grouped together, each with their own inputs and outputs. This means two things:</p> <ol> <li>We could have split this rule into 3 separate rules with dependencies. There is no official guideline on whether to split rules like this, but a good rule of thumb is: does it make sense to run these commands together? Is it not too computationally expensive (time, memory, CPU) to run these rules together?</li> <li><code>{output.bam}</code> and <code>{output.bam_sorted}</code> are outputs of  <code>samtools view</code> and  <code>samtools sort</code>\u2026 But they are also inputs of  <code>samtools sort</code> and  <code>samtools index</code>! This means that files that are created by a command can instantly be re-used in subsequent commands within a same rule!</li> </ol> <p>Exercise: Do you see any drawbacks to using a rule like this?</p> Answer <p>When Snakemake has a problem and crashes, it removes the current rule outputs to avoid further computation with corrupted files. This means that if the first two commands complete but the last one (here, <code>samtools index</code>) fails and doesn\u2019t produce the expected output, Snakemake will remove all the outputs, including those that were created without error (here, <code>{output.bam}</code> and <code>{output.bam_sorted}</code>), causing a waste of time and money</p> <p>Using the same sample as before (<code>highCO2_sample1</code>), the workflow can be run with <pre><code>snakemake -c 1 -p --sdm=apptainer results/highCO2_sample1/highCO2_sample1_mapped_reads_sorted.bam\n</code></pre> However, you will soon run the entire workflow, so it might be worth waiting!</p>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-count-mapped-reads","title":"Creating a rule to count mapped reads","text":"<p>Most of analyses happening downstream the alignment step, including Differential Expression Analyses, are starting off read counts, by exon or by gene. However, you are still missing those counts!</p> Genome annotations <ul> <li>To count reads mapping on genomic features, we first need a definition of those features, called annotations. Here, we chose one of the best-known model organism, S. cerevisiae, which has been annotated for a long time. Its annotations are easily findable on the NCBI or the Saccharomyces Genome Database. If an organism has not been annotated yet, there are ways to work around this problem, but this is an entirely different field that we won\u2019t discuss here!</li> <li>You should also know that there are two main annotations format: GTF and GFF. Former is lighter and easier to work with, so this is what you will use</li> </ul> Chromosome names must match between files <p>If you are working with genome sequences and annotations from different sources, remember that they must contain matching chromosome names, otherwise counting will not work, as the counting software will not be able to read counts to match exon/gene locations.</p> <p>We already wrote a rule to count reads mapping on each gene of the S. cerevisiae genome using featureCounts:</p> <pre><code>rule reads_quantification_genes:\n    \"\"\"\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly.\n    \"\"\"\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -o {output.gene_level} {input.bam_once_sorted}\n        mv {output.gene_level}.summary {output.gene_summary}\n        '''\n</code></pre> Explanation of featureCounts command <ul> <li>You can find information on how to use featureCounts and its parameters with <code>featureCounts -h</code></li> <li><code>-t</code>: specify on which feature type to count reads</li> <li><code>-g</code>: specify if and how to gather feature counts. Here, reads are counted by features (exon) (<code>-t</code>) and exon counts are gathered by \u2018meta-features\u2019 (genes) (<code>-g</code>)</li> <li><code>-s</code>: perform strand-specific read counting<ul> <li>Strandedness is determined by looking at mRNA library preparation kit. It can also be determined a posteriori with scripts such as infer_experiment.py from the RSeQC package</li> </ul> </li> <li><code>-p</code>: specify that input data contain paired-end reads</li> <li><code>--countReadPairs</code>: count read pairs instead of reads</li> <li><code>-B</code>: only count read pairs that have both ends aligned</li> <li><code>-C</code>: do not count read pairs that have their two ends mapping to different chromosomes or mapping on same chromosome but on different strands</li> <li><code>--largestOverlap</code>: assign reads to meta-feature/feature that has largest number of overlapping bases</li> <li><code>--verbose</code>: output verbose information, such as unmatched chromosome/contig names</li> <li><code>-F</code>: specify format of annotation file</li> <li><code>-a</code>: specify path of file containing annotations (i.e. input files, in GTF format)</li> <li><code>-o</code>: specify path of file containing count results (i.e. output file, in tsv format)</li> <li>Paths of sorted .bam file(s) (i.e. input file(s)) are not specified with an parameter, they are simply added at the end of the command</li> </ul> <p>Exercise: Copy this rule in your Snakefile. What does L18 do? Why did we add it?</p> Answer <p>The <code>mv</code> command can be used to move or rename a file. Here, it does the latter. featureCounts outputs a second, separate file (in tsv format) containing summary statistics about read counting, with the name <code>&lt;output_name&gt;.summary</code>. For example, if the output is <code>test.tsv</code>, summary will be printed in <code>test.tsv.summary</code>. However, there is no parameter available to choose the filename, so if we need this file as an output, we have to manually rename it.</p> <p>It would be interesting to know what is happening when featureCounts runs. This is where the <code>log</code> and <code>benchmark</code> directives come into play!</p> <p>(Optional) Exercise: If you have time, add the <code>log</code> and <code>benchmark</code> directives to the rule. Don\u2019t forget to update the directive values to match the ones you used in your previous rules. You can check out slides 29-35 of the presentation (available here) for information on those directives.</p> Logs and benchmarks <ul> <li><code>log</code> and <code>benchmark</code> directives must contain the same <code>wildcards</code> as the <code>output</code> directive, here <code>sample</code></li> <li>Logs need to be handled manually, so you need to redirect what is produced by featureCounts to the log file. You can redirect both <code>stdout</code> and <code>stderr</code> streams with <code>&amp;&gt; {log}</code> at the end of the command</li> </ul> Answer <pre><code>rule reads_quantification_genes:\n    \"\"\"\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly.\n    \"\"\"\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:  # log directive\n        'logs/{sample}/{sample}_genes_read_quantification.log'  # Path of log file\n    benchmark:  # benchmark directive\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt'  # Path of benchmark file\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        '''\n</code></pre>"},{"location":"course_material/day2/3_generalising_snakemake/#running-the-whole-workflow","title":"Running the whole workflow","text":"<p>Exercise: If you have not done it after each step, run the entire workflow on your sample of choice. What command will you use to run it? Once Snakemake has finished, check the log of rule <code>reads_quantification_genes</code>. Does it contain anything? How many read pairs were assigned to a feature?</p> Answer <p>Because all rules are chained together, you only need to specify one final output to trigger the execution of all previous rules. Using the same sample as before (<code>highCO2_sample1</code>). You can add the <code>-F</code> parameter to force an entire re-run, which should take ~10 min.: <pre><code>snakemake -c 1 -F --sdm=apptainer -p results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n</code></pre> If you used the same path for the log file as the rule given above, you can check it with: <pre><code>cat logs/highCO2_sample1/highCO2_sample1_genes_read_quantification.log\n</code></pre> In the log files, you can find a summary of the parameters used by <code>featureCounts</code>, its inputs and outputs\u2026 and the number of read pairs successfully assigned to a gene; with <code>highCO2_sample1</code>, 817894 read pairs (83.8% of total) were assigned: <pre><code>        ==========     _____ _    _ ____  _____  ______          _____\n        =====         / ____| |  | |  _ \\|  __ \\|  ____|   /\\   |  __ \\\n          =====      | (___ | |  | | |_) | |__) | |__     /  \\  | |  | |\n            ====      \\___ \\| |  | |  _ &lt;|  _  /|  __|   / /\\ \\ | |  | |\n              ====    ____) | |__| | |_) | | \\ \\| |____ / ____ \\| |__| |\n        ==========   |_____/ \\____/|____/|_|  \\_\\______/_/    \\_\\_____/\n      v2.0.6\n\n//========================== featureCounts setting ===========================\\\\\n||                                                                            ||\n||             Input files : 1 BAM file                                       ||\n||                                                                            ||\n||                           highCO2_sample1_mapped_reads_sorted.bam          ||\n||                                                                            ||\n||             Output file : highCO2_sample1_genes_read_quantification.tsv    ||\n||                 Summary : highCO2_sample1_genes_read_quantification.ts ... ||\n||              Paired-end : yes                                              ||\n||        Count read pairs : yes                                              ||\n||              Annotation : Scerevisiae.gtf (GTF)                            ||\n||      Dir for temp files : results/highCO2_sample1                          ||\n||                                                                            ||\n||                 Threads : 1                                                ||\n||                   Level : meta-feature level                               ||\n||      Multimapping reads : not counted                                      ||\n|| Multi-overlapping reads : not counted                                      ||\n||   Min overlapping bases : 1                                                ||\n||                                                                            ||\n\\\\============================================================================//\n\n//================================= Running ==================================\\\\\n||                                                                            ||\n|| Load annotation file Scerevisiae.gtf ...                                   ||\n||    Features : 7507                                                         ||\n||    Meta-features : 7127                                                    ||\n||    Chromosomes/contigs : 17                                                ||\n||                                                                            ||\n|| Process BAM file highCO2_sample1_mapped_reads_sorted.bam...                ||\n||    Strand specific : reversely stranded                                    ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 975705                                               ||\n||    Successfully assigned alignments : 817894 (83.8%)                       ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Write the final count table.                                               ||\n|| Write the read assignment summary.                                         ||\n||                                                                            ||\n|| Summary of counting results can be found in file \"results/highCO2_sample1  ||\n|| /highCO2_sample1_genes_read_quantification.tsv.summary\"                    ||\n||                                                                            ||\n\\\\============================================================================//\n</code></pre></p> <p>(Optional) Exercise: If you have time, check Snakemake\u2019s log in <code>.snakemake/log/</code>. Is everything as you expected, especially wildcard values, input and output names\u2026?</p> Answer <p>You can check the logs with: <pre><code>cat .snakemake/log/&lt;latest_log&gt;\n</code></pre> This general log says exactly what was run by Snakemake, after placeholders, <code>wildcards</code>\u2026 were replaced by their actual values. It is identical to what appears on your screen when you run Snakemake.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#visualising-the-workflow-dag","title":"Visualising the workflow DAG","text":"<p>You have now implemented and run the main steps of the workflow. It is always a good idea to visualise the whole process to check for errors and inconsistencies. Snakemake\u2019s has a built-in workflow visualisation feature to do this: the <code>--dag</code> parameter, which shows a dependency graph of all the jobs (rules appear once per wildcard value and wildcard value are displayed).</p> <p>Exercise: Visualise the entire workflow\u2019s Directed Acyclic Graph using <code>--dag</code>. Remember that Snakemake prints a DAG in text format, so you need to pipe its results into the <code>dot</code> command to transform it into a picture with <code>| dot -Tpng &gt; &lt;image_path&gt;.png</code>. Do you need to specify a target to the <code>snakemake</code> command?</p> Creating a DAG <ul> <li>Try to follow the official recommendations on workflow structure, which states that images are supposed to go in the <code>images/</code> subfolder<ul> <li><code>images/</code> is not automatically created by Snakemake because it isn\u2019t handled as part of an actual run, so you need to create it beforehand. You did this when you set up the workflow structure</li> </ul> </li> <li>If you already computed all outputs of the workflow, steps in the DAG will have dotted lines. To visualise the DAG before running the workflow, add <code>-F/--forceall</code> to the snakemake command to force the execution of all jobs<ul> <li>You can also use <code>-f &lt;target&gt;</code> to show fewer jobs</li> </ul> </li> </ul> The <code>dot</code> command <ul> <li><code>dot</code> is a part of the graphviz package and is used to draw hierarchical or layered drawings of directed graphs, i.e. graphs in which edges (arrows) have a direction</li> <li><code>-T</code>: choose image format. Available formats are listed here</li> </ul> Answer <p>To run the command without target, you can use: <pre><code>snakemake -c 1 --dag -F | dot -Tpng &gt; images/dag.png\n</code></pre> We added the <code>-F</code> parameter to force Snakemake to compute the entire DAG and ensure all jobs are shown. However, you will get a <code>WorkflowError: Target rules may not contain wildcards.</code> error. This makes sense, because if you don\u2019t give a target to Snakemake, it can\u2019t compute the wildcard values. To run the command using the same sample as before (<code>highCO2_sample1</code>), you can target one of the final outputs of the workflow: <pre><code>snakemake -c 1 --dag -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpng &gt; images/dag.png\n</code></pre></p> <p>You should get the following DAG:  Workflow rulegraph at the end of the session </p> <p>An important thing to remember: <code>--dag</code> implicitly activates <code>--dry-run/--dryrun/-n</code>, which means that no jobs are executed during DAG computation.</p> <p>Snakemake can also create two other graphs:</p> <ul> <li>A rulegraph, created with the <code>--rulegraph</code> parameter: dependency graph of all the rules (rules appear only once)     <pre><code>snakemake -c 1 --rulegraph -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpdf &gt; images/rulegraph.pdf\n</code></pre></li> <li>A filegraph, created with the <code>--filegraph</code> parameter: dependency graph of all the rules with inputs and outputs (rule appears once, <code>wildcards</code> are shown but not replaced)     <pre><code>snakemake -c 1 --filegraph -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tjpg &gt; images/filegraph.jpg\n</code></pre></li> </ul> <p>Here is a comparison of the different workflow graphs:</p> <p> </p> Workflow DAG, rulegraph and filegraph"},{"location":"course_material/day2/4_optimising_snakemake/","title":"Decorating and optimising a Snakemake workflow","text":""},{"location":"course_material/day2/4_optimising_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Use non-file parameters and config files in rules</li> <li>Make a workflow process list of inputs rather than one at a time</li> <li>Modularise a workflow</li> <li>Aggregate final outputs in a target rule</li> <li>(Optimise resource usage in a workflow)</li> <li>(Create rules with non-conventional outputs)</li> </ul>"},{"location":"course_material/day2/4_optimising_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/4_optimising_snakemake/#snakefile-from-previous-session","title":"Snakefile from previous session","text":"<p>If you didn\u2019t finish the previous part or didn\u2019t do the optional exercises, you can restart from a fully commented Snakefile, with log messages and benchmarks implemented in all rules. You can download it here or download it in your current directory with:</p> <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/refs/heads/2024_update/docs/solutions_day2/session2/workflow/Snakefile\n</code></pre>"},{"location":"course_material/day2/4_optimising_snakemake/#exercises","title":"Exercises","text":"<p>This series of exercises focuses on how to improve the workflow that you developed in the previous session. As a result, you will add only one rule to your workflow. But, fear not, it\u2019s a crucial one!</p> Development and back-up <p>During this session, you will modify your Snakefile quite heavily, so it may be a good idea to make back-ups from time to time (with <code>cp</code> or a simple copy/paste) or use a versioning system. As a general rule, if you have a doubt on the code you are developing, do not hesitate to make a back-up beforehand.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#using-non-file-parameters-and-config-files","title":"Using non-file parameters and config files","text":""},{"location":"course_material/day2/4_optimising_snakemake/#non-file-parameters","title":"Non-file parameters","text":"<p>As you have seen, Snakemake execution revolves around input and output files. However, a lot of software also use non-file parameters to run. In the previous presentation and series of exercises, we advocated against using hard-coded file paths. Yet, if you look back at previous rules, you will find two occurrences of this behaviour in <code>shell</code> directives:</p> <ul> <li>In rule <code>read_mapping</code>, the index parameter: <pre><code>-x resources/genome_indices/Scerevisiae_index\n</code></pre></li> <li>In rule <code>reads_quantification_genes</code>, the annotation parameter: <pre><code>-a resources/Scerevisiae.gtf\n</code></pre></li> </ul> <p>This reduces readability and makes it very hard to change the values of these parameters, because this requires to change the <code>shell</code> directive code.</p> <p>The <code>params</code> directive was (partly) designed to solve this problem: it contains parameters and variables that can be accessed in the <code>shell</code> directive. It allows to specify additional non-file parameters instead of hard-coding them into shell commands or using them as inputs/outputs.</p> Main properties of parameters from the <code>params</code> directive <ul> <li>Their values can be of any type (integer, string, list\u2026)</li> <li>Their values can depend on wildcard values and use input functions (explained here). This means that parameters can be changed conditionally, for example depending on the value of a wildcard<ul> <li>In contrast to the <code>input</code> directive, the <code>params</code> directive can take more arguments than only <code>wildcards</code>, namely <code>input</code>, <code>output</code>, <code>threads</code>, and <code>resources</code></li> </ul> </li> <li>Similarly to <code>{input}</code> and <code>{output}</code> placeholders, they can be accessed from within the <code>shell</code> directive with the <code>{params}</code> placeholder</li> <li>Multiple parameters can be defined in a rule (do not forget the comma between each entry!) and they can also be named. While it isn\u2019t mandatory, un-named parameters are not explicit at all, so you should always name your parameters</li> </ul> <p>Here is an example of <code>params</code> utilisation:</p> <pre><code>rule get_header:\n    input:\n        'data/example.txt'\n    output:\n        'results/header.txt'\n    params:\n        lines = 5\n    shell:\n        'head -n {params.lines} {input} &gt; {output}'\n</code></pre> <p>Exercise: Pick one of the two hard-coded paths mentioned earlier and replace it using <code>params</code>.</p> Answer <p>You need to add a <code>params</code> directive to the rule, name the parameter and replace the path by the placeholder in the <code>shell</code> directive. We did this for both rules so that you can check everything. Feel free to copy this in your Snakefile. For clarity, only lines that changed are shown below:</p> <ul> <li> <p><code>read_mapping</code>: <pre><code>params:\n    index = 'resources/genome_indices/Scerevisiae_index'\nshell:\n    'hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n    -x {params.index} --threads {threads} \\  # Parameter was replaced here\n    -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}'\n</code></pre></p> </li> <li> <p><code>reads_quantification_genes</code>: <pre><code>params:\n    annotations = 'resources/Scerevisiae.gtf'\nshell:\n    'featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n    -B -C --largestOverlap --verbose -F GTF \\\n    -a {params.annotations} -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}'  # Parameter was replaced here\n</code></pre></p> </li> </ul> <p>But doing this only shifted the problem: now, hard-coded paths are in <code>params</code> instead of <code>shell</code>. This is better, but not by much! Luckily, there is an even better way to handle parameters: instead of hard-coding parameter values in the Snakefile, Snakemake can use parameters (and values) defined in config files.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#config-files","title":"Config files","text":"<p>Config files are stored in the <code>config</code> subfolder and written in JSON or YAML format. You will use the latter for this course as it is more user-friendly. In .yaml files:</p> <ul> <li>Parameters are defined with the syntax <code>&lt;name&gt;: &lt;value&gt;</code></li> <li>Values can be strings, integers, booleans\u2026<ul> <li>For a complete overview of available value types, see this list</li> </ul> </li> <li>A parameter can have multiple values, each value being on an indented line starting with \u201c-\u201c<ul> <li>These values will be stored in a Python list when Snakemake parses the config file</li> </ul> </li> <li>Parameters can be named and nested to have a hierarchical structure, each sub-parameter and its value being on an indented lines<ul> <li>These parameters will be stored as a dictionary when Snakemake parses the config file</li> </ul> </li> </ul> <p>Config files will be parsed by Snakemake when executing the workflow, and parameters and their values will be stored in a Python dictionary named <code>config</code>. The config file path can be specified in the Snakefile with <code>configfile: &lt;path/to/file.yaml&gt;</code> at the top of the file, or at runtime with the execution parameter <code>--configfile &lt;path/to/file.yaml&gt;</code>.</p> <p>The example below shows a parameter with a single value (<code>lines_number</code>), a parameter with multiple values (<code>samples</code>), and nested parameters (<code>resources</code>):</p> <pre><code>lines_number: 5  # Parameter with single value (string, int, float, bool ...)\nsamples:  # Parameter with multiple values\n    - sample1\n    - sample2\nresources:  # Nested parameters\n    threads: 4\n    memory: 4G\n</code></pre> <p>Then, each parameter can be accessed in Snakemake with:</p> <pre><code>config['lines_number']  # --&gt; 5\nconfig['samples']  # --&gt; ['sample1', 'sample2']  # A list of parameters becomes a list\nconfig['resources']  # --&gt; {'threads': 4, 'memory': '4G'}  # A list of named parameters becomes a dictionary\nconfig['resources']['threads']  # --&gt; 4\n</code></pre> Accessing config values in <code>shell</code> <p>You cannot use values from the <code>config</code> dictionary directly in a <code>shell</code> directive. If you need to access parameter value in <code>shell</code>, first define it in <code>params</code> and assign its value from the dictionary, then use <code>params.&lt;name&gt;</code> in <code>shell</code>.</p> <p>Exercise: Create a config file in YAML format and fill it with variables and values to replace one of the two hard-coded parameters mentioned before. Then replace the hard-coded parameter values by variables from the config file. Finally, add the config file import on top of your Snakefile.</p> Answer <p>First, create an empty config file: <pre><code>touch config/config.yaml  # Create empty config file\n</code></pre></p> <p>Then, fill it with the desired values: <pre><code># Configuration options of RNAseq-analysis workflow\n# Location of genome indices\nindex: 'resources/genome_indices/Scerevisiae_index'\n# Location of annotation file\nannotations: 'resources/Scerevisiae.gtf'\n</code></pre></p> <p>Then, replace the <code>params</code> values in the Snakefile. We did this for both rules so that you can check everything. Feel free to copy this in your Snakefile. For simplicity, only lines that changed are shown below:</p> <ul> <li> <p><code>read_mapping</code>: <pre><code>params:\n    index = config['index']\n</code></pre></p> </li> <li> <p><code>reads_quantification_genes</code>: <pre><code>params:\n    annotations = config['annotations']\n</code></pre></p> </li> </ul> <p>Finally, add the file path on top of the Snakefile: <code>configfile: 'config/config.yaml'</code></p> <p>From now on, if you need to change these values, you can easily do it in the config file instead of modifying the code!</p>"},{"location":"course_material/day2/4_optimising_snakemake/#modularising-a-workflow","title":"Modularising a workflow","text":"<p>If you develop a large workflow, you are bound to encounter some cluttering problems. Have a look at your current Snakefile: with only four rules, it is already almost 150 lines long. Imagine what happens when your workflow has dozens of rules? The Snakefile may (will?) become messy and harder to maintain and edit. This is why it quickly becomes crucial to modularise your workflow. This approach also makes it easier to re-use pieces of one workflow into another. Snakemake can be modularised on three different levels:</p> <ol> <li> <p>The most fine-grained level is wrappers</p> More information on wrappers <p>Wrappers allow to quickly use popular tools and libraries in Snakemake workflows, thanks to the <code>wrapper</code> directive. Wrappers are automatically downloaded and deploy a conda environment when running the workflow, which increases reproducibility. However their implementation can be \u2018rigid\u2019 and sometimes it may be better to write your own rule. See the official documentation for more explanations</p> </li> <li> <p>For larger parts belonging to the same workflow, it is recommended to split the main Snakefile into smaller snakefiles, each containing rules with a common topic. Smaller snakefiles are then integrated into the main Snakefile with the <code>include</code> statement. In this case, all rules share a common config file. See the official documentation for more explanations</p> Rules organisation <p>There is no official guideline on how to regroup rules, but a simple and logic approach is to create \u201cthematic\u201d snakefiles, i.e. place rules related to the same topic in the same file. Modularisation is a common practice in programming in general: it is often easier to group all variables, functions, classes\u2026 related to a common theme into a single script, package, software\u2026</p> </li> <li> <p>The final level of modularisation is modules</p> More on modules <p>It enables combination and re-use of rules in the same workflow and between workflows. This is done with the <code>module</code> statement, similarly to Python <code>import</code>. See the official documentation for more explanations</p> </li> </ol> <p>In this course, you will only use the second level of modularisation. Briefly, the idea is to write a main Snakefile in <code>workflow/Snakefile</code>, to place the other snakefile containing rules in the subfolder <code>workflow/rules</code> (these \u2018sub-Snakefile\u2019 should end with <code>.smk</code>, the recommended file extension of Snakemake) and to tell Snakemake to import the modular snakefile in the main Snakefile with the <code>include: &lt;path/to/snakefile.smk&gt;</code> syntax.</p> <p>Exercise: Move your current Snakefile into the subfolder <code>workflow/rules</code> and rename it to <code>read_mapping.smk</code>. Then create a new Snakefile in <code>workflow/</code> and import <code>read_mapping.smk</code> in it using the <code>include</code> syntax. You should also move the importation of the config file from the modular Snakefile to the main one.</p> Answer <p>First, move and rename the main Snakefile: <pre><code>mv workflow/Snakefile workflow/rules/read_mapping.smk  # Move and rename main Snakefile to modular snakefile\ntouch workflow/Snakefile  # Recreate main Snakefile\n</code></pre></p> <p>Then, add <code>include</code> and <code>configfile</code> statements to the new Snakefile. It should resemble this: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\n</code></pre></p> <p>Finally, remove the config file import (<code>configfile: 'config/config.yaml'</code>) from the modular snakefile (<code>workflow/rules/read_mapping.smk</code>).</p> Relative paths <ul> <li>Include statements are relative to the directory of the Snakefile in which they occur. For example, if the Snakefile is in <code>workflow</code>, then Snakemake will search for included snakefiles in <code>workflow/path/to/other/snakefile</code>, regardless of the working directory</li> <li>You can place snakefiles in a sub-directory without changing input and output paths, because these paths are relative to the working directory</li> <li>However, you will need to edit paths to external scripts and conda environments, because these paths are relative to the snakefile from which they are called (this will be discussed in the last series of exercises)</li> </ul> <p>If you have trouble visualising what an <code>include</code> statement does, you can imagine that the entire content of the included file gets copied into the Snakefile. As a consequence, syntaxes like <code>rules.&lt;rule_name&gt;.output.&lt;output_name&gt;</code> can still be used in modular snakefiles, even if the rule <code>&lt;rule_name&gt;</code> is defined in another snakefile. However, you have to make sure that the snakefile in which <code>&lt;rule_name&gt;</code> is defined is included before the snakefile that uses <code>rules.&lt;rule_name&gt;.output</code>. This is also true for input functions and checkpoints.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#using-a-target-rule-instead-of-a-target-file","title":"Using a target rule instead of a target file","text":"<p>Modularisation also offers a great opportunity to facilitate workflows execution. By default, if no target is given in the command line, Snakemake executes the first rule in the Snakefile. So far, you have always executed the workflow with a target file to avoid this behaviour. But we can actually use this property to make execution easier by writing a pseudo-rule (also called target-rule and usually named rule <code>all</code>) which contains all the desired outputs files as inputs in the Snakefile. This rule will look like this:</p> <pre><code>rule all:\n    input:\n        'path/to/ouput1',\n        'path/to/ouput2',\n        '...'\n</code></pre> <p>Exercise: Implement a rule <code>all</code> in your Snakefile to generate the final outputs by default when running <code>snakemake</code> without specifying a target. Then, test your workflow with a dry-run and the <code>-F</code> parameter. How many rules does Snakemake run?</p> Content of a rule <code>all</code> <ul> <li>A rule is not required to have an output nor a <code>shell</code> directive</li> <li>Inputs of rule <code>all</code> should be the final outputs that you want to generate, here those of rule <code>reads_quantification_genes</code></li> </ul> Answer <p><code>reads_quantification_genes</code> is currently creating the last workflow outputs (with the same example as before, <code>results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv</code>). We need to use these files as inputs of rule <code>all</code>: <pre><code># Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        'results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv'\n</code></pre></p> <p>You can launch a dry-run with: <pre><code>snakemake -c 4 -F -p -n\n</code></pre></p> <p>You should see all the rules appearing, including rule <code>all</code>, and the job stats: <pre><code>rule all:\n    input: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n    jobid: 0\n    reason: Forced execution\n    resources: tmpdir=/tmp\n\nJob stats:\njob                           count\n--------------------------  -------\nall                               1\nfastq_trim                        1\nread_mapping                      1\nreads_quantification_genes        1\nsam_to_bam                        1\ntotal                             5\n</code></pre> Snakemake runs 5 rules in total: the 4 of the previous session and the rule <code>all</code>.</p> <p>After several (dry-)runs, you may have noticed that the rule order is not always the same: apart from Snakemake considering the first rule of the workflow as a default target, the order of rules in Snakefile/snakefiles is arbitrary and does not influence the DAG of jobs.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#aggregating-outputs-to-process-lists-of-files","title":"Aggregating outputs to process lists of files","text":"<p>Using a target rule like the one presented in the previous paragraph gives another opportunity to make things easier. In the previous rule <code>all</code>, inputs are still hard-coded\u2026 and you know that this is not an optimal solution, especially if there are many samples to process. The <code>expand()</code> function will solve both problems.</p> <p><code>expand()</code> is used to generate a list of output files by automatically expanding a wildcard expression to several values. In other words, it will replace a wildcard in an expression by all the values of a list, successively. For instance, <code>expand('{sample}.tsv', sample=['A', 'B', 'C'])</code> will create the list of files <code>['A.tsv', 'B.tsv', 'C.tsv']</code>.</p> <p>Exercise: Use an expand syntax to transform rule <code>all</code> to generate a list of final outputs with all the samples. Then, test your workflow with a dry-run and the <code>-F</code> parameter.</p> Two things are required for an expand syntax <ol> <li>A Python list of values that will replace a wildcard; here, a sample list</li> <li>An output path with a wildcard that can be turned into an <code>expand()</code> function to create all the required outputs</li> </ol> Answer <p>First, we need to add a sample list in the Snakefile, before rule <code>all</code>. This list contains all the values that the wildcard will be replaced with: <pre><code>SAMPLES = ['highCO2_sample1', 'highCO2_sample2', 'highCO2_sample3', 'lowCO2_sample1', 'lowCO2_sample2', 'lowCO2_sample3']\n</code></pre></p> <p>Then, we need to transform the rule <code>all</code> inputs to use the <code>expand</code> function: <pre><code>expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=SAMPLES)\n</code></pre></p> <p>The Snakefile should like this: <pre><code># Sample list\nSAMPLES = ['highCO2_sample1', 'highCO2_sample2', 'highCO2_sample3', 'lowCO2_sample1', 'lowCO2_sample2', 'lowCO2_sample3']\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=SAMPLES)\n</code></pre></p> <p>You can launch a dry-run with the same command as before: <pre><code>snakemake -c 4 -F -p -n\n</code></pre></p> <p>You should see rule <code>all</code> requiring 6 inputs and all the other rules appearing 6 times (1 for each sample): <pre><code>rule all:\n    input: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv, results/highCO2_sample2/highCO2_sample2_genes_read_quantification.tsv, results/highCO2_sample3/highCO2_sample3_genes_read_quantification.tsv, results/lowCO2_sample1/lowCO2_sample1_genes_read_quantification.tsv, results/lowCO2_sample2/lowCO2_sample2_genes_read_quantification.tsv, results/lowCO2_sample3/lowCO2_sample3_genes_read_quantification.tsv\n    jobid: 0\n    reason: Forced execution\n    resources: tmpdir=/tmp\n\nJob stats:\njob                           count\n--------------------------  -------\nall                               1\nfastq_trim                        6\nread_mapping                      6\nreads_quantification_genes        6\nsam_to_bam                        6\ntotal                            25\n</code></pre></p> <p>But there is an even better solution! At the moment, samples are defined as a list in the Snakefile. Processing other samples still means having to locate and change some chunks of code. To further improve workflow usability, samples can instead be defined in config files, so they can easily be added, removed, or modified by users without actually modifying code.</p> <p>Exercise: Implement a parameter in the config file to specify sample names and modify the rule <code>all</code> to use this parameter instead of the <code>SAMPLES</code> variable in the <code>expand()</code> syntax.</p> Answer <p>First, we need to add the sample names to the config file: <pre><code># Configuration options of RNAseq-analysis workflow\n\n# Location of genome indices\nindex: 'resources/genome_indices/Scerevisiae_index'\n\n# Location of annotation file\nannotations: 'resources/Scerevisiae.gtf'\n\n# Sample names\nsamples:\n  - highCO2_sample1\n  - highCO2_sample2\n  - highCO2_sample3\n  - lowCO2_sample1\n  - lowCO2_sample2\n  - lowCO2_sample3\n</code></pre></p> <p>Then, we need to remove <code>SAMPLES</code> from the Snakefile and use the config file in <code>expand()</code> instead: <pre><code># Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=config['samples'])\n</code></pre> Here, <code>config['samples']</code> is a Python list containing strings, each string being a sample name. This is because a list of parameters become a list when the config file is parsed. You can launch a dry-run with the same command as before (<code>snakemake -c 4 -F -p -n</code>) and you should see the same jobs.</p> An even more Snakemake-idiomatic solution <p>There is an even better and more Snakemake-idiomatic version of the <code>expand()</code> syntax in rule <code>all</code>: <pre><code>expand(rules.reads_quantification_genes.output.gene_level, sample=config['samples'])\n</code></pre> This entirely removes the need to write output paths, even though it might be less easy to understand at first sigh.</p> <p>Exercise: Run the workflow on the other samples and generate the workflow DAG and filegraph. If you implemented parallelisation and multithreading in all the rules, the execution should take less than 10 min in total to process all the samples, otherwise it will be a few minutes longer.</p> Answer <p>You can run the workflow by removing <code>-F</code> and <code>-n</code> from the dry-run command, which makes a very simple command: <pre><code>snakemake -c 4 -p --sdm=apptainer\n</code></pre></p> <p>To generate the DAG, you can use: <pre><code>snakemake -c 1 -F -p --dag | dot -Tpng &gt; images/all_samples_dag.png\n</code></pre> <p> </p></p> <p>If needed, open the picture in a new tab to zoom in. Then, you can generate the filegraph with: <pre><code>snakemake -c 1 -F -p --filegraph | dot -Tpng &gt; images/all_samples_filegraph.png\n</code></pre> <p> </p> You probably noticed that these two figures have an extra rule, <code>fastq_qc_sol4</code>. It is the rule implemented in the supplementary exercise below.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#extra-optimising-resource-usage-in-a-workflow","title":"Extra: optimising resource usage in a workflow","text":"<p>This part is an extra-exercise about resource usage in Snakemake. It is quite long, so do it only if you finished all the other exercises.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#multithreading","title":"Multithreading","text":"<p>When working with real, larger, datasets, some processes can take a long time to run. Fortunately, computation time can be decreased by running jobs in parallel and using several threads or cores for a single job.</p> <p>Exercise: What are the two things you need to add to a rule to enable multithreading?</p> Answer <p>You need to add:</p> <ol> <li>The <code>threads</code> directive to tell Snakemake that it needs to allocate several threads to this rule</li> <li>The software-specific parameters in the <code>shell</code> directive to tell a software that it can use several threads</li> </ol> <p>If you add only the first element, the software will not be aware of the number of threads allocated to it and will use its default number of threads (usually one). If you add only the second element, Snakemake will allocate only one thread to the software, which means that it will run slowly or crash (the software expects multiple threads but gets one).</p> <p>Usually, you need to read documentation to identify which software can make use of multithreading and which parameters control multithreading. We did it for you to save some time:</p> <ul> <li><code>atropos trim</code>, <code>hisat2</code>, <code>samtools view</code>, and <code>samtools sort</code> can parallelise with the <code>--threads &lt;nb_thread&gt;</code> parameter</li> <li><code>featureCounts</code> can parallelise with the <code>-T &lt;nb_thread&gt;</code> parameter</li> <li><code>samtools index</code> can\u2019t parallelise. Remember that multithreading only applies to software that were developed to this end, Snakemake itself cannot parallelise a software!</li> </ul> <p>Unfortunately, there is no easy way to find the optimal number of threads for a job. It depends on tasks, datasets, software, resources you have at your disposal\u2026 It often takes of few rounds of trial and error to see what works best. We already decided the number of threads to use for each software:</p> <ul> <li>4 threads for <code>hisat2</code></li> <li>2 for all the other software</li> </ul> <p>Exercise: Implement multithreading in a rule of your choice (it\u2019s usually best to start by multithreading the longest job, here <code>read_mapping</code>, but the example dataset is small, so it doesn\u2019t really matter).</p> <code>threads</code> placeholder <p><code>threads</code> can also be replaced by a <code>{threads}</code> placeholder in the <code>shell</code> directive.</p> Answer <p>We implemented multithreading in all the rules so that you can check everything. Feel free to copy this in your Snakefile: <pre><code>rule fastq_trim:\n    '''\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    '''\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    log:\n        'logs/{sample}/{sample}_atropos_trimming.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_trimming.txt'\n    threads: 2  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2'\n    shell:\n        '''\n        echo \"Trimming reads in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt; {log}\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" --threads {threads} \\  # Add multithreading to software\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2} &amp;&gt;&gt; {log}\n        echo \"Trimmed files saved in &lt;{output.trim1}&gt; and &lt;{output.trim2}&gt; respectively\" &gt;&gt; {log}\n        echo \"Trimming report saved in &lt;{log}&gt;\" &gt;&gt; {log}\n        '''\n\nrule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    threads: 4  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x resources/genome_indices/Scerevisiae_index --threads {threads} \\  # Add multithreading to software\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n\nrule sam_to_bam:\n    '''\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    '''\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    log:\n        'logs/{sample}/{sample}_mapping_sam_to_bam.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping_sam_to_bam.txt'\n    threads: 2  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/samtools%3A1.21--h50ea8bc_0'\n    shell:\n        '''\n        echo \"Converting &lt;{input.sam}&gt; to BAM format\" &gt; {log}\n        samtools view {input.sam} --threads {threads} -b -o {output.bam} 2&gt;&gt; {log}  # Add multithreading to software\n        echo \"Sorting .bam file\" &gt;&gt; {log}\n        samtools sort {output.bam} --threads {threads} -O bam -o {output.bam_sorted} 2&gt;&gt; {log}  # Add multithreading to software\n        echo \"Indexing sorted .bam file\" &gt;&gt; {log}\n        samtools index -b {output.bam_sorted} -o {output.index} 2&gt;&gt; {log}\n        echo \"Sorted file saved in &lt;{output.bam_sorted}&gt;\" &gt;&gt; {log}\n        '''\n\nrule reads_quantification_genes:\n    '''\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly. The strandedness parameter\n    is determined by get_strandedness().\n    '''\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt'\n    threads: 2  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}  # Add multithreading to software\n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>Exercise: Do you need to change anything in the <code>snakemake</code> command to run the workflow with 4 cores?</p> Answer <p>You need to provide additional cores to Snakemake with the parameter <code>-c 4</code>. Using the same sample as before (<code>highCO2_sample1</code>), the workflow can be run with: <pre><code>snakemake -c 4 -F -p --sdm=apptainer results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n</code></pre></p> <p>The number of threads allocated to all jobs running at a given time cannot exceed the value specified with <code>--cores</code>, so if you use <code>-c 1</code>, Snakemake will not be able to use multiple threads. Conversely, if you ask for more threads in a rule than what was provided with <code>--cores</code>, Snakemake will cap rule threads at <code>--cores</code> to avoid requesting too many. Another benefit of increasing <code>--cores</code> is to allow Snakemake to run multiple jobs in parallel (for example, here, running two jobs using two threads each).</p> <p>If you run the workflow from scratch with multithreading in all rules, it should take ~6 min, compared to ~10 min before (i.e. a 40% decrease!). This gives you an idea of how powerful multithreading is when datasets and computing power get bigger!</p> Things to keep in mind when using parallel execution <ul> <li>On-screen output from parallel jobs will be mixed, so save any output to log files instead</li> <li>Parallel jobs use more RAM. If you run out then either your OS will swap data to disk (which slows data access), or a process will die (which can crash Snakemake)</li> <li>Parallelising is not without consequences and has a cost. This is a topic too wide for this course, but just know that using too many cores on a dataset that is too small can slow down computation, as explained here.</li> </ul>"},{"location":"course_material/day2/4_optimising_snakemake/#controlling-memory-usage-and-runtime","title":"Controlling memory usage and runtime","text":"<p>Another way to optimise resource usage in a workflow is to control the amount of memory and runtime of each job. This ensures your instance (computer, cluster\u2026) won\u2019t run out of memory during computation (which could interrupt jobs or even crash the instance) and that your jobs will run in a reasonable amount of time (a job taking more time to run than usual might be a sign that something is going on).</p> Resource usage and schedulers <p>Optimising resource usage is especially important when submitting jobs to a scheduler (for instance on a cluster), as it allows a better and more precise definition of your job priority: jobs with low threads/memory/runtime requirements often have a higher priority than heavy jobs, which means they will often start first.</p> <p>Controlling memory usage and runtime in Snakemake is easier than multithreading: you only need to need to use the <code>resources</code> directive with the <code>memory</code> or <code>runtime</code> keywords and in most software, you don\u2019t even need to specify the amount of memory available to the software via a parameter. Determining how much memory and runtime to use is also easier\u2026 after the first run of your workflow. Do you remember the benchmark files you (may have) obtained at the end of the previous series of exercises? Now is the time to take a look at them:</p> s h: m: s max_rss max_vms max_uss max_pss io_in io_out mean_load cpu_time 31.3048 0:00:31 763.04 904.29 757.89 758.37 1.81 230.18 37.09 11.78 <ul> <li><code>s</code> and <code>h:m:s</code> give the job wall clock time (in seconds and hours-minutes-seconds, respectively), which is the actual time taken from the start of a software to its end. You can use these results to infer a safe value for the <code>runtime</code> keyword</li> <li>Likewise, you can use <code>max_rss</code> (shown in megabytes) to figure out how much memory was used by the job and use this value in the <code>memory</code> keyword</li> </ul> What are the other columns? <p>In case you are wondering about the other columns of the table, the official documentation has detailed explanations about their content.</p> <p>Here are some suggested values for the current workflow:</p> <ul> <li><code>fastq_trim</code>: 500 MB</li> <li><code>read_mapping</code>: 2 GB</li> <li><code>sam_to_bam</code>: 250 MB</li> <li><code>reads_quantification_genes</code>: 500 MB</li> </ul> <p>Exercise: Implement memory usage limit in a rule of your choice.</p> Two ways to declare memory values  <p>There are two ways to declare memory values in <code>resources</code>:</p> <ol> <li><code>mem_&lt;unit&gt; = n</code></li> <li><code>mem = 'n&lt;unit&gt;'</code>: in this case, you must pass a string, so you have to enclose <code>n&lt;unit&gt;</code> with quotes <code>''</code></li> </ol> <p><code>&lt;unit&gt;</code> is a unit in [B, KB, MB, GB, TB, PB, KiB, MiB, GiB, TiB, PiB] and <code>n</code> is a float.</p> Answer <p>We implemented memory usage control in all the rules so that you can check everything. Rules <code>fastq_trim</code> and <code>read_mapping</code> have the first format while rules <code>sam_to_bam</code> and <code>reads_quantification_genes</code> have the second one. Feel free to copy this in your Snakefile: <pre><code>rule fastq_trim:\n    '''\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    '''\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    log:\n        'logs/{sample}/{sample}_atropos_trimming.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_trimming.txt'\n    resources:  # Add directive\n        mem_mb = 500  # Add keyword and value with format 1\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2'\n    shell:\n        '''\n        echo \"Trimming reads in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt; {log}\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" --threads {threads} \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2} &amp;&gt;&gt; {log}\n        echo \"Trimmed files saved in &lt;{output.trim1}&gt; and &lt;{output.trim2}&gt; respectively\" &gt;&gt; {log}\n        echo \"Trimming report saved in &lt;{log}&gt;\" &gt;&gt; {log}\n        '''\n\nrule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    resources:  # Add directive\n        mem_gb = 2  # Add keyword and value with format 1\n    threads: 4\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x resources/genome_indices/Scerevisiae_index --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n\nrule sam_to_bam:\n    '''\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    '''\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    log:\n        'logs/{sample}/{sample}_mapping_sam_to_bam.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping_sam_to_bam.txt'\n    resources:  # Add directive\n        mem = '250MB'  # Add keyword and value with format 2\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/samtools%3A1.21--h50ea8bc_0'\n    shell:\n        '''\n        echo \"Converting &lt;{input.sam}&gt; to BAM format\" &gt; {log}\n        samtools view {input.sam} --threads {threads} -b -o {output.bam} 2&gt;&gt; {log}\n        echo \"Sorting .bam file\" &gt;&gt; {log}\n        samtools sort {output.bam} --threads {threads} -O bam -o {output.bam_sorted} 2&gt;&gt; {log}\n        echo \"Indexing sorted .bam file\" &gt;&gt; {log}\n        samtools index -b {output.bam_sorted} -o {output.index} 2&gt;&gt; {log}\n        echo \"Sorted file saved in &lt;{output.bam_sorted}&gt;\" &gt;&gt; {log}\n        '''\n\nrule reads_quantification_genes:\n    '''\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly. The strandedness parameter\n    is determined by get_strandedness().\n    '''\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt'\n    resources:  # Add directive\n        mem = '500MB'  # Add keyword and value with format 2\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>Finally, contrary to multithreading, note that you don\u2019t need to change the <code>snakemake</code> command to launch the workflow!</p>"},{"location":"course_material/day2/4_optimising_snakemake/#extra-using-non-conventional-outputs","title":"Extra: using non-conventional outputs","text":"<p>This part is an extra-exercise about non-conventional Snakemake outputs. It is quite long, so do it only if you finished all the other exercises.</p> <p>Snakemake has several built-in utilities to assign properties to outputs that are deemed \u2018special\u2019. These properties are listed in the table below:</p> Property Syntax Function Temporary <code>temp('file.txt')</code> File is deleted as soon as it is not required by a future jobs Protected <code>protected('file.txt')</code> File cannot be overwritten after job ends (useful to prevent erasing a file by mistake, for example files requiring heavy computation) Ancient <code>ancient('file.txt')</code> Ignore file timestamp and assume file is older than any outputs: file will not be re-created when re-running workflow, except when <code>--force</code> options are used Directory <code>directory('directory')</code> Output is a directory instead of a file (use <code>touch()</code> instead if possible) Touch <code>touch('file.txt')</code> Create an empty flag file <code>file.txt</code> regardless of shell commands (only if commands finished without errors) <p>The next paragraphs will show how to use some of these properties.</p>"},{"location":"course_material/day2/4_optimising_snakemake/#removing-and-safeguarding-outputs","title":"Removing and safeguarding outputs","text":"<p>This part shows a few examples using <code>temp()</code> and <code>protected()</code> flags.</p> <p>Exercise: Can you think of a convenient use of the <code>temp()</code> flag?</p> Answer <p><code>temp()</code> is extremely useful to automatically remove intermediary outputs that are no longer needed.</p> <p>Exercise: In your workflow, identify outputs that are intermediary and mark them as temporary with <code>temp()</code>.</p> Answer <p>Unsorted .bam and .sam outputs seem like great candidates to be marked as temporary. One could argue that trimmed .fastq files could also be marked as temporary, but we won\u2019t bother with them here. For clarity, only lines that changed are shown below:</p> <ul> <li> <p>rule <code>read_mapping</code>: <pre><code>output:\n    sam = temp('results/{sample}/{sample}_mapped_reads.sam'),\n</code></pre></p> </li> <li> <p>rule <code>sam_to_bam</code>: <pre><code>output:\n    bam = temp('results/{sample}/{sample}_mapped_reads.bam'),\n</code></pre></p> </li> </ul> Advantages and drawbacks of <code>temp()</code> <p>On one hand, removing temporary outputs is a great way to save storage space. If you look at the size of your current <code>results/</code> folder (<code>du -bchd0 results/</code>), you will notice that it drastically increased. Just removing these two files would allow to save ~1 GB. While it may not seem like much, remember that you usually have much bigger files and many more samples!</p> <p>On the other hand, using temporary outputs might force you to re-run more jobs than necessary if an input changes, so think carefully before using <code>temp()</code>.</p> <p>Exercise: On the contrary, is there a file of your workflow that you would like to protect? If so, mark it with <code>protected()</code>.</p> Answer <p>Sorted .bam files from rule <code>sam_to_bam</code> seem like good candidates for protection: <pre><code>output:\n    bam_sorted = protected('results/{sample}/{sample}_mapped_reads_sorted.bam'),\n</code></pre> If you set this output as protected, be careful when you want to re-run your workflow to recreate the file!</p>"},{"location":"course_material/day2/4_optimising_snakemake/#using-an-output-directory-the-fastqc-example","title":"Using an output directory: the FastQC example","text":"<p>FastQC is a program designed to spot potential problems in high-througput sequencing datasets. It is a very popular tool, notably because it is fast and does not require a lot of configuration. It runs a set of analyses on one or more sequence files in FASTQ or BAM format and produces a quality report with plots that summarise the results. It highlights potential problems that might require a closer look in your dataset.</p> <p>As such, it would be interesting to run FastQC on the original and trimmed .fastq files to check whether trimming actually improved read quality. FastQC can be run interactively or in batch mode, during which it saves results as an HTML file and a ZIP file. You will later see that running FastQC in batch mode is a bit harder than it looks.</p> Data types and FastQC <p>FastQC does not differentiate between sequencing techniques and as such can be used to look at libraries coming from a large number of experiments (Genomic Sequencing, ChIP-Seq, RNAseq, BS-Seq etc\u2026).</p> <p>If you run <code>fastqc -h</code>, you will notice something a bit surprising, but not unusual in bioinformatics: <pre><code>    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it. If this option is not set then the\n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n\n   -d --dir         Selects a directory to be used for temporary files written when\n                    generating report images. Defaults to system temp directory if\n                    not specified.\n</code></pre></p> <p>Two files are produced for each .fastq file and these files appear in the same directory as the input file: FastQC does not allow to specify the output file names! However, you can set an alternative output directory, even though it needs to be manually created before FastQC is run.</p> <p>There are different solutions to this problem:</p> <ol> <li>Work with the default file names produced by FastQC and leave the reports in the same directory as the input files</li> <li>Create the outputs in a new directory and leave the reports with their default name</li> <li>Create the outputs in a new directory and tell Snakemake that the directory itself is the output</li> <li>Force a naming convention by manually renaming the FastQC output files within the rule</li> </ol> <p>It would be too long to test all four solutions, so you will work on the 3<sup>rd</sup> or the 4<sup>th</sup> solution. Here is a brief summary of solutions 1 and 2:</p> <ol> <li>This could work, but it\u2019s better not to put reports in the same directory as input sequences. As a general principle, when writing Snakemake rules, we prefer to be in charge of the output names and to have all the files linked to a sample in the same directory</li> <li>This involves manually constructing the output directory path to use with the <code>-o</code> parameter, which works but isn\u2019t very convenient</li> </ol> <p>The first part of the FastQC command is: <pre><code>fastqc --format fastq --threads {threads} --outdir {folder_path} --dir {folder_path} &lt;input_fastq1&gt; &lt;input_fastq2&gt;\n</code></pre></p> Explanation of FastQC parameters <ul> <li><code>-t/--threads</code>: specify how many files can be processed simultaneously. Here, it will be 2 because inputs are paired-end files</li> <li><code>-o/--outdir</code>: create output files in specified output directory</li> <li><code>-d/--dir</code>: select a directory to be used for temporary files</li> </ul> <p>Choose between solution 3 or 4 and implement it. You will find more information and help below.</p> Solution 3Solution 4 <p>This option is equivalent to tell Snakemake not to worry about individual files at all and consider an entire directory as the rule output.</p> <p>Exercise: Implement a single rule to run FastQC on both the original and trimmed .fastq files (four files in total) using directories as ouputs with the <code>directory()</code> flag.</p> <ul> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0</code></li> </ul> Answer <p>This makes the rule definition quite \u2018simple\u2019 compared to solution 4: <pre><code>rule fastq_qc_sol3:\n    '''\n    This rule performs a QC on paired-end .fastq files before and after trimming.\n    '''\n    input:\n        reads1 = rules.fastq_trim.input.reads1,\n        reads2 = rules.fastq_trim.input.reads2,\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        before_trim = directory('results/{sample}/fastqc_reports/before_trim/'),\n        after_trim = directory('results/{sample}/fastqc_reports/after_trim/')\n    log:\n        'logs/{sample}/{sample}_fastqc.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_fastqc.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0'\n    shell:\n        '''\n        echo \"Creating output directory &lt;{output.before_trim}&gt;\" &gt; {log}\n        mkdir -p {output.before_trim} 2&gt;&gt; {log}  # FastQC doesn't create output directories so we have to do it manually\n        echo \"Performing QC of reads before trimming in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {output.before_trim} \\\n        --dir {output.before_trim} {input.reads1} {input.reads2} &amp;&gt;&gt; {log}\n        echo \"Results saved in &lt;{output.before_trim}&gt;\" &gt;&gt; {log}\n        echo \"Creating output directory &lt;{output.after_trim}&gt;\" &gt;&gt; {log}\n        mkdir -p {output.after_trim} 2&gt;&gt; {log}  # FastQC doesn't create output directories so we have to do it manually\n        echo \"Performing QC of reads after trimming in &lt;{input.trim1}&gt; and &lt;{input.trim2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {output.after_trim} \\\n        --dir {output.after_trim} {input.trim1} {input.trim2} &amp;&gt;&gt; {log}\n        echo \"Results saved in &lt;{output.after_trim}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <code>.snakemake_timestamp</code> <p>When <code>directory()</code> is used, Snakemake creates an empty file called <code>.snakemake_timestamp</code> in the output directory. This is the marker file it uses to know whether it needs to re-run the rule producing the directory.</p> <p>Overall, this rule works quite well and allows for an easy rule definition. However, in this case, individual files are not explicitly named as outputs and this may cause problems to chain rules later. Also, remember that some software won\u2019t give you any control at all over the outputs, which is why you need a back-up plan, i.e. solution 4: the most powerful solution is still to use shell commands to move and/or rename files to names you want. Also, the Snakemake developers advise to use <code>directory()</code> only as a last resort and to use <code>touch()</code> instead.</p> <p>This option amounts to let FastQC follows its default behaviour but manually rename the files afterwards to obtain the exact outputs we require.</p> <p>Exercise: Implement a single rule to run FastQC on both the original and trimmed .fastq files (four files in total) and rename the files created by FastQC to the desired output names using the <code>mv &lt;old_name&gt; &lt;new_name&gt;</code> command.</p> <ul> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0</code></li> </ul> Answer <p>This makes the rule definition (much) more complicated than solution 3: <pre><code>rule fastq_qc_sol4:\n    '''\n    This rule performs a QC on paired-end .fastq files before and after trimming.\n    '''\n    input:\n        reads1 = rules.fastq_trim.input.reads1,\n        reads2 = rules.fastq_trim.input.reads2,\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        # QC before trimming\n        html1_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_1.html',  # Forward-read report in HTML format before trimming\n        zipfile1_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_1.zip',  # Forward-read report in ZIP format before trimming\n        html2_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_2.html',  # Reverse-read report in HTML format before trimming\n        zipfile2_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_2.zip',  # Reverse-read report in ZIP format before trimming\n        # QC after trimming\n        html1_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_1.html',  # Forward-read report in HTML format after trimming\n        zipfile1_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_1.zip',  # Forward-read report in ZIP format after trimming\n        html2_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_2.html',  # Forward-read report in HTML format after trimming\n        zipfile2_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_2.zip'  # Forward-read report in ZIP format after trimming\n    params:\n        wd = 'results/{sample}/fastqc_reports/',  # Temporary directory to store files before renaming\n        # QC before trimming\n        html1_before = 'results/{sample}/fastqc_reports/{sample}_1_fastqc.html',  # Default FastQC output name for forward-read report in HTML format before trimming\n        zipfile1_before = 'results/{sample}/fastqc_reports/{sample}_1_fastqc.zip',  # Default FastQC output name for forward-read report in ZIP format before trimming\n        html2_before = 'results/{sample}/fastqc_reports/{sample}_2_fastqc.html',  # Default FastQC output name for reverse-read report in HTML format before trimming\n        zipfile2_before = 'results/{sample}/fastqc_reports/{sample}_2_fastqc.zip',  # Default FastQC output name for reverse-read report in ZIP format before trimming\n        # QC after trimming\n        html1_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_1_fastqc.html',# Default FastQC output name for forward-read report in HTML format after trimming\n        zipfile1_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_1_fastqc.zip',# Default FastQC output name for forward-read report in ZIP format after trimming\n        html2_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_2_fastqc.html',# Default FastQC output name for reverse-read report in HTML format after trimming\n        zipfile2_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_2_fastqc.zip'# Default FastQC output name for reverse-read report in ZIP format after trimming\n    log:\n        'logs/{sample}/{sample}_fastqc.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_fastqc.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0'\n    shell:\n        '''\n        echo \"Creating output directory &lt;{params.wd}&gt;\" &gt; {log}\n        mkdir -p {params.wd} 2&gt;&gt; {log}  # FastQC doesn't create output directories so we have to do it manually\n        echo \"Performing QC of reads before trimming in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {params.wd} \\\n        --dir {params.wd} {input.reads1} {input.reads2} &amp;&gt;&gt; {log}\n        echo \"Renaming results from original fastq analysis\" &gt;&gt; {log}  # Renames files because we can't choose fastqc output\n        mv {params.html1_before} {output.html1_before} 2&gt;&gt; {log}\n        mv {params.zipfile1_before} {output.zipfile1_before} 2&gt;&gt; {log}\n        mv {params.html2_before} {output.html2_before} 2&gt;&gt; {log}\n        mv {params.zipfile2_before} {output.zipfile2_before} 2&gt;&gt; {log}\n        echo \"Performing QC of reads after trimming in &lt;{input.trim1}&gt; and &lt;{input.trim2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {params.wd} \\\n        --dir {params.wd} {input.trim1} {input.trim2} &amp;&gt;&gt; {log}\n        echo \"Renaming results from trimmed fastq analysis\" &gt;&gt; {log}  # Renames files because we can't choose fastqc output\n        mv {params.html1_after} {output.html1_after} 2&gt;&gt; {log}\n        mv {params.zipfile1_after} {output.zipfile1_after} 2&gt;&gt; {log}\n        mv {params.html2_after} {output.html2_after} 2&gt;&gt; {log}\n        mv {params.zipfile2_after} {output.zipfile2_after} 2&gt;&gt; {log}\n        echo \"Results saved in &lt;results/{wildcards.sample}/fastqc_reports/&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>This solution is very long and much more complicated than the other one. However, it makes up for its complexity by allowing a total control on what is happening: with this method, we can choose where temporary files are saved as well as output names. It could have been shortened by using <code>-o .</code> to tell FastQC to create files in the current working directory instead of a specific one, but this would have created another problem: if we run multiple jobs in parallel, then Snakemake may try to produce files from different jobs at the same temporary destination. In this case, the different Snakemake instances would be trying to write to the same temporary files at the same time, overwriting each other and corrupting the output files.</p> <p>Three interesting things are happening in both versions of this rule:</p> <ul> <li>Similarly to outputs, it is possible to refer to inputs of a rule directly in another rule with the syntax <code>rules.&lt;rule_name&gt;.input.&lt;input_name&gt;</code></li> <li>FastQC doesn\u2019t create output directories by itself (other programs might insist that output directories do not already exist), so you have to manually create it with <code>mkdir</code> in the <code>shell</code> directive before running FastQC<ul> <li>Overall, most software will create the required directories for you; FastQC is an exception</li> </ul> </li> <li>When using output files, Snakemake creates the missing folders by itself. However, when using a <code>directory(</code>) output, Snakemake will not create the directory. Otherwise, this would guarantee the rule to succeed every time, independently of the <code>shell</code> directive status: as soon as the directory is created, the rule succeeds, even if the <code>shell</code> directive fails. The current behaviour prevents Snakemake from continuing the workflow when a command may have failed. This also shows that solution 3, albeit simple, can be risky</li> </ul> Controlling execution flow <p>If you want to make sure that a certain rule is executed before another, you can write the outputs of the first rule as inputs of the second one, even if the rule doesn\u2019t use them. For example, you could force the execution of FastQC before mapping reads with one more line in rule <code>read_mapping</code>: <pre><code>rule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2,\n        fastqc = rules.fastq_qc_sol4.output.html1_before  # This single line will force the execution of FastQC before read mapping\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    params:\n        index = 'resources/genome_indices/Scerevisiae_index'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    resources:\n        mem_gb = 2\n    threads: 4\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x {params.index} --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p>"},{"location":"course_material/day2/5_reproducibility_snakemake/","title":"Being reproducible with Snakemake","text":""},{"location":"course_material/day2/5_reproducibility_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Use an input function to work with an unknown number of files</li> <li>Run scripts from other languages (Python and R)</li> <li>Deploy a rule-specific conda environment</li> <li>Deploy a rule-specific Docker/Apptainer container</li> </ul>"},{"location":"course_material/day2/5_reproducibility_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#snakefile-from-previous-session","title":"Snakefile from previous session","text":"<p>If you didn\u2019t finish the previous part or didn\u2019t do the optional exercises, you can restart from fully commented snakefiles, with a supplementary .fastq files quality check rule and multithreading, memory usage control implemented in all rules. You can download the files here or copy them locally if you cloned the course repository (they are located in <code>containers-snakemake-training/docs/solutions_day2/session3</code>):</p> <pre><code># Go to repository root, containers-snakemake-training, then copy folder with:\ncp -r docs/solutions_day2/session3 &lt;destination_path&gt;\n</code></pre>"},{"location":"course_material/day2/5_reproducibility_snakemake/#exercises","title":"Exercises","text":"<p>In this series of exercises, you will create the last two rules of the workflow. Each rule will execute a script (one in Python and one in R; don\u2019t worry, this is not a programming course, so we wrote the scripts for you!), and both rules will have dedicated environments that you will need to take into account in the snakefiles.</p> Development and back-up <p>During this session as well, you will modify your Snakefile quite heavily, so it may be a good idea to make back-ups from time to time (with <code>cp</code> or a simple copy/paste) or use a versioning system. As a general rule, if you have a doubt on the code you are developing, do not hesitate to make a back-up beforehand.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#creating-a-rule-to-gather-read-counts","title":"Creating a rule to gather read counts","text":"<p>To perform a Differential Expression Analysis (DEA), it is easier to have a single file gathering all the read counts from the different samples. The next rule that you will create will both find the required files and merge them, thanks to an input function and a Python script.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#building-the-general-rule-structure","title":"Building the general rule structure","text":"<p>We already wrote the common elements of the rule so that you can focus on the most interesting parts (the missing <code>input</code> and the missing elements at the end):</p> <pre><code>rule count_table:\n    '''\n    This rule merges gene count tables of an assembly into one table.\n    '''\n    input:\n        ?\n    output:\n        count_table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    benchmark:\n        'benchmarks/total_count_table.txt'\n    resources:\n        mem_mb = 500\n    threads: 1\n    ?:\n        ?\n    ?:\n        ?\n</code></pre> <p>Here, there is no <code>wildcards</code> in the rule: only one file will be created, and its name will not change depending on sample names because we use all the samples to create it.</p> Explicit is better than implicit <p>Even if a software cannot multithread, it is useful to add <code>threads: 1</code> to keep the syntax consistent between rules and clearly state that the software works with a single thread.</p> <p>Exercise: Given that this rule and the next one will be quite different from the previous ones, it is a good idea to implement them in a new snakefile. Create a new file <code>workflow/rules/analysis.smk</code> and copy the previous rule structure in it. Do you need to change anything else to use this rule in your workflow?</p> Answer <p>To actually use this rule, Snakemake needs to be aware that it exists: this is done with the <code>include</code> statement. We need to add the following lines to the main Snakefile (<code>workflow/Snakefile</code>): <pre><code>include: 'rules/analyses.smk'\n</code></pre></p> <p>The Snakefile will look like this: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        expand(rules.reads_quantification_genes.output.gene_level, sample=config['samples'])\n</code></pre></p> <p>Let\u2019s start filling those missing elements!</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#gathering-the-input-files","title":"Gathering the input files","text":"<p>This task is quite complex: we need a way to identify all the rule inputs and gather them in a Python list. Here, there are only six samples, so in theory, you could list them directly\u2026 However, it isn\u2019t good practice and it quickly becomes un-manageable when the number of sample increases. Fortunately, there is a much more elegant and convenient solution: an input function, which provides the added benefit of scaling up very well.</p> <p>We wrote one for you: <pre><code># Input function used in rule count_table\ndef get_gene_counts(wildcards):\n    '''\n    This function lists count tables from every sample in the config file\n    '''\n    return [f\"results/{sample}/{sample}_genes_read_quantification.tsv\"\n            for sample in config['samples']]\n</code></pre></p> Snakemake wildcards vs Python f-strings <p>This input function is pure Python code: in the return statement, <code>{sample}</code> isn\u2019t a wildcard, it is an f-string variable! This shows that you can natively use basic Python elements in a workflow: Snakemake will still be able understand them. This is because Snakemake was built on top of Python.</p> <p>This function will loop over the list of samples in the config file and replace <code>{sample}</code> with the current sample name of the iteration to create a string which is the output path from the rule <code>reads_quantification_genes</code> of said sample. Then, it will aggregate all the paths in a list and return this list.</p> More on input functions <ul> <li>Input functions take the <code>wildcards</code> global object as single argument</li> <li>You can access wildcard values inside an input function with the syntax <code>{wildcards.wildcards_name}</code></li> <li>Input functions can return lists of files, which will be automatically handled like multiple inputs by Snakemake<ul> <li>Input functions can also return a dictionary; in this case, the function should be called with the <code>unpack()</code> function: <pre><code>input: unpack(&lt;function_name&gt;)\n</code></pre> The dictionary keys will be used as input names and the dictionary values will be used as input values, providing a list of named inputs</li> </ul> </li> </ul> Input functions and output directory <p>Input functions are evaluated before the workflow is executed, so they cannot be used to list the content of an output directory, since it does not exist before the workflow is executed. Instead, you can use a checkpoint to trigger a re-evaluation of the DAG.</p> <p>Exercise: Insert the function <code>get_gene_counts()</code> in <code>workflow/rules/analysis.smk</code> and adapt the input value of <code>count_table</code> accordingly. Do you need to insert the function in a specific location?</p> Answer <p>The first step is to add the input function to the file. However, it needs to appear before the rule <code>count_table</code>, otherwise we will see the error <code>name 'get_gene_counts' is not defined</code>. In other words, the function needs to be defined before Snakemake looks for it when it parses the input. Then, we need to set the function name as the rule input value.</p> <p>The modular snakefile <code>workflow/rules/analysis.smk</code> will resemble this: <pre><code># Input function used in rule count_table\ndef get_gene_counts(wildcards):\n    '''\n    This function lists count tables from every sample in the config file\n    '''\n    return [f\"results/{sample}/{sample}_genes_read_quantification.tsv\"\n            for sample in config['samples']]\n\nrule count_table:\n    '''\n    This rule merges gene count tables of an assembly into one table.\n    '''\n    input:\n        get_gene_counts  # Add input function to rule\n    output:\n        count_table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    benchmark:\n        'benchmarks/total_count_table.txt'\n    resources:\n        mem_mb = 500\n    threads: 1\n    ?:\n        ?\n    ?:\n        ?\n</code></pre> You don\u2019t need to use parentheses or specify any argument when you call an input function in the <code>input</code> directive. Doing so would actually change Snakemake behaviour!</p> <p>Now that the rule inputs are defined, we need to set-up the script to process them.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#using-a-python-script-in-snakemake","title":"Using a Python script in Snakemake","text":""},{"location":"course_material/day2/5_reproducibility_snakemake/#getting-the-script","title":"Getting the script","text":"<p>The counts will be concatenated thanks to a script called <code>count_table.py</code>. It was written in Python, takes a list of files as input, and produces one output, a tab-separated table containing read counts of the different samples for each gene. You can download it with:</p> <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/docs/solutions_day2/session4/workflow/scripts/count_table.py\n</code></pre> <p>Or you can copy it from here:</p> Click here to see a nice Python script! count_table.py<pre><code>'''\nMerge gene counts from all samples of an assembly into a single table.\n'''\n\n\nimport os\nimport pandas as pd  # Non-native package\nimport sys\n\n\n# Constants\nFIELDS = ['Geneid', 'Reads_quant']\nSTR_TO_REMOVE = '_genes_read_quantification.tsv'\n\n\n# Functions\ndef import_clean(table):\n    print(f'Importing and cleaning quantification data from &lt;{table}&gt;')\n    reads = pd.read_csv(table, sep='\\t', comment='#')\n    reads.rename(columns={reads.columns[-1]: 'Reads_quant'}, inplace=True)\n    print('Sorting &lt;gene&gt; table by Chromosome then Start position')\n    # New columns are simpler and will be used to properly reorder the table\n    print('\\tCreating temporary columns')\n    # Get unique Chr ID using a set\n    reads['Chr_new'] = reads['Chr'].apply(lambda x: ''.join(set(x.split(';'))))\n    # Select start of the first exon\n    reads['Start_new'] = reads['Start'].apply(lambda x: int(x.split(';')[0]))\n    print('\\tSorting table')\n    reads.sort_values(['Chr_new', 'Start_new'], ascending=[True, True],\n                      inplace=True)\n    print('\\tRemoving temporary columns')\n    reads.drop(['Chr_new', 'Start_new'], axis='columns', inplace=True)\n    final_table = reads[FIELDS].set_index('Geneid', drop=True)\n    return final_table\n\n\n# Main code execution\nif __name__ == '__main__':\n\n    with open(snakemake.log[0], 'w') as logfile:\n\n        # Redirect everything from the script to Snakemake log\n        sys.stderr = sys.stdout = logfile\n\n        print('Getting data from snakemake')\n        list_of_files = snakemake.input\n        count_table = snakemake.output.table\n\n        output_dir = os.path.dirname(count_table)\n        os.makedirs(output_dir, exist_ok=True)\n\n        print(f'Initialising global table with &lt;{list_of_files[0]}&gt;')\n        total_table = import_clean(list_of_files[0])\n\n        for file in list_of_files[1:]:\n            print(f'\\tAdding data from &lt;{file}&gt;')\n            tmp_table = import_clean(file)\n            total_table = pd.concat([total_table, tmp_table], axis=1)\n\n        print('Renaming columns')\n        column_titles = [os.path.basename(x).replace(STR_TO_REMOVE, '')\n                         for x in list_of_files]\n        total_table.columns = column_titles\n\n        print(f'Saving final table in &lt;{count_table}&gt;')\n        total_table.to_csv(count_table, sep='\\t', header=True, index=True)\n        print('Done')\n</code></pre> <p>Exercise: Get the script with your favourite method and place it the proper folder according to the official documentation. Where should you store it?</p> Answer <p>Scripts should be gathered in their own dedicated folder: <code>workflow/scripts</code>.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#deciding-how-to-run-the-script","title":"Deciding how to run the script","text":"<p>If you remember the presentation, there are two directives that you can use to run external scripts in Snakemake: <code>run</code> and <code>script</code>. While both allow to run Python code, they are not equivalent, so there is a choice to make!</p> <p>Exercise: Check out the script content. Depending on what you find, choose a directive and implement it in place of the last two missing elements (<code>?</code>) of rule <code>count_table</code>.</p> Script path is relative\u2026 <p>\u2026 to the Snakefile calling it. If you followed the recommended workflow structure, modular snakefiles are placed in a <code>rules</code> subfolder (like <code>rules/analysis.smk</code>) and scripts are placed in a <code>scripts</code> subfolder (like <code>scripts/count_table.py</code>). You need to find a path between those two subfolders.</p> Answer <p>There are two things to check before deciding which directive to use:</p> <ol> <li> <p>The script length:</p> <p>If we open the script in a text editor or run <code>wc -l workflow/scripts/count_table.py</code>, we see that it is 67 lines long. It is also quite complex, with function definitions, loops\u2026 This favours the <code>script</code> directive, as it\u2019s better to use <code>run</code> with short and simple code.</p> </li> <li> <p>The use of external packages (packages that are not included in a default Python installation):</p> <p>Another way to put this is: does the script need a special environment to work? If so, then we have to use the <code>script</code> directive, as it is the only one to accommodate for conda environments or containers. This means that this criteria takes precedence over the previous one: if we need to run a short script within a dedicated environment, <code>script</code> is the only way to do it.</p> <p>Here, there are several <code>import</code> statements at the top of the script, including <code>import pandas as pd  # Non-native package</code>. <code>pandas</code> is a great package, but it is not part of the default packages natively shipped with Python. This means that the script needs a dedicated environment to run and confirm that we need the <code>script</code> directive.</p> </li> </ol> <p>With this in mind, the rule will be: <pre><code>rule count_table:\n    '''\n    This rule merges gene count tables of an assembly into one table.\n    '''\n    input:\n        get_gene_counts\n    output:\n        count_table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    benchmark:\n        'benchmarks/total_count_table.txt'\n    resources:\n        mem_mb = 500\n    threads: 1\n    ?:\n        ?\n    script:  # Add script directive\n        '../scripts/count_table.py'  # Add script location relative to rule file\n</code></pre></p> Using the same Python script in and out of Snakemake <p>To avoid code redundancy, it would be ideal to have a script that can be called by Snakemake but also work with standard Python (and be used outside Snakemake). The two main ways to do this are:</p> <ul> <li>Implement the script as a module/package and use this module in Snakemake, for example with a command-line interface in <code>shell</code></li> <li>Test whether the <code>snakemake</code> object exists in the script:<ul> <li>If so, the script can process the Snakemake values<ul> <li>When the script is launched by Snakemake, there is an object called <code>snakemake</code> that provides access to the same objects that are available in the <code>run</code> and <code>shell</code> directives (<code>input</code>, <code>output</code>, <code>params</code>, <code>wildcards</code>, <code>log</code>, <code>threads</code>, <code>resources</code>, config). For instance, you can use <code>snakemake.input[0]</code> to access the first input file of a rule, or <code>snakemake.input.input_name</code> to access a named input</li> </ul> </li> <li>If not, the script can use other parameters, for example those coming from command-line parsing</li> </ul> </li> </ul>"},{"location":"course_material/day2/5_reproducibility_snakemake/#providing-a-rule-specific-conda-environment","title":"Providing a rule-specific conda environment","text":"<p>Given the presence of a non-default package in the script, we need to find a solution to make it accessible inside the rule. The easiest way to do that is to create a rule-specific conda environment. In Snakemake, you can do this by providing an environment config file (in YAML format) to the rule with the <code>conda</code> directive.</p> <p>(Optional) Exercise: If you have time, you can create your own config file for the environment using the tip on \u2018Environment features\u2019 below. If you need a reminder on how an environment file look, you can check out slide 19 of the presentation (available here). Otherwise, you can directly skip to the answer.</p> Environment features <ul> <li>Environment <code>name</code> is <code>py3.12</code></li> <li>It uses two <code>channels</code>: <code>conda-forge</code> and <code>bioconda</code> (in that order)</li> <li>It requires <code>python</code> with at least version <code>3.12</code></li> <li>It requires <code>pandas</code> with version <code>2.2.3</code> exactly</li> <li>Like with scripts, config files should be stored in their own dedicated folder: <code>workflow/envs</code></li> </ul> Answer <p>The config file, created in <code>workflow/envs/py.yaml</code> should look like this: <pre><code># Environment file to perform data processing with Python\nname : py3.12\nchannels :\n    - conda-forge\n    - bioconda\ndependencies :\n    - python &gt;= 3.12\n    - pandas == 2.2.3\n</code></pre></p> <p>Exercise: Add the conda environment to the rule.</p> Environment file path is relative\u2026 <p>\u2026 to the Snakefile calling it. If you followed the recommended workflow structure, modular snakefiles are placed in a <code>rules</code> subfolder (like <code>rules/analysis.smk</code>) and environment files are placed in a <code>envs</code> subfolder (like <code>envs/py.yaml</code>). You need to find a path between those two subfolders.</p> Answer <p>We need to fill the last two missing elements with the directive name, <code>conda</code>, and its value, the script location: <pre><code>    rule count_table:\n        '''\n        This rule merges gene count tables of an assembly into one table.\n        '''\n        input:\n            get_gene_counts\n        output:\n            count_table = 'results/total_count_table.tsv'\n        log:\n            'logs/total_count_table.log'\n        benchmark:\n            'benchmarks/total_count_table.txt'\n        resources:\n            mem_mb = 500\n        threads: 1\n        conda:  # Add conda directive\n            '../envs/py.yaml'  # Add config file location relative to rule file\n        script:\n            '../scripts/count_table.py'\n</code></pre></p> <p>Using conda environments improves reproducibility for many reasons, including version control and the fact that users do not need to manually manage software dependencies. The first workflow execution after adding Conda environments will take more time than usual because <code>snakemake</code> (through <code>conda</code>) has to download and install all the software in the working directory.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#adapting-the-snakefile-and-running-the-rule","title":"Adapting the Snakefile and running the rule","text":"<p>All that is left is running the rule to create the table.</p> <p>Exercise: Find the <code>snakemake</code> command you should run to create the desired output (which one is it?) and execute the workflow. Is there anything else to do beforehand?</p> Answer <p>We cannot launch the workflow directly: first, we need to update rule <code>all</code> input to use the output of rule <code>count_table</code>. After this, your Snakefile should be: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        rules.count_table.output.table  # New input matching output of rule `count_table`\n</code></pre></p> <p>Finally, run the workflow with: <pre><code>snakemake -c 4 -p --sdm=conda\n</code></pre> Do not forget to add <code>--sdm=conda</code>, otherwise Snakemake will not use the environment you provided.</p> <p>You should see the rule <code>count_table</code> executed with 6 files as input. You can also check the log of rule <code>count_table</code> to see if the script worked as intended.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#creating-a-rule-to-detect-differentially-expressed-genes-deg","title":"Creating a rule to detect Differentially Expressed Genes (DEG)","text":"<p>The final rule that you will create in this course will use an R script to process the global read count table previously created and detect differentially expressed genes. As such, you will see several common elements between this rule and rule <code>count_table</code> (external scripts, dedicated environments, rule structure\u2026) and the process to implement this rule will also be very similar. However, it will be easier as you won\u2019t need to use an input function.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#building-the-general-rule-structure-again","title":"Building the general rule structure (again)","text":"<p>We also wrote the common elements of the rule so that you can focus on the most interesting parts (the missing elements at the end):</p> <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots control graphs (PCA, heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    benchmark:\n        'benchmarks/differential_expression.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    ?:\n        ?\n    ?:\n        ?\n</code></pre> <p>Once again, we do not need to use <code>wildcards</code> in this rule, because all the files are precisely defined.</p> <p>Exercise: Given the rule structure above, update the <code>Snakefile</code> so that it creates the final output of the workflow.</p> Answer <p>There is only one thing to update in the target rule input: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        rules.differential_expression.output.deg  # New input matching output of rule `differential_expression`\n</code></pre> Remember that we don\u2019t need to add both outputs of rule <code>differential_expression</code> as inputs of rule <code>all</code>, only one suffices.</p> <p>As mentioned above, we don\u2019t need an input function because the input of rule <code>differential_expression</code> is easy to identify, so we\u2019ll directly focus on a way to run the R script.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#using-an-r-script-in-snakemake","title":"Using an R script in Snakemake","text":""},{"location":"course_material/day2/5_reproducibility_snakemake/#downloading-the-script","title":"Downloading the script","text":"<p>The DE analyses will be performed thanks to a script called <code>DESeq2.R</code>. It was written in R, takes a read count table as input, and produces two outputs, a tab-separated table containing DEG (and statistical results) and a .pdf file containing control plots of the analysis. You can download it here or with the command:</p> <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/docs/solutions_day2/session4/workflow/scripts/DESeq2.R\n</code></pre> <p>Exercise: Download the script and place it the right folder.</p> Answer <p>You can place this script in the same folder than the Python script, <code>workflow/scripts</code>. There is nothing in the official documentation about placing scripts from different languages in separate folders, but if you use a large number of scripts, it might be worth considering. You could also gather scripts by topic, similarly to <code>.smk</code> files.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#running-the-script","title":"Running the script","text":"<p>The next exercise won\u2019t be as guided as the other ones. This is done on purpose as you have seen everything you need to solve it!</p> <p>Exercise: Find a way to run the R script and fill the missing elements in the rule.</p> What do you need to take into account? <ul> <li>The directive you need to run the script</li> <li>The location/path of the script</li> <li>Check whether the script need a special environment<ul> <li>If so, remember a certain Docker image you created yesterday</li> </ul> </li> </ul> Answer <p>Like with the Python script, there are two problems to solve to run the R script:</p> <ol> <li> <p>Which directive to use?</p> <p>There isn\u2019t much of a choice here\u2026 If you remember the presentation, there is only one way to run non-Python code in Snakemake: the <code>script</code> directive: <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots control graphs (PCA, heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    benchmark:\n        'benchmarks/differential_expression.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    ?:\n        ?\n    script:  # Add script directive\n        '../scripts/DESeq2.R'  # Add script location relative to rule file\n</code></pre></p> </li> <li> <p>Does it use external packages and need a specific environment?</p> <p>If you look at the top of the script, you will see several (11 to be exact!) <code>library()</code> calls. Each of them imports an external package. All of these could be gathered in a conda environment, however when numerous libraries are involved, it is sometimes easier to use a container. During Day 1 - Session 3 (Working with Dockerfiles), you built your own Docker image, called <code>deseq2</code>. This image actually contains everything required by the script: <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots control graphs (PCA, heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    benchmark:\n        'benchmarks/differential_expression.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    container:  # Add container directive\n        'docker://geertvangeest/deseq2:v1'  # Try with your own image; if it doesn't work, use Geert's\n    script:\n        '../scripts/DESeq2.R'\n</code></pre></p> </li> </ol> Using the same R script in and out of Snakemake <p>Inside the script, an S4 object named <code>snakemake</code>, analogous to the Python one, is available and allows access to Snakemake objects: <code>input</code>, <code>output</code>, <code>params</code>, <code>wildcards</code>, <code>log</code>, <code>threads</code>, <code>resources</code>, and <code>config</code>. Here, the syntax follows that of S4 classes with attributes that are R lists. For example, you can access the first input file with <code>snakemake@input[[1]]</code> (remember that in R, indexing starts at 1). Named objects can be accessed the same way, by providing the name instead of an index: <code>snakemake@input[[\"myfile\"]]</code> to access the input called <code>myfile</code>.</p> <p>Now, all that is left is running the workflow, check its outputs and visualise its DAG!</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#running-the-workflow","title":"Running the workflow","text":"<p>Exercise: Run the workflow. How many DEGs are detected during the analysis?</p> Answer <p>You can run the workflow with: <pre><code>snakemake -c 4 -p --sdm=apptainer\n</code></pre> Do not forget to add <code>--sdm apptainer</code>, otherwise Snakemake will not pull the image and the script will be executed in the default environment (which will most likely lead to a crash).</p> <p>During the run, you should see log messages about Snakemake managing the Docker image: <pre><code>Pulling singularity image docker://geertvangeest/deseq2:v1.\n[...]\nActivating singularity image /path/to/snakemake_rnaseq/.snakemake/singularity/8bfdbe93244feb95887ab5d33a705017.simg\n</code></pre></p> <p>To find how many genes are differentially expressed, check out the last output file, <code>results/deg_list.tsv</code>. 10 genes are differentially expressed in total: 4 up-regulated and 6 down-regulated.</p> Containerisation of Conda-based workflows <p>Snakemake can also automatically generate a Dockerfile that contains all required environments in a human readable way. If you want to see how a Snakemake-generated Dockerfile looks like, use: <pre><code>snakemake -c 1 --containerize &gt; Dockerfile\n</code></pre></p> <p>(Optional) Exercise: If you had to re-run the entire workflow from scratch, what command would you use?</p> Answer <p>You can re-run the whole workflow with: <pre><code>snakemake -c 4 -p -F --sdm=conda --sdm=apptainer\n</code></pre></p> <ul> <li><code>-F</code> forces the execution of the entire workflow</li> <li>Remember that you need both Conda and Docker-based environments for this run! You can combine <code>--sdm conda</code> and <code>--sdm apptainer</code> into a single command <code>--sdm conda apptainer</code>. Otherwise, you will lack some software and packages and the workflow will crash!</li> </ul> <p>Exercise: Visualise the DAG of the entire workflow.</p> Answer <p>You can get the DAG with: <pre><code>snakemake -c 1 -p -F --dag | dot -T png &gt; images/total_dag.png\n</code></pre></p> <p>You should get the following DAG (open the picture in a new tab to zoom in):  </p> <p>Congratulations, you made it to the end! You are now able to create a Snakemake workflow and make it reproducible thanks to Conda and Docker/Apptainer! To make things even better, have a look at Snakemake\u2019s best practices!</p>"},{"location":"course_material/day2/6_debugging_snakemake/","title":"Additional concepts and workflow design/debugging","text":""},{"location":"course_material/day2/6_debugging_snakemake/#additional-advanced-concepts","title":"Additional advanced concepts","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/6_debugging_snakemake/#designing-a-workflow","title":"Designing a workflow","text":"<p>There are many ways to design a new workflow, but these few pieces of advice will be useful in most cases:</p> <ul> <li>Start with a pen and paper: try to find out how many rules you will need and how they depend on each other. In other terms, start by sketching the DAG of your workflow!<ul> <li>Remember that Snakemake has a bottom-up approach (it goes from the final outputs to the first input), so it may be easier for you to work in that order as well and write your last rule first</li> <li>Determine which rules (if any) aggregate or split inputs and create input functions accordingly (the topic is tackled in this series of exercises)</li> </ul> </li> <li>Make sure your input and output directives are right before worrying about anything else, especially the shell sections. There is no point in executing commands with the wrong inputs/outputs!<ul> <li>Remember that Snakemake builds the DAG before running the shell commands, so you can use the <code>-n/--dry-run/--dryrun</code> parameter to test the workflow before running it. You can even do that without writing all the shell commands!</li> </ul> </li> <li>List any parameters or settings that might need to be adjusted later</li> <li>Choose meaningful and easy-to-understand names for your rules, inputs, outputs, parameters, <code>wildcards</code>\u2026 to make your Snakefile as readable as possible. This is true for every script, piece of code, variable\u2026 and Snakemake is no exception! Have a look at The Zen of Python for more information</li> </ul>"},{"location":"course_material/day2/6_debugging_snakemake/#debugging-a-workflow","title":"Debugging a workflow","text":"<p>It is very likely you will see bugs and errors the first time you try to run a new Snakefile: don\u2019t be discouraged, this is normal!</p>"},{"location":"course_material/day2/6_debugging_snakemake/#order-of-operations-in-snakemake","title":"Order of operations in Snakemake","text":"<p>The topic was approached when we discussed DAGs, but to efficiently debug a workflow, it is worth taking a deeper look at what Snakemake does when you execute the command <code>snakemake -c 1 &lt;target&gt;</code>. There are 3 main phases:</p> <ol> <li>Prepare to run:<ol> <li>Read all the rule definitions from the Snakefile</li> </ol> </li> <li>Resolve the DAG (happens when Snakemake says \u2018Building DAG of jobs\u2019):<ol> <li>Check what output(s) are required</li> <li>Look for matching input(s) by looking at the outputs of all the rules</li> <li>Fill in the <code>wildcards</code> to determine the exact input(s) of the matching rule(s)</li> <li>Check whether this(these) input(s) is(are) available; if not, repeat Step 2 until everything is resolved</li> </ol> </li> <li>Run:<ol> <li>If needed, create the output(s) folder path</li> <li>If needed, remove the outdated output(s)</li> <li>Fill in the placeholders and run the shell commands</li> <li>Check that the commands ran without errors and produced the expected output(s)</li> </ol> </li> </ol>"},{"location":"course_material/day2/6_debugging_snakemake/#debugging-advice","title":"Debugging advice","text":"<p>Sometimes, Snakemake will give you a precise error report, but other times\u2026 less so. Try to identify which phase of execution failed (see previous paragraph on order of operations) and double-check the most common error causes for that phase:</p> <ol> <li>Parsing phase failures (phase 1):<ul> <li>Syntax errors, among which (but not limited to):<ul> <li>Missing commas/colons/semicolons</li> <li>Unbalanced quotes/brackets/parenthesis</li> <li>Wrong indentation</li> <li>These errors can be easily solved using a text editor with Python/Snakemake text colouring</li> </ul> </li> <li>Failure to evaluate expressions<ul> <li>Problems in functions (<code>expand()</code>, input functions\u2026) in input/output directives</li> <li>Python logic added outside of rules</li> </ul> </li> <li>Other problems with rule definition<ul> <li>Invalid rule names/directives</li> <li>Invalid wildcard names</li> <li>Mismatched <code>wildcards</code></li> </ul> </li> </ul> </li> <li>DAG building failures (phase 2, before Snakemake tries to run any job):<ul> <li>Failure to determine the target</li> <li>Ambiguous rules making the same output(s)</li> <li>On the contrary, no rule making the required output(s)</li> <li>Circular dependency (violating the \u2018Acyclic\u2019 property of a DAG).</li> <li>Write-protected output(s)</li> </ul> </li> <li>DAG running failures (phase 3, <code>--dry-run</code> works and builds the DAG, but the jobs execution fails):<ul> <li>Shell command returning non-zero status</li> <li>Missing output file(s) after the commands have run</li> <li>Reference to a <code>$shell_variable</code> before it was set</li> <li>Use of a wrong/unknown placeholder inside <code>{ }</code></li> <li>When a job fails, Snakemake reports an error, deletes all output file(s) for that job (because of potential corruption), and stops</li> </ul> </li> </ol>"}]}
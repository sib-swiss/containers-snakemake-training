{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course website","text":""},{"location":"#teachers","title":"Teachers","text":"<ul> <li>Antonin Thi\u00e9baut  </li> <li>Rafael Riudavets Puig  </li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Antonin Thi\u00e9baut  </li> <li>Rafael Riudavets Puig  </li> <li>Geert van Geest  </li> <li>Patricia Palagi  </li> </ul>"},{"location":"#attribution","title":"Attribution","text":"<p>This course is partly inspired by the Carpentries Docker course, the official Snakemake tutorial and the Introduction to Snakemake workshop from SIB-Days 2022.</p>"},{"location":"#material","title":"Material","text":"<ul> <li>This website</li> <li>Shared document (through email)</li> </ul>"},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>After this course, you will be able to:</p> <ul> <li>Understand the basic principles and advantages of Workflow Management Systems</li> <li>Create data analysis workflows with Snakemake</li> <li>Combine Snakemake with containers and conda environments to build reproducible computational workflows</li> <li>Run Snakemake workflows locally and in HPC environments</li> </ul>"},{"location":"#learning-outcomes-explained","title":"Learning outcomes explained","text":"<p>To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter (found at Course material) starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.</p>"},{"location":"#exercises","title":"Exercises","text":"<p>Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.</p>"},{"location":"#asking-questions","title":"Asking questions","text":"<p>During lectures, you are encouraged to raise your hand if you have questions. Feel free to speak up!</p> <p>Asking questions online:</p> <p>If the course is online, a main source of communication will be our Slack channel. Ask background questions that interest you personally at #background. During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the Slack channel #q-and-a.  This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option:</p> <p>The teachers will review the answers, and add/modify if necessary. If you are really stuck and need specific tutor support, write the teachers or helpers personally.</p> <p>To summarise:</p> <ul> <li>During lectures: raise hand</li> <li>Personal interest questions: #background on Slack</li> <li>During exercises: raise hand/#q-and-a on Slack</li> </ul>"},{"location":"course_schedule/","title":"Course schedule","text":"Session Start End Topic Session 1 9:00 09:45 Introduction to workflow management systems Exercises 1 09:45 10:15 Building a simple analysis workflow with Snakemake 10:15 10:30 BREAK Session 2 10:30 11:15 Using wildcards to increase scalability Exercises 2 11:15 12:00 Updating the analysis workflow to increase scalability 12:00 13:00 LUNCH BREAK Session 3 13:00 13:30 Using configuration files to increase transferability Exercises 3 13:30 14:00 Complementing our workflow with a configfile Session 4 14:00 14:30 Combining Snakemake with conda and software containers Exercises 4 14:30 15:15 Updating the analysis workflow to use conda/containers 15:15 15:30 BREAK Session 5 15:30 16:15 Snakemake in HPC environments Exercises 5 16:15 17:00 Updating our workflow to send jobs via SLURM"},{"location":"precourse/","title":"Precourse preparations","text":""},{"location":"precourse/#background-knowledge","title":"Background knowledge","text":"<p>As is stated in the course prerequisites at the announcement web page, we expect participants to have a basic understanding of working with the command line on UNIX-based systems and a a good understanding of Docker and Apptainer/Singularity containers.</p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>You can test your UNIX skills with a quiz here. If you don\u2019t have experience with UNIX command line, or if you are unsure whether you meet the prerequisites, follow our online UNIX tutorial.</p>"},{"location":"precourse/#dockerapptainer","title":"Docker/Apptainer","text":"<p>If you don\u2019t have experience with Docker or Apptainer containers, or if you are unsure whether you meet the prerequisites, check out the material of the dedicated SIB course.</p>"},{"location":"precourse/#software","title":"Software","text":""},{"location":"precourse/#os-and-terminal","title":"OS and terminal","text":"<p>If you are using a UNIX or UNIX-like OS (e.g MacOS), you already have a terminal readily usable for the course. If you are working with Windows, although Windows Powershell is suitable for that, we strongly recommend to install a UNIX or \u2018UNIX-like\u2019 terminal. You can get this by using MobaXterm or WSL2 (recommended solution).</p>"},{"location":"precourse/#code-editor","title":"Code editor","text":"<p>You should have a modern code editor installed. During the course, we can give only limited help for installation and set-up issues, so we will only \u201cofficially\u201d support VScode. If you are already very familiar with another combination of modern code editor/command line interface, feel free to use it, but know that support for this will be limited.</p> <p>Other required installations</p> <p>In addition to VScode, you would need to have followed the instructions to set up the <code>remote-ssh</code> extension:</p> <ul> <li>OpenSSH compatible client: this is usually pre-installed on your OS. You can check whether the command <code>ssh</code> exists by typing it in a terminal</li> <li>Remote-SSH extension: to install it, open VSCode and click on the extensions icon (four squares) on the left side of the window. Search for <code>Remote-SSH</code> and click on <code>Install</code></li> </ul>"},{"location":"precourse/#ssh-connections","title":"SSH connections","text":"<p>In addition to your local computer, you will be working on an Amazon Web Services (AWS) Elastic Cloud (EC2) server. This Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through <code>ssh</code>. If you are enrolled in the course, you have access to a shared document containing instruction to retrieve your username and private ssh key, granting you access to a personal home directory on the server.</p> If you want to know more about <code>ssh</code> <p>If you are not familiar with <code>ssh</code>, you can check the Heidelberg University tutorial for information on how to set it up and use it.</p> <p>Here are instructions on how to use VScode to connect with SSH to a remote server. First, place the <code>key_username.pem</code> file in the proper folder:</p> WindowsmacOS/Linux <p>Open a PowerShell terminal, <code>cd</code> to the directory where you have stored your private key (<code>key_username.pem</code> file) and move it to <code>~\\.ssh</code>: <pre><code>Move-Item -Path key_username.pem -Destination $HOME\\.ssh\n</code></pre></p> <p>Open a terminal, <code>cd</code> to the directory where you have stored your private key (<code>key_username.pem</code> file), change the permissions of the key file and move it to <code>~/.ssh</code>: <pre><code>chmod 400 key_username.pem\nmv key_username.pem ~/.ssh\n</code></pre></p> <p>Then:</p> <ul> <li>Open VScode and click on the green or blue button in the bottom left corner</li> <li>Select <code>Connect to Host...</code> and then <code>Configure SSH Hosts...</code></li> <li>Specify a location for the SSH config file (preferably the same directory as where your keys are stored): <code>~/.ssh/config</code></li> <li> <p>A skeleton config file will be provided. Edit it, so it looks like this (replace <code>username</code> with your username, and make sure the IP address in <code>HostName</code> match what the one given in the shared document):</p> WindowsMacOS/Linux <p><pre><code>Host sib_course_remote\n    User username\n    HostName 18.195.137.58\n    IdentityFile ~\\.ssh\\key_username.pem\n</code></pre> Note: if you are working with the Windows SSH executable (for example <code>C:\\WINDOWS\\System32\\OpenSSH\\ssh.exe</code>), you may have to use the full path of the key file instead of a relative one in <code>IdentityFile</code>: <pre><code>    IdentityFile C:\\Users\\&lt;windows_username&gt;\\.ssh\\key_username.pem\n</code></pre></p> <pre><code>Host sib_course_remote\n    User username\n    HostName 18.195.137.58\n    IdentityFile ~/.ssh/key_username.pem\n</code></pre> </li> </ul> <p>Finally:</p> <ul> <li>Save and close the config file</li> <li>Click again on the green or blue button in the bottom left corner</li> <li>Select <code>Connect to Host...</code>, and then <code>sib_course_remote</code>. You will be asked which operating system is used on the remote. Choose <code>Linux</code></li> </ul> <p>You can also find a video tutorial below:</p> If you are not working with VScode <p>If you are not working with VScode, you can login to the remote server with the following command in a terminal: <pre><code>ssh -i key_username.pem username@18.195.137.58\n</code></pre> If you want to edit files directly on the server, you can mount a directory with <code>sshfs</code>.</p>"},{"location":"course_material/1_guidelines/","title":"General guidelines","text":""},{"location":"course_material/1_guidelines/#course-goal","title":"Course goal","text":"<p>Throughout this course, you will build and refine a workflow to process bulk RNA-seq data. This includes trimming reads, aligning them to a reference genome, performing Quality Controls (QC), counting mapped reads, and identifying differentially expressed genes (DEGs). By the end of the course, you will have constructed a complete, functional workflow with commonly used Snakemake features. You will also have gained experience running the workflow both locally and on a High Performance Computing (HPC) environment. This workflow will be a useful reference for implementing your own workflows in the future.</p>"},{"location":"course_material/1_guidelines/#software","title":"Software","text":"<p>All the software needed in this workflow is either:</p> <ul> <li>Already installed in the <code>snakemake</code> conda environment available on the cluster</li> <li>Already installed in a Docker container</li> <li>Will be installed via conda environments/containers during today\u2019s exercises</li> </ul> <p>All information of this course is based on the official documentation for Snakemake version <code>9.11.6</code>.</p>"},{"location":"course_material/1_guidelines/#website-colour-code-explanation","title":"Website colour code explanation","text":"<p>We tried to use a colour code throughout the website to make the different pieces of information easily distinguishable. Here\u2019s a quick summary about the colour blocks you will encounter:</p> <p>This is a supplementary piece of information</p> <p>This is a tip to help you solve an exercise</p> <p>This is the answer to an exercise</p> <p>This is a warning about a potential problem</p> <p>This is an explanation about a common bug/error</p>"},{"location":"course_material/1_guidelines/#exercises","title":"Exercises","text":"<p>Each series of exercises is divided into multiple questions. Each question provides a background explanation, a description of the task at hand and additional details when required.</p> <p>Hints for challenging questions</p> <p>For the most challenging questions, hints will be provided. However, you should first try to solve the problem without them!</p>"},{"location":"course_material/1_guidelines/#answers","title":"Answers","text":"<p>Do not hesitate to modify and overwrite your code from previous answers as difficulty is incremental. The questions are designed to incite you to build your answers upon the previous ones.</p> <p>Restarting from a clean Snakefile</p> <ul> <li>If you feel that you drifted too far apart from the solution, you can always restart from files provided in the solutions folder of the course repository</li> <li>At the start of sessions 3, 4 and 5, you will also find a short note with a command to download the complete Snakefile from the previous session</li> </ul> <p>If something is not clear at any point, please call us and we will do our best to answer your questions! You can also check the official Snakemake documentation for more information.</p>"},{"location":"course_material/1_guidelines/#computing-environment","title":"Computing environment","text":"<p>Development and computation</p> <p>You can develop and write your scripts in a distant folder (using an <code>ssh</code> connection via VScode, recommended) or locally (if you do so, you will need to copy them on the server with <code>scp</code> before running them), but remember that all computation should be performed on the server, so don\u2019t forget to log in!.</p> <p><code>Error: Command not found</code></p> <p>If you try to run a command and get an error such as <code>Command 'snakemake' not found</code>, you are probably in the wrong conda environment:</p> <ul> <li>To list available conda environments, use <code>conda env list</code></li> <li>To activate an environment, use <code>conda activate &lt;env_name&gt;</code></li> <li>To deactivate an environment, use <code>conda deactivate</code></li> <li>To list packages installed in an environment, activate it and use <code>conda list</code>. The computing environment on the server is called <code>snakemake</code>.</li> </ul>"},{"location":"course_material/2_introduction_snakemake/","title":"Introduction to Snakemake","text":""},{"location":"course_material/2_introduction_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand the structure of a Snakemake workflow</li> <li>Write rules and Snakefiles to produce the desired outputs</li> <li>Chain rules together</li> <li>Run a Snakemake workflow</li> </ul>"},{"location":"course_material/2_introduction_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/2_introduction_snakemake/#structuring-a-workflow","title":"Structuring a workflow","text":"<p>It is advised to implement your code in a directory called <code>workflow</code> (you will learn more about workflow structure in the next series of exercises). Filenames and locations are up to you, but we recommend that you at least group all workflow outputs in a <code>results</code> folder.</p>"},{"location":"course_material/2_introduction_snakemake/#exercises","title":"Exercises","text":"<p>This series of exercises will bear no biological meaning, on purpose: it is designed to explain the fundamentals of Snakemake.</p>"},{"location":"course_material/2_introduction_snakemake/#creating-a-basic-rule","title":"Creating a basic rule","text":"<p>A rule is the smallest block of code with which you can build a workflow. It is a set of instructions to create one or more output(s) from zero or more input(s). When a rule is executed (in other words, applied to specific input/output file(s)), it is called a job. The definition of a rule always starts with the keyword <code>rule</code>. Similarly to Python classes and their attributes, rules have directives, which contain information about their properties.</p> <p>To create the simplest rule possible, you need at least two directives:</p> <ul> <li><code>output</code>: path of the output file</li> <li><code>shell</code>: shell commands that will create the output when they are executed</li> </ul> <p>Other directives will be explained throughout the course.</p> <p>Exercise: The following example shows the minimal syntax to implement a rule. What do you think it does? Does it create a file? If so, how is it called?</p> <pre><code>rule hello_world:\n    output:\n        'results/hello.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/hello.txt'\n</code></pre> Answer <p>This rule uses the <code>echo</code> shell command to print <code>Hello world!</code> in an output file called <code>hello.txt</code>, located in the <code>results</code> folder.</p> <p>Rules are defined and written in a file called Snakefile (note the capital <code>S</code> and the absence of extension in the filename). This file should be located at the workflow root directory (here, <code>workflow/Snakefile</code>).</p>"},{"location":"course_material/2_introduction_snakemake/#executing-a-workflow-with-a-specific-output","title":"Executing a workflow with a specific output","text":"<p>It is now time to execute your first workflow! To do this, you need to tell Snakemake what is your target, i.e. what is the specific output that you want to generate. A target can be any output from any rule in the workflow.</p> <p>Exercise: Create a Snakefile and copy the previous rule in it. Then, execute the workflow with <code>snakemake -c 1 &lt;target&gt;</code>. What value should you use for <code>&lt;target&gt;</code>? Once Snakemake execution is finished, can you locate the output file?</p> What does <code>-c/--cores</code> do? <p>The <code>-c/--cores N</code> parameter controls the maximum number of CPU cores used in parallel. If N is omitted or \u2018all\u2019, Snakemake will use all available CPU cores, which is useful but can also be dangerous on a High Performance Computing (HPC) cluster or a local machine. In case of cluster/cloud execution, this argument sets the maximum number of cores requested from the cluster or cloud scheduler.</p> Code indentation in Snakemake <p>As Snakemake is built on top of Python, proper code indentation is crucial. Wrong indentation often results in cryptic errors. We recommend using indents of 4 spaces, but here are two rules that should be followed at all times:</p> <ul> <li>Do not mix space and tab indents in a file</li> <li>Always use the same indent length</li> </ul> Answer <ul> <li>The target value is the file you want to generate, here <code>results/hello.txt</code>. The command to execute the workflow is: <pre><code>snakemake -c 1 results/hello.txt\n</code></pre></li> <li>The output is located in the <code>results</code> folder. You can check the folder content with <code>ls -alh results/</code></li> <li>You can check the output content with <code>cat results/hello.txt</code></li> </ul> <p>During the workflow execution, Snakemake automatically created the missing folder of the output path, <code>results/</code>. If several nested folders are missing (for example, <code>test1/test2/test3/hello.txt</code>), Snakemake will create the entire folder structure (<code>test1/test2/test3/</code>).</p> <p>Exercise: Re-run the exact same command. What happens?</p> Answer <p>Nothing! You get a message saying that Snakemake did not run anything: <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre> This is normal, because the desired output is already present and accounted for!</p> Snakemake re-run policy <p>By default, Snakemake runs a job if:</p> <ul> <li>A target file explicitly requested in the <code>snakemake</code> command is missing or an intermediate file is missing and is required to produce a target file</li> <li>It detects input files that have been modified more recently than output files, based on their modification dates. In this case, Snakemake will re-generate existing outputs</li> <li>Code (including <code>params</code> directive, see here for more information) has changed since last workflow execution</li> <li>Computing environment has changed since last workflow execution</li> </ul> <p>Snakemake re-runs can be forced:</p> <ul> <li>For a specific rule using the <code>-R/--forcerun</code> parameter: <code>snakemake -c 1 -R &lt;rule_name&gt;</code></li> <li>For a specific target using the <code>-f/--force</code> parameter: <code>snakemake -c 1 -f &lt;target&gt;</code></li> <li>For all workflow outputs using the <code>-F/--forceall</code> parameter: <code>snakemake -c 1 -F</code></li> </ul> <p>In practice, Snakemake re-run policy can be altered, but we will not cover this topic in the course (see \u2013rerun-triggers parameter in Snakemake CLI help and this git issue for more information).</p> <p>In the previous rule, the values of the two directives are strings. In the <code>shell</code> directive (other types of values will be seen later in the course), long strings (which includes software commands) can be written on multiple lines for clarity by encasing each line in quotes:</p> <pre><code>rule long_message:\n    output:\n        'results/long_message.txt'\n    shell:\n        'echo \"I want to print a very very very very very '\n        'very very very long string in my output\" &gt; results/long_message.txt'\n</code></pre> <p>Here, Snakemake will concatenate the two lines (i.e. paste the two lines together) and execute the resulting command: <pre><code>echo \"I want to print a very very very very very very very very very very long string in my output\" &gt; results/long_message.txt\n</code></pre></p>"},{"location":"course_material/2_introduction_snakemake/#understanding-the-input-directive","title":"Understanding the input directive","text":"<p>Another directive used by most rules is <code>input</code>. It usually indicates a path to a file required by the rule to create the output. In the following example, we wrote a rule that uses the file <code>results/hello.txt</code> as an input, and copies its content to <code>results/copied_file.txt</code>:</p> <pre><code>rule copy_file:\n    input:\n        'results/hello.txt'\n    output:\n        'results/copied_file.txt'\n    shell:\n        'cp results/hello.txt results/copied_file.txt'\n</code></pre> <p>You will use the <code>input</code> directive in the next exercises.</p>"},{"location":"course_material/2_introduction_snakemake/#creating-a-workflow-with-several-rules","title":"Creating a workflow with several rules","text":"<p>As you may have guessed from the previous rule, the <code>input</code> and <code>output</code> directives allow us to create links (also called dependencies) between rules and files. Here, the <code>input</code> of rule <code>copy_file</code> requires the <code>output</code> of rule <code>hello_world</code>. In other terms, this is\u2026 a workflow! Let\u2019s build one with two rules and run it!</p>"},{"location":"course_material/2_introduction_snakemake/#rule-order-matters","title":"Rule order matters!","text":"<p>Exercise: Add the rule <code>copy_file</code> to your Snakefile, after rule <code>hello_world</code>. Then, run the workflow without specifying an output with <code>snakemake -c 1</code>. What happens?</p> Your Snakefile should look like this <pre><code>rule hello_world:\n    output:\n        'results/hello.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/hello.txt'\n\nrule copy_file:\n    input:\n        'results/hello.txt'\n    output:\n        'results/copied_file.txt'\n    shell:\n        'cp results/hello.txt results/copied_file.txt'\n</code></pre> Answer <p>Nothing! You get the same message as before, saying that Snakemake did not run anything: <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre></p> <p>When you do not specify a target, the one selected by default is the output of the first rule in the Snakefile, here <code>results/hello.txt</code> of rule <code>hello_world</code>. While this behaviour may seem weird, it will prove very useful later! In this case, <code>results/hello.txt</code> already exists from your previous runs, so Snakemake doesn\u2019t recompute anything.</p> <p>Let\u2019s try to better understand how rule dependencies work in Snakemake.</p>"},{"location":"course_material/2_introduction_snakemake/#chaining-rules","title":"Chaining rules","text":"<p>The execution principle behind Snakemake is to create a Directed Acyclic Graph (DAG) that defines dependencies between all inputs and outputs of the workflow. Starting from jobs generating the final desired outputs, Snakemake checks whether required inputs exist. If they do not, it looks for a rule that can generate these inputs and so on until all dependencies are resolved. This is why Snakemake is said to have a \u2018bottom-up\u2019 approach: it starts from last outputs and go back to first inputs.</p> <code>MissingInputException</code> <p><code>MissingInputException</code> is a common error in Snakemake. It means that Snakemake couldn\u2019t find a way to generate targets during DAG computation because an input file is missing. This is a case of broken dependency between rules. This error is often caused by typos in input or output paths (for example, output of rule <code>hello_world</code> not matching input of rule <code>copy_file</code>), so make sure to double-check them!</p> <p>Exercise: With this in mind, identify the target you need to use to trigger the execution of rule <code>copy_file</code>. Add the <code>-F</code> parameter to the <code>snakemake</code> command and execute the workflow. What do you see?</p> What do we use <code>-F/--forceall</code> here? <p>The <code>-F/--forceall</code> parameter forces the re-creation of all workflow outputs. It is used here to avoid manually removing files, but it should be used carefully, especially with large workflows which contains a lot of outputs.</p> Answer <ul> <li>To trigger the execution of the second rule, you need to use <code>results/copied_file.txt</code> as target. The command is: <pre><code>snakemake -c 1 -F results/copied_file.txt\n</code></pre></li> <li>You should now see Snakemake execute two rules and produce both targets/outputs: to generate output <code>results/copied_file.txt</code>, Snakemake requires input <code>results/hello.txt</code>. Before the workflow is executed, this file does not exist, therefore, Snakemake looks for a rule that generates <code>results/hello.txt</code>, here rule <code>hello_world</code>. The process is then repeated for <code>hello_world</code>. In this case, the rule does not require any input, so all dependencies are resolved, and Snakemake can generate the DAG</li> </ul> <p>While it is possible to pass a space-separated list of targets in a Snakemake command, writing all the intermediary outputs does not look like a good idea: it is very time-consuming, error-prone\u2026 and annoying! Imagine what would happen with a workflow generating hundreds of files?! Using rule dependencies effectively solve this problem: you only need to ask Snakemake for the final outputs, and it will create the necessary intermediary outputs by itself!</p>"},{"location":"course_material/2_introduction_snakemake/#rule-dependencies-can-be-easier-to-write","title":"Rule dependencies can be easier to write","text":"<p>Creating rule dependencies using long file paths can be cumbersome, especially when you are dealing with a large number of files/rules. But there is a dedicated Snakemake syntax that makes this process easier to set-up: it is possible (and recommended!) to refer to the output of a rule in another rule with the following syntax: <code>rules.&lt;rule_name&gt;.output</code>. It has several advantages, among which:</p> <ul> <li>It limits the risk of error because you do not have to write filenames in several locations</li> <li>Changes in the output name are automatically propagated to rules that use it, which means that you only need to change the name once, in the rule that defines it</li> <li>It makes the code much clearer and easier to understand: with this syntax, you instantly know the object type (a <code>rule</code>), how/where it is created (<code>hello_world</code>), and what it is (an <code>output</code>)</li> </ul> Rules must produce unique outputs <p>Because of rule dependency, it is mandatory that an output be generated by a single rule. Rules generating the same output are called ambiguous. When Snakemake encounters ambiguous rules, it is not able to decide -at least by itself- which rule to use to generate this output, so it stops the execution. In reality, there are solutions to deal with ambiguous rules, but they should be avoided as much as possible, so we will not cover them in this course. See the official documentation for more information).</p> To quote or not to quote? <p>As opposed to strings, like <code>'results/hello.txt'</code>, quotes are not required around <code>rules.&lt;rule_name&gt;.output</code> statements, because they are Snakemake objects.</p> <p>The following example implements this syntax for the two rules defined above: <pre><code>rule hello_world:\n    output:\n        'results/hello.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/hello.txt'\n\nrule copy_file:\n    input:\n        rules.hello_world.output  # Dependency syntax\n    output:\n        'results/copied_file.txt'\n    shell:\n        'cp results/hello.txt results/copied_file.txt'\n</code></pre></p> <p>Try to use this syntax as much as possible in the next series of exercises!</p>"},{"location":"course_material/3_generalising_snakemake/","title":"Making a more general-purpose Snakemake workflow","text":""},{"location":"course_material/3_generalising_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Create rules with multiple inputs and outputs</li> <li>Make the code shorter and more general by using placeholders and <code>wildcards</code></li> <li>Visualise a workflow DAG</li> <li>(Check a workflow\u2019s behaviour)</li> </ul>"},{"location":"course_material/3_generalising_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/3_generalising_snakemake/#advice-and-reminders","title":"Advice and reminders","text":"<p>In each rule, you should try (as much as possible) to:</p> <ul> <li>Choose meaningful rule names</li> <li>Use placeholders and <code>wildcards</code><ul> <li>Choose meaningful wildcard names</li> <li>You can use the same wildcard names in multiple rules for consistency and readability, but remember that Snakemake will treat them as independent <code>wildcards</code> and their values will not be shared: rules are self-contained and <code>wildcards</code> are local to each rule (see this very nice summary on <code>wildcards</code>)</li> </ul> </li> <li>Use multiple (named) inputs/outputs when needed/possible</li> <li>Use rules dependency, with the syntax <code>rules.&lt;rule_name&gt;.output</code><ul> <li>If you use named outputs (recommended), the syntax becomes <code>rules.&lt;rule_name&gt;.output.&lt;output_name&gt;</code></li> <li>If you use numbered outputs (don\u2019t), the syntax becomes <code>rules.&lt;rule_name&gt;.output[n]</code>, with <code>n</code> starting at 0 (Python indexing)</li> </ul> </li> </ul>"},{"location":"course_material/3_generalising_snakemake/#testing-your-workflows-logic","title":"Testing your workflow\u2019s logic","text":"<ul> <li>If you have a doubt, do not hesitate to test your workflow logic with a dry-run (<code>-n/--dry-run/--dryrun</code> parameter): <code>snakemake -c 1 -n &lt;target&gt;</code>. Snakemake will then show all the jobs needed to generate <code>&lt;target&gt;</code> as well as a reason field explaining why each job is required</li> <li>To visualise the exact commands executed by each job (with placeholders and <code>wildcards</code> replaced by their values), run snakemake with the <code>-p/--printshellcmds</code> parameter: <code>snakemake -c 1 -p &lt;target&gt;</code></li> <li>These two parameters are often used together to check an entire workflow: <code>snakemake -c 1 -n -p &lt;target&gt;</code></li> </ul>"},{"location":"course_material/3_generalising_snakemake/#data-origin","title":"Data origin","text":"<p>The data you will use during the exercises was produced in this work. Briefly, the team studied the transcriptional response of a strain of baker\u2019s yeast, Saccharomyces cerevisiae, facing environments with different concentrations of CO<sub>2</sub>. To this end, they performed 150 bp paired-end sequencing of mRNA-enriched samples. Detailed information on all the samples are available here, but just know that for the purpose of the course, we selected 6 samples (3 replicates per condition, low and high CO<sub>2</sub>) and down-sampled them to 1 million read pairs each to reduce computation time.</p>"},{"location":"course_material/3_generalising_snakemake/#exercises","title":"Exercises","text":"<p>One of the aims of today\u2019s course is to develop a simple, yet efficient, workflow to analyse bulk RNAseq data. This workflow takes reads coming from RNA sequencing as inputs and produces a list of genes that are differentially expressed between two conditions. The files containing reads are in FASTQ format and the final output will be a tab-separated file containing a list of genes with expression changes, results of statistical tests\u2026</p> <p>In this series of exercises, you will create the workflow \u2018backbone\u2019, i.e. rules that are the most computationally expensive, namely:</p> <ul> <li>A rule to trim poor-quality reads</li> <li>A rule to map trimmed reads on a reference genome</li> <li>A rule to convert and sort files from SAM format to BAM format</li> <li>A rule to count reads mapping on each gene</li> </ul> Designing and debugging a workflow <p>If you have problems designing your Snakemake workflow or debugging it, you can find some help here.</p> <p>At the end of this series of exercises, your workflow should look like this:</p> <p> </p> Workflow rulegraph at the end of the session"},{"location":"course_material/3_generalising_snakemake/#downloading-data-and-setting-up-folder-structure","title":"Downloading data and setting up folder structure","text":"<p>In this part, you will download the data and start building the directory structure of your workflow according to the official recommendations. You already started doing so in the previous series of exercises and at the end of the course, it should resemble this: <pre><code>\u2502\u2500\u2500 .gitignore\n\u2502\u2500\u2500 README.md\n\u2502\u2500\u2500 LICENSE.md\n\u2502\u2500\u2500 benchmarks\n\u2502   \u2502\u2500\u2500 sample1.txt\n\u2502   \u2514\u2500\u2500 sample2.txt\n\u2502\u2500\u2500 config\n\u2502   \u2502\u2500\u2500 config.yaml\n\u2502   \u2514\u2500\u2500 some-sheet.tsv\n\u2502\u2500\u2500 data\n\u2502   \u2502\u2500\u2500 sample1.fastq\n\u2502   \u2514\u2500\u2500 sample2.fastq\n\u2502\u2500\u2500 images\n\u2502   \u2502\u2500\u2500 dag.png\n\u2502   \u2502\u2500\u2500 filegraph.png\n\u2502   \u2514\u2500\u2500 rulegraph.png\n\u2502\u2500\u2500 logs\n\u2502   \u2502\u2500\u2500 sample1.log\n\u2502   \u2514\u2500\u2500 sample2.log\n\u2502\u2500\u2500 results\n\u2502   \u2502\u2500\u2500 DEG_list.tsv\n\u2502   \u2502\u2500\u2500 sample1\n\u2502   \u2502   \u2514\u2500\u2500 sample1.bam\n\u2502   \u2514\u2500\u2500 sample2\n\u2502       \u2514\u2500\u2500 sample2.bam\n\u2502\u2500\u2500 resources\n\u2502   \u2502\u2500\u2500 Scerevisiae.fasta\n\u2502   \u2502\u2500\u2500 Scerevisiae.gtf\n\u2502   \u2514\u2500\u2500 genome_indices\n|       \u2502\u2500\u2500 Scerevisiae_index.1.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.2.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.3.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.4.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.5.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.6.ht2\n|       \u2502\u2500\u2500 Scerevisiae_index.7.ht2\n|       \u2514\u2500\u2500 Scerevisiae_index.8.ht2\n\u2514\u2500\u2500 workflow\n    \u2502\u2500\u2500 Snakefile\n    \u2502\u2500\u2500 envs\n    \u2502   \u2502\u2500\u2500 tool1.yaml\n    \u2502   \u2514\u2500\u2500 tool2.yaml\n    \u2502\u2500\u2500 rules\n    \u2502   \u2502\u2500\u2500 module1.smk\n    \u2502   \u2514\u2500\u2500 module2.smk\n    \u2514\u2500\u2500 scripts\n        \u2502\u2500\u2500 script1.py\n        \u2514\u2500\u2500 script2.R\n</code></pre></p> <p>For now, the main thing to remember is that code should go into the <code>workflow</code> subfolder  and the rest is mostly input/output files. The only exception is the <code>config</code> subfolder, but it will be explained later. All output files generated in the workflow should be stored under <code>results/</code>.</p> <p>Let\u2019s download the data, uncompress them and build the first part of the directory structure. Make sure you are connected to server, then run this in your VScode terminal: <pre><code>wget https://containers-snakemake-training.s3.eu-central-1.amazonaws.com/snakemake_rnaseq.tar.gz  # Download data\ntar -xvf snakemake_rnaseq.tar.gz  # Uncompress archive\nrm snakemake_rnaseq.tar.gz  # Delete archive\ncd snakemake_rnaseq/  # Start developing in new folder\n</code></pre></p> <p>In <code>snakemake_rnaseq/</code>, you should see two subfolders:</p> <ul> <li><code>data/</code>, which contains data to analyse</li> <li><code>resources/</code>, which contains retrieved resources, here assembly, genome indices and annotation file of S. cerevisiae. It may also contain small resources delivered along with the workflow</li> </ul> <p>We also need to create the other missing subfolders and the Snakefile: <pre><code>mkdir -p config/ images/ workflow/envs workflow/rules workflow/scripts  # Create subfolder structure\ntouch workflow/Snakefile  # Create empty Snakefile\n</code></pre></p> What does <code>-p</code> do? <p>The <code>-p</code> parameter of <code>mkdir</code> make parent directories as needed and does not return an error if the directory already exists.</p> <p>Snakefile marks the workflow entry point. It will be automatically discovered when running Snakemake from the root folder, here <code>snakemake_rnaseq/</code>.</p> Using a Snakefile from a non-default location <p>Snakemake can use a Snakefile from a non-default location thanks to the <code>-s/--snakefile</code> parameter: <pre><code>snakemake -c 1 -s &lt;Snakefile_path&gt; &lt;target&gt;\n</code></pre></p> <p>However, it is highly discouraged as it hampers reproducibility.</p> Relative paths in Snakemake <p>All paths in a Snakefile are relative to the working directory in which the <code>snakemake</code> command is executed:</p> <ul> <li>If you execute Snakemake in <code>snakemake_rnaseq/</code>, the relative path to the input files in the rule is <code>data/&lt;sample&gt;.fastq</code></li> <li>If you execute Snakemake in <code>snakemake_rnaseq/workflow/</code>, the relative path to the input files in the rule is <code>../data/&lt;sample&gt;.fastq</code></li> </ul> <p>If you followed the advice at the top of this page, Snakemake should create all the other missing folders by itself, so it is time to create the rules mentioned earlier. If needed, you can check here for a few pieces of advice on workflow design.</p> \u2018bottom-up\u2019 or \u2018top-down\u2019 development? <p>Even if it is often easier to start from final outputs and work backwards to first inputs, the next exercises are presented in the opposite direction (first inputs to last outputs) to make the session easier to understand.</p>"},{"location":"course_material/3_generalising_snakemake/#important-do-not-process-all-the-samples","title":"Important: do not process all the samples!","text":"<p>Do not try to process all the samples yet, even if we asked you to use wildcards. For now, choose only one sample (which means two .fastq files because reads are paired-end). You will see an efficient way to process a list of files in the next series of exercises.</p>"},{"location":"course_material/3_generalising_snakemake/#creating-a-rule-to-trim-reads","title":"Creating a rule to trim reads","text":"<p>Usually, when dealing with sequencing data, the first step is to improve read quality by removing low quality bases, stretches of As and Ns and reads that are too short.</p> Trimming sequencing adapters <p>In theory, trimming should also remove sequencing adapters, but you will not do it here to keep computation time low and avoid parsing other files to extract adapter sequences.</p> <p>You will use atropos to trim reads. The first part of the trimming command is: <pre><code>atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\"\n</code></pre></p> Explanation of atropos parameters <ul> <li><code>-q 20,20</code>: trim low-quality bases from 5\u2019 and 3\u2019 ends of each read before adapter removal</li> <li><code>--minimum-length 25</code>: discard trimmed reads that are shorter than 25 bp</li> <li><code>--trim-n</code>: trim Ns at the ends of reads</li> <li><code>--preserve-order</code>: preserve order of reads in input files</li> <li><code>--max-n 10</code>: discard reads with more than 10 Ns</li> <li><code>--no-cache-adapters</code>: do not cache adapters list as \u2018.adapters\u2019 in the working directory</li> <li><code>-a \"A{{20}}\" -A \"A{{20}}\"</code>: remove series of 20 As in adapter sequences (<code>-a</code> for first read of the pair, <code>-A</code> for the second one)<ul> <li>The usual command line syntax is <code>-a \"A{20}\"</code>. Here, brackets were doubled to prevent Snakemake from interpreting <code>{20}</code> as a wildcard</li> </ul> </li> </ul> <p>Now, a few questions might come to mind when you need to use specific software in a workflow:</p> <ul> <li>Is the software already installed in the machine I am working on?</li> <li>If not, how do I install it quickly and easily?</li> <li>How can I make sure that everyone using my workflow has it installed? With the exact same version?!</li> </ul> <p>To solve this problem, Snakemake can use package managers (more on this later) or container managers, like <code>docker</code> and <code>apptainer</code>, to deploy rule-specific environments. The latter is done with the <code>container</code> directive and its value should be the location of the image: it can be either a local path or a remote URL. Allowed URLs are everything supported by <code>apptainer</code>, including <code>shub://</code> and <code>docker://</code>.</p> <p>Exercise:</p> <ul> <li>Complete the atropos command given above with parameters to specify inputs (files to trim) and outputs (trimmed files)<ul> <li>You can find information on how to use <code>atropos</code> and its parameters with <code>atropos trim -h</code> or you can look at the tip below</li> </ul> </li> <li>Implement a rule containing your command to trim reads contained in a .fastq files<ul> <li>You will need a rule name, and the <code>input</code>, <code>output</code>, <code>container</code> and <code>shell</code> directives</li> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2</code></li> </ul> </li> </ul> atropos inputs and outputs <ul> <li>.fastq files to trim are located in <code>data/</code></li> <li>Paths of files to trim (i.e. input files, in FASTQ format) are specified with the parameters <code>-pe1</code> (first read) and <code>-pe2</code> (second read)</li> <li>Paths of trimmed files (i.e. output files, also in FASTQ format) are specified with the parameters <code>-o</code> (first read) and <code>-p</code> (second read)</li> </ul> Answer <p>This is one way of writing this rule, but definitely not the only way (this is true for all the rules presented in these exercises): <pre><code>rule fastq_trim:\n    \"\"\"\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    \"\"\"\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    container:\n        'https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2'\n    shell:\n        '''\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2}\n        '''\n</code></pre></p> <p>There are three interesting things happening here:</p> <ol> <li>We added a comment between double quotes under the rule name (L2-7). In Python, it is called a docstring. While it is not mandatory, it is good practice (and very recommended) to write docstrings to explain what a rule does, what are its inputs, outputs, parameters\u2026</li> <li>We used the <code>{sample}</code> wildcards twice in output paths (L12-13). This is because we prefer to have all files from a single sample in the same directory</li> <li>We used a backslash <code>\\</code> at the end of L18-19 to split a very long command in smaller lines. This is purely \u2018cosmetic\u2019 but it avoids very long lines that are painful to read, understand and debug\u2026</li> </ol> <p>Exercise: If you had to run the workflow by specifying only one output, what command would you use?</p> Answer <p>To process the sample <code>highCO2_sample1</code>, for example, you would use: <pre><code>snakemake -c 1 -p --sdm=apptainer results/highCO2_sample1/highCO2_sample1_atropos_trimmed_1.fastq\n</code></pre></p> <ul> <li>You don\u2019t need to ask for the two outputs of the rule: asking only for one will still trigger execution, but the workflow will complete without errors if and only if both outputs are present. Like with intermediary files, this property also helps reducing the number of targets to write in the snakemake command used to execute the workflow!</li> <li>Do not forget to add <code>--sdm=apptainer</code>, otherwise Snakemake will not pull the image and the command will be executed in the default environment (which will most likely lead to a crash). Don\u2019t worry if the first execution is somewhat slow: Snakemake has to download the image. The next ones will be much faster as the images are cached</li> </ul>"},{"location":"course_material/3_generalising_snakemake/#creating-a-rule-to-map-trimmed-reads-onto-a-reference-genome","title":"Creating a rule to map trimmed reads onto a reference genome","text":"<p>Once the reads are trimmed, the next step is to map those reads onto the species genome (S. cerevisiae strain S288C) to eventually obtain read counts. The reference assembly used in this exercise is RefSeq GCF_000146045.2 and was retrieved via the NCBI website. You will use HISAT2 to map reads.</p> HISAT2 genome index <p>To align reads to a genome, HISAT2 relies on a graph-based index. To save some time, we built the genome index for you, using: <pre><code>hisat2-build -p 24 -f resources/Scerevisiae.fasta resources/genome_indices/Scerevisiae_index\n</code></pre> The parameters are:</p> <ul> <li><code>-p</code>: number of threads to use</li> <li><code>-f</code>: genomic sequence in FASTA format</li> <li><code>Scerevisiae_index</code>: global name shared by all the index files</li> </ul> <p>Genome indices can be found in <code>resources/genome_indices/</code>.</p> <p>The first part of the mapping command is: <pre><code>hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal\n</code></pre></p> Explanation of HISAT2 parameters <ul> <li><code>--dta</code>: report alignments tailored for transcript assemblers</li> <li><code>--fr</code>: set alignment of -1, -2 mates to forward/reverse (position of reads in a pair relatively to each other)</li> <li><code>--no-mixed</code>: remove unpaired alignments for paired reads</li> <li><code>--no-discordant</code>: remove discordant alignments for paired reads</li> <li><code>--time</code>: print wall-clock time taken by search phases</li> <li><code>--new-summary</code>: print alignment summary in a new style</li> <li><code>--no-unal</code>: suppress SAM records for reads that failed to align</li> </ul> <p>Exercise:</p> <ul> <li>Complete the HISAT2 command given above with parameters to specify inputs and outputs<ul> <li>You will need 2 inputs, 2 outputs (in 2 different formats) and the genome indices mentioned above (should they be considered as inputs?)<ul> <li>You can find more information on how to use HISAT2 and its parameters with <code>hisat2 -h</code> or you can look at the tip below</li> </ul> </li> </ul> </li> <li>Implement a rule containing your command to map trimmed reads contained in .fastq files<ul> <li>You will need a rule name, and the <code>input</code>, <code>output</code>, <code>container</code> and <code>shell</code> directives</li> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6</code></li> </ul> </li> </ul> HISAT2 inputs and outputs <ul> <li>Paths of trimmed files (i.e. input files) are specified with the parameters <code>-1</code> (first read) and <code>-2</code> (second read)</li> <li>Basename of genome indices (binary format) is specified with the parameter <code>-x</code>. The files have a shared name of <code>resources/genome_indices/Scerevisiae_index</code>, which is the value you need to use for <code>-x</code></li> <li>Path of mapped reads files (i.e. output file, in SAM format) is specified with the parameter <code>-S</code> (do not forget the .sam extension at the end of the filename)</li> <li>Path of mapping report (i.e. output file, in text format) is specified with the parameter <code>--summary-file</code></li> </ul> Answer <pre><code>rule read_mapping:\n    \"\"\"\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    \"\"\"\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x resources/genome_indices/Scerevisiae_index \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report}\n        '''\n</code></pre> <p>Exercise: What do you think about the value of <code>-x</code>?</p> Answer <p>Something very interesting is happening here: to run, HISAT2 requires several genome index files. As such, they could (should?) be considered as inputs\u2026 and this is a problem:</p> <ul> <li>On one hand, the <code>-x</code> parameter only accepts strings containing a file path and common name. This means that if you were to manually add all the indices as inputs, HISAT2 would not recognize them\u2026 and crash!</li> <li>On the other hand, if you were add the value of <code>-x</code> as input, Snakemake would look for a file called <code>resources/genome_indices/Scerevisiae_index</code>\u2026 and crash because this file doesn\u2019t exist!</li> </ul> <p>This highlights the fact that inputs must be files and this is why we directly added the value of <code>-x</code> in the command. However, this is not very convenient: you will see later a better way to deal with this problem.</p> <p>Using the same sample as before (<code>highCO2_sample1</code>), the workflow can be run with: <pre><code>snakemake -c 1 --sdm=apptainer -p results/highCO2_sample1/highCO2_sample1_mapped_reads.sam\n</code></pre></p> <p>That being said, we recommend not to run it for the moment, because this step is the longest of the workflow (with current settings, it will take ~6 min to complete). Still, if you want to run it now, you can, but you should launch it and start working on the next rules while it finishes.</p>"},{"location":"course_material/3_generalising_snakemake/#creating-a-rule-to-convert-and-sort-sam-files-to-bam-format","title":"Creating a rule to convert and sort .sam files to BAM format","text":"<p>HISAT2 only outputs mapped reads in SAM format. However, most downstream analysis tools use BAM format, which is the compressed binary version of SAM format and, as such, is much smaller, easier to manipulate and transfer and allows a faster data retrieval. Additionally, many analyses require .bam files to be sorted by genomic coordinates and indexed because sorted .bam files can be processed much more easily and quickly than unsorted ones. Operations on .sam and .bam files are usually performed with Samtools.</p> What are alignment formats? <p>More information on alignment formats can be found on samtools github repository.</p> <p>We wrote a single rule performing all these operations for you: <pre><code>rule sam_to_bam:\n    \"\"\"\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    \"\"\"\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    container:\n        'https://depot.galaxyproject.org/singularity/samtools%3A1.21--h50ea8bc_0'\n    shell:\n        '''\n        samtools view {input.sam} -b -o {output.bam}\n        samtools sort {output.bam} -O bam -o {output.bam_sorted}\n        samtools index -b {output.bam_sorted} -o {output.index}\n        '''\n</code></pre></p> Explanation of Samtools parameters <ul> <li>You can find information on how to use Samtools and its parameters with <code>samtools --help</code></li> <li><code>samtools view</code>:<ul> <li><code>-b</code>: create an output in BAM format</li> <li><code>-o</code>: path of output file</li> </ul> </li> <li><code>samtools sort</code>:<ul> <li><code>-O bam</code>: create an output in BAM format</li> <li><code>-o</code>: path of output file</li> </ul> </li> <li><code>samtools index</code>:<ul> <li><code>-b</code>: create an index in BAI format</li> </ul> </li> </ul> <p>Exercise: Copy this rule in your Snakefile. Don\u2019t forget to update the output values to match the ones you used in your previous rules. Do you notice anything surprising in this rule?</p> Answer <p>Let\u2019s start with a quick breakdown of the <code>shell</code> directive:</p> <ul> <li>L15: <code>samtools view</code> converts a file in SAM format to BAM format</li> <li>L16: <code>samtools sort</code> sorts a .bam file by genomic coordinates</li> <li>L17: <code>samtools index</code> indexes a sorted .bam file. The index must have the exact same basename as its associated .bam file; the only difference is that it finishes with the extension <code>.bam.bai</code> instead of <code>.bam</code></li> </ul> <p>The interesting thing is that so far, all the rules had only one command in the <code>shell</code> directive. In this rule, there are three commands grouped together, each with their own inputs and outputs. This means two things:</p> <ol> <li>We could have split this rule into 3 separate rules with dependencies. There is no official guideline on whether to split rules like this, but a good rule of thumb is: does it make sense to run these commands together? Is it not too computationally expensive (time, memory, CPU) to run these rules together?</li> <li><code>{output.bam}</code> and <code>{output.bam_sorted}</code> are outputs of  <code>samtools view</code> and  <code>samtools sort</code>\u2026 But they are also inputs of  <code>samtools sort</code> and  <code>samtools index</code>! This means that files that are created by a command can instantly be re-used in subsequent commands within a same rule!</li> </ol> <p>Exercise: Do you see any drawbacks to using a rule like this?</p> Answer <p>When Snakemake has a problem and crashes, it removes the current rule outputs to avoid further computation with corrupted files. This means that if the first two commands complete but the last one (here, <code>samtools index</code>) fails and doesn\u2019t produce the expected output, Snakemake will remove all the outputs, including those that were created without error (here, <code>{output.bam}</code> and <code>{output.bam_sorted}</code>), causing a waste of time and money</p> <p>Using the same sample as before (<code>highCO2_sample1</code>), the workflow can be run with: <pre><code>snakemake -c 1 -p --sdm=apptainer results/highCO2_sample1/highCO2_sample1_mapped_reads_sorted.bam\n</code></pre> However, you will soon run the entire workflow, so it might be worth waiting!</p>"},{"location":"course_material/3_generalising_snakemake/#creating-a-rule-to-count-mapped-reads","title":"Creating a rule to count mapped reads","text":"<p>Most of analyses happening downstream the alignment step, including Differential Expression Analyses, are starting off read counts, by exon or by gene. However, you are still missing those counts!</p> Genome annotations <ul> <li>To count reads mapping on genomic features, we first need a definition of those features, called annotations. Here, we chose one of the best-known model organism, S. cerevisiae, which has been annotated for a long time. Its annotations are easily findable on the NCBI or the Saccharomyces Genome Database. If an organism has not been annotated yet, there are ways to work around this problem, but this is an entirely different field that we won\u2019t discuss here!</li> <li>You should also know that there are two main annotations format: GTF and GFF. Former is lighter and easier to work with, so this is what you will use</li> </ul> Chromosome names must match between files <p>If you are working with genome sequences and annotations from different sources, remember that they must contain matching chromosome names, otherwise counting will not work, as the counting software will not be able to read counts to match exon/gene locations.</p> <p>We already wrote a rule to count reads mapping on each gene of the S. cerevisiae genome using featureCounts: <pre><code>rule reads_quantification_genes:\n    \"\"\"\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly.\n    \"\"\"\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -o {output.gene_level} {input.bam_once_sorted}\n        mv {output.gene_level}.summary {output.gene_summary}\n        '''\n</code></pre></p> Explanation of featureCounts command <ul> <li>You can find information on how to use featureCounts and its parameters with <code>featureCounts -h</code></li> <li><code>-t</code>: specify on which feature type to count reads</li> <li><code>-g</code>: specify if and how to gather feature counts. Here, reads are counted by features (exon) (<code>-t</code>) and exon counts are gathered by \u2018meta-features\u2019 (genes) (<code>-g</code>)</li> <li><code>-s</code>: perform strand-specific read counting<ul> <li>Strandedness is determined by looking at mRNA library preparation kit. It can also be determined a posteriori with scripts such as infer_experiment.py from the RSeQC package</li> </ul> </li> <li><code>-p</code>: specify that input data contain paired-end reads</li> <li><code>--countReadPairs</code>: count read pairs instead of reads</li> <li><code>-B</code>: only count read pairs that have both ends aligned</li> <li><code>-C</code>: do not count read pairs that have their two ends mapping to different chromosomes or mapping on same chromosome but on different strands</li> <li><code>--largestOverlap</code>: assign reads to meta-feature/feature that has largest number of overlapping bases</li> <li><code>--verbose</code>: output verbose information, such as unmatched chromosome/contig names</li> <li><code>-F</code>: specify format of annotation file</li> <li><code>-a</code>: specify path of file containing annotations (i.e. input files, in GTF format)</li> <li><code>-o</code>: specify path of file containing count results (i.e. output file, in tsv format)</li> <li>Paths of sorted .bam file(s) (i.e. input file(s)) are not specified with an parameter, they are simply added at the end of the command</li> </ul> <p>Exercise: Copy this rule in your Snakefile. What does L18 do? Why did we add it?</p> Answer <p>The <code>mv</code> command can be used to move or rename a file. Here, it does the latter. featureCounts outputs a second, separate file (in tsv format) containing summary statistics about read counting, with the name <code>&lt;output_name&gt;.summary</code>. For example, if the output is <code>test.tsv</code>, summary will be printed in <code>test.tsv.summary</code>. However, there is no parameter available to choose the filename, so if we need this file as an output, we have to manually rename it.</p> <p>It would be interesting to know what is happening when featureCounts runs. This is where the <code>log</code> directive comes into play!</p> <p>(Optional) Exercise: If you have time, add the <code>log</code> directive to the rule. Don\u2019t forget to update the directive values to match the ones you used in your previous rules. You can check out slides 27-30 of the presentation (available here) for information on this directive.</p> Logs <ul> <li>The <code>log</code> directive must contain the same <code>wildcards</code> as the <code>output</code> directive, here <code>sample</code></li> <li>Logs need to be handled manually, so you need to redirect what is produced by featureCounts to the log file. You can redirect both <code>stdout</code> and <code>stderr</code> streams with <code>&amp;&gt; {log}</code> at the end of the command</li> </ul> Answer <pre><code>rule reads_quantification_genes:\n    \"\"\"\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly.\n    \"\"\"\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:  # log directive\n        'logs/{sample}/{sample}_genes_read_quantification.log'  # Path of log file\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        '''\n</code></pre>"},{"location":"course_material/3_generalising_snakemake/#running-the-whole-workflow","title":"Running the whole workflow","text":"<p>Exercise: If you have not done it after each step, run the entire workflow on your sample of choice. What command will you use to run it? Once Snakemake has finished, check the log of rule <code>reads_quantification_genes</code>. Does it contain anything? How many read pairs were assigned to a feature?</p> Answer <p>Because all rules are chained together, you only need to specify one final output to trigger the execution of all previous rules. Using the same sample as before (<code>highCO2_sample1</code>). You can add the <code>-F</code> parameter to force an entire re-run, which should take ~10 min.: <pre><code>snakemake -c 1 -F --sdm=apptainer -p results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n</code></pre> If you used the same path for the log file as the rule given above, you can check it with: <pre><code>cat logs/highCO2_sample1/highCO2_sample1_genes_read_quantification.log\n</code></pre> In the log files, you can find a summary of the parameters used by <code>featureCounts</code>, its inputs and outputs\u2026 and the number of read pairs successfully assigned to a gene; with <code>highCO2_sample1</code>, 817894 read pairs (83.8% of total) were assigned: <pre><code>        ==========     _____ _    _ ____  _____  ______          _____\n        =====         / ____| |  | |  _ \\|  __ \\|  ____|   /\\   |  __ \\\n          =====      | (___ | |  | | |_) | |__) | |__     /  \\  | |  | |\n            ====      \\___ \\| |  | |  _ &lt;|  _  /|  __|   / /\\ \\ | |  | |\n              ====    ____) | |__| | |_) | | \\ \\| |____ / ____ \\| |__| |\n        ==========   |_____/ \\____/|____/|_|  \\_\\______/_/    \\_\\_____/\n      v2.0.6\n\n//========================== featureCounts setting ===========================\\\\\n||                                                                            ||\n||             Input files : 1 BAM file                                       ||\n||                                                                            ||\n||                           highCO2_sample1_mapped_reads_sorted.bam          ||\n||                                                                            ||\n||             Output file : highCO2_sample1_genes_read_quantification.tsv    ||\n||                 Summary : highCO2_sample1_genes_read_quantification.ts ... ||\n||              Paired-end : yes                                              ||\n||        Count read pairs : yes                                              ||\n||              Annotation : Scerevisiae.gtf (GTF)                            ||\n||      Dir for temp files : results/highCO2_sample1                          ||\n||                                                                            ||\n||                 Threads : 1                                                ||\n||                   Level : meta-feature level                               ||\n||      Multimapping reads : not counted                                      ||\n|| Multi-overlapping reads : not counted                                      ||\n||   Min overlapping bases : 1                                                ||\n||                                                                            ||\n\\\\============================================================================//\n\n//================================= Running ==================================\\\\\n||                                                                            ||\n|| Load annotation file Scerevisiae.gtf ...                                   ||\n||    Features : 7507                                                         ||\n||    Meta-features : 7127                                                    ||\n||    Chromosomes/contigs : 17                                                ||\n||                                                                            ||\n|| Process BAM file highCO2_sample1_mapped_reads_sorted.bam...                ||\n||    Strand specific : reversely stranded                                    ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 975705                                               ||\n||    Successfully assigned alignments : 817894 (83.8%)                       ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Write the final count table.                                               ||\n|| Write the read assignment summary.                                         ||\n||                                                                            ||\n|| Summary of counting results can be found in file \"results/highCO2_sample1  ||\n|| /highCO2_sample1_genes_read_quantification.tsv.summary\"                    ||\n||                                                                            ||\n\\\\============================================================================//\n</code></pre></p> <p>(Optional) Exercise: If you have time, check Snakemake\u2019s log in <code>.snakemake/log/</code>. Is everything as you expected, especially wildcard values, input and output names\u2026?</p> Answer <p>You can check the logs with: <pre><code>cat .snakemake/log/&lt;latest_log&gt;\n</code></pre> This general log says exactly what was run by Snakemake, after placeholders, <code>wildcards</code>\u2026 were replaced by their actual values. It is identical to what appears on your screen when you run Snakemake.</p>"},{"location":"course_material/3_generalising_snakemake/#visualising-the-workflow-dag","title":"Visualising the workflow DAG","text":"<p>You have now implemented and run the main steps of the workflow. It is always a good idea to visualise the whole process to check for errors and inconsistencies. Snakemake has a built-in workflow visualisation feature to do this: the <code>--dag</code> parameter, which shows a dependency graph of all the jobs (rules appear once per wildcard value and wildcard value are displayed).</p> <p>Exercise: Visualise the entire workflow\u2019s Directed Acyclic Graph using <code>--dag</code>. Remember that Snakemake prints a DAG in text format, so you need to pipe its results into the <code>dot</code> command to transform it into a picture with <code>| dot -Tpng &gt; &lt;image_path&gt;.png</code>. Do you need to specify a target to the <code>snakemake</code> command?</p> Creating a DAG <ul> <li>Try to follow the official recommendations on workflow structure, which states that images are supposed to go in the <code>images/</code> subfolder<ul> <li><code>images/</code> is not automatically created by Snakemake because it isn\u2019t handled as part of an actual run, so you need to create it beforehand. You did this when you set up the workflow structure</li> </ul> </li> <li>If you already computed all outputs of the workflow, steps in the DAG will have dotted lines. To visualise the DAG before running the workflow, add <code>-F/--forceall</code> to the snakemake command to force the execution of all jobs<ul> <li>You can also use <code>-f &lt;target&gt;</code> to show fewer jobs</li> </ul> </li> </ul> The <code>dot</code> command <ul> <li><code>dot</code> is a part of the graphviz package and is used to draw hierarchical or layered drawings of directed graphs, i.e. graphs in which edges (arrows) have a direction</li> <li><code>-T</code>: choose image format. Available formats are listed here</li> </ul> Answer <p>To run the command without target, you can use: <pre><code>snakemake -c 1 --dag -F | dot -Tpng &gt; images/dag.png\n</code></pre> We added the <code>-F</code> parameter to force Snakemake to compute the entire DAG and ensure all jobs are shown. However, you will get a <code>WorkflowError: Target rules may not contain wildcards.</code> error. This makes sense, because if you don\u2019t give a target to Snakemake, it can\u2019t compute the wildcard values. To run the command using the same sample as before (<code>highCO2_sample1</code>), you can target one of the final outputs of the workflow: <pre><code>snakemake -c 1 --dag -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpng &gt; images/dag.png\n</code></pre></p> <p>You should get the following DAG:  Workflow rulegraph at the end of the session </p> <p>An important thing to remember: <code>--dag</code> implicitly activates <code>--dry-run/--dryrun/-n</code>, which means that no jobs are executed during DAG computation.</p> <p>Snakemake can also create two other graphs:</p> <ul> <li>A rulegraph, created with the <code>--rulegraph</code> parameter: dependency graph of all the rules (rules appear only once)     <pre><code>snakemake -c 1 --rulegraph -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpdf &gt; images/rulegraph.pdf\n</code></pre></li> <li>A filegraph, created with the <code>--filegraph</code> parameter: dependency graph of all the rules with inputs and outputs (rule appears once, <code>wildcards</code> are shown but not replaced)     <pre><code>snakemake -c 1 --filegraph -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tjpg &gt; images/filegraph.jpg\n</code></pre></li> </ul> <p>Here is a comparison of the different workflow graphs:</p> <p> </p> Workflow DAG, rulegraph and filegraph"},{"location":"course_material/4_optimising_snakemake/","title":"Decorating and optimising a Snakemake workflow","text":""},{"location":"course_material/4_optimising_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Use non-file parameters and config files in rules</li> <li>Modularise a workflow</li> <li>Make a workflow process list of inputs instead of one input at a time</li> <li>Aggregate outputs in a target rule</li> <li>(Optimise CPU usage)</li> </ul>"},{"location":"course_material/4_optimising_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/4_optimising_snakemake/#snakefile-from-previous-session","title":"Snakefile from previous session","text":"<p>If you didn\u2019t finish the previous part or didn\u2019t do the optional exercises, you can restart from a fully commented Snakefile, with log messages implemented in all rules. You can download it here or download it in your current directory with: <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/docs/solutions/session2/workflow/Snakefile\n</code></pre></p>"},{"location":"course_material/4_optimising_snakemake/#exercises","title":"Exercises","text":"<p>This series of exercises focuses on how to improve the workflow that you developed in the previous session. As a result, you will add only one rule to your workflow. But, fear not, it\u2019s a crucial one!</p> Development and back-up <p>During this session, you will modify your Snakefile quite heavily, so it may be a good idea to make back-ups from time to time (with <code>cp</code> or a simple copy/paste) or use a versioning system. As a general rule, if you have a doubt on the code you are developing, do not hesitate to make a back-up beforehand.</p>"},{"location":"course_material/4_optimising_snakemake/#using-non-file-parameters-and-config-files","title":"Using non-file parameters and config files","text":""},{"location":"course_material/4_optimising_snakemake/#non-file-parameters","title":"Non-file parameters","text":"<p>As you have seen, Snakemake execution revolves around input and output files. However, a lot of software also use non-file parameters to run. In the previous presentation and series of exercises, we advocated against using hard-coded file paths. Yet, if you look back at previous rules, you will find two occurrences of this behaviour in <code>shell</code> directives:</p> <ul> <li>In rule <code>read_mapping</code>, the index parameter: <pre><code>-x resources/genome_indices/Scerevisiae_index\n</code></pre></li> <li>In rule <code>reads_quantification_genes</code>, the annotation parameter: <pre><code>-a resources/Scerevisiae.gtf\n</code></pre></li> </ul> <p>This reduces readability and makes it very hard to change the values of these parameters, because this requires to change the <code>shell</code> directive code.</p> <p>The <code>params</code> directive was (partly) designed to solve this problem: it contains parameters and variables that can be accessed in the <code>shell</code> directive. It allows to specify additional non-file parameters instead of hard-coding them into shell commands or using them as inputs/outputs.</p> Main properties of parameters from the <code>params</code> directive <ul> <li>Their values can be of any type (integer, string, list\u2026)</li> <li>Their values can depend on wildcard values and use input functions (explained here). This means that parameters can be changed conditionally, for example depending on the value of a wildcard<ul> <li>In contrast to the <code>input</code> directive, the <code>params</code> directive can take more arguments than only <code>wildcards</code>, namely <code>input</code>, <code>output</code>, <code>threads</code>, and <code>resources</code></li> </ul> </li> <li>Similarly to <code>{input}</code> and <code>{output}</code> placeholders, they can be accessed from within the <code>shell</code> directive with the <code>{params}</code> placeholder</li> <li>Multiple parameters can be defined in a rule (do not forget the comma between each entry!) and they can also be named. While it isn\u2019t mandatory, un-named parameters are not explicit at all, so you should always name your parameters</li> </ul> <p>Here is an example of <code>params</code> utilisation: <pre><code>rule get_header:\n    input:\n        'data/example.txt'\n    output:\n        'results/header.txt'\n    params:\n        lines = 5\n    shell:\n        'head -n {params.lines} {input} &gt; {output}'\n</code></pre></p> <p>Exercise: Pick one of the two hard-coded paths mentioned earlier and replace it using <code>params</code>.</p> Answer <p>You need to add a <code>params</code> directive to the rule, name the parameter and replace the path by the placeholder in the <code>shell</code> directive. We did this for both rules so that you can check everything. Feel free to copy this in your Snakefile. For clarity, only lines that changed are shown below:</p> <ul> <li> <p><code>read_mapping</code>: <pre><code>params:\n    index = 'resources/genome_indices/Scerevisiae_index'\nshell:\n    'hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n    -x {params.index} \\  # Parameter was replaced here\n    -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}'\n</code></pre></p> </li> <li> <p><code>reads_quantification_genes</code>: <pre><code>params:\n    annotations = 'resources/Scerevisiae.gtf'\nshell:\n    'featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n    -B -C --largestOverlap --verbose -F GTF \\\n    -a {params.annotations} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}'  # Parameter was replaced here\n</code></pre></p> </li> </ul> <p>But doing this only shifted the problem: now, hard-coded paths are in <code>params</code> instead of <code>shell</code>. This is better, but not by much! Luckily, there is an even better way to handle parameters: instead of hard-coding parameter values in the Snakefile, Snakemake can use parameters (and values) defined in config files.</p>"},{"location":"course_material/4_optimising_snakemake/#config-files","title":"Config files","text":"<p>Config files are stored in the <code>config</code> subfolder and written in JSON or YAML format. You will use the latter for this course as it is more user-friendly. In .yaml files:</p> <ul> <li>Parameters are defined with the syntax <code>&lt;name&gt;: &lt;value&gt;</code></li> <li>Values can be strings, integers, booleans\u2026<ul> <li>For a complete overview of available value types, see this list</li> </ul> </li> <li>A parameter can have multiple values, each value being on an indented line starting with \u201c-\u201c<ul> <li>These values will be stored in a Python list when Snakemake parses the config file</li> </ul> </li> <li>Parameters can be named and nested to have a hierarchical structure, each sub-parameter and its value being on an indented lines<ul> <li>These parameters will be stored as a dictionary when Snakemake parses the config file</li> </ul> </li> </ul> <p>Config files will be parsed by Snakemake when executing the workflow, and parameters and their values will be stored in a Python dictionary named <code>config</code>. The config file path can be specified in the Snakefile with <code>configfile: &lt;path/to/file.yaml&gt;</code> at the top of the file, or at runtime with the execution parameter <code>--configfile &lt;path/to/file.yaml&gt;</code>.</p> <p>The example below shows a parameter with a single value (<code>lines_number</code>), a parameter with multiple values (<code>samples</code>), and nested parameters (<code>resources</code>): <pre><code>lines_number: 5  # Parameter with single value (string, int, float, bool ...)\nsamples:  # Parameter with multiple values\n    - sample1\n    - sample2\nresources:  # Nested parameters\n    threads: 4\n    memory: 4G\n</code></pre></p> <p>Then, each parameter can be accessed in Snakemake with: <pre><code>config['lines_number']  # --&gt; 5\nconfig['samples']  # --&gt; ['sample1', 'sample2']  # A list of parameters becomes a list\nconfig['resources']  # --&gt; {'threads': 4, 'memory': '4G'}  # A list of named parameters becomes a dictionary\nconfig['resources']['threads']  # --&gt; 4\n</code></pre></p> Accessing config values in <code>shell</code> <p>You cannot use values from the <code>config</code> dictionary directly in a <code>shell</code> directive. If you need to access parameter value in <code>shell</code>, first define it in <code>params</code> and assign its value from the dictionary, then use <code>params.&lt;name&gt;</code> in <code>shell</code>.</p> <p>Exercise: Create a config file in YAML format and fill it with variables and values to replace one of the two hard-coded parameters mentioned before. Then replace the hard-coded parameter values by variables from the config file. Finally, add the config file import on top of your Snakefile.</p> Answer <p>First, create an empty config file: <pre><code>touch config/config.yaml  # Create empty config file\n</code></pre></p> <p>Then, fill it with the desired values: <pre><code># Configuration options of RNAseq-analysis workflow\n# Location of genome indices\nindex: 'resources/genome_indices/Scerevisiae_index'\n# Location of annotation file\nannotations: 'resources/Scerevisiae.gtf'\n</code></pre></p> <p>Then, replace the <code>params</code> values in the Snakefile. We did this for both rules so that you can check everything. Feel free to copy this in your Snakefile. For simplicity, only lines that changed are shown below:</p> <ul> <li> <p><code>read_mapping</code>: <pre><code>params:\n    index = config['index']\n</code></pre></p> </li> <li> <p><code>reads_quantification_genes</code>: <pre><code>params:\n    annotations = config['annotations']\n</code></pre></p> </li> </ul> <p>Finally, add the file path on top of the Snakefile: <code>configfile: 'config/config.yaml'</code></p> <p>From now on, if you need to change these values, you can easily do it in the config file instead of modifying the code!</p>"},{"location":"course_material/4_optimising_snakemake/#modularising-a-workflow","title":"Modularising a workflow","text":"<p>If you develop a large workflow, you are bound to encounter some cluttering problems. Have a look at your current Snakefile: with only four rules, it is already almost 150 lines long. Imagine what happens when your workflow has dozens of rules? The Snakefile may (will?) become messy and harder to maintain and edit. This is why it quickly becomes crucial to modularise your workflow. This approach also makes it easier to re-use pieces of one workflow into another. Snakemake can be modularised on three different levels:</p> <ol> <li> <p>The most fine-grained level is wrappers</p> More information on wrappers <p>Wrappers allow to quickly use popular tools and libraries in Snakemake workflows, thanks to the <code>wrapper</code> directive. Wrappers are automatically downloaded and deploy a conda environment when running the workflow, which increases reproducibility. However their implementation can be \u2018rigid\u2019 and sometimes it may be better to write your own rule. See the official documentation for more explanations</p> </li> <li> <p>For larger parts belonging to the same workflow, it is recommended to split the main Snakefile into smaller snakefiles, each containing rules with a common topic. Smaller snakefiles are then integrated into the main Snakefile with the <code>include</code> statement. In this case, all rules share a common config file. See the official documentation for more explanations</p> Rules organisation <p>There is no official guideline on how to regroup rules, but a simple and logic approach is to create \u201cthematic\u201d snakefiles, i.e. place rules related to the same topic in the same file. Modularisation is a common practice in programming in general: it is often easier to group all variables, functions, classes\u2026 related to a common theme into a single script, package, software\u2026</p> </li> <li> <p>The final level of modularisation is modules</p> More on modules <p>It enables combination and re-use of rules in the same workflow and between workflows. This is done with the <code>module</code> statement, similarly to Python <code>import</code>. See the official documentation for more explanations</p> </li> </ol> <p>In this course, you will only use the second level of modularisation. Briefly, the idea is to write a main Snakefile in <code>workflow/Snakefile</code>, to place the other snakefile containing rules in the subfolder <code>workflow/rules</code> (these \u2018sub-Snakefile\u2019 should end with <code>.smk</code>, the recommended file extension of Snakemake) and to tell Snakemake to import the modular snakefile in the main Snakefile with the <code>include: &lt;path/to/snakefile.smk&gt;</code> syntax.</p> <p>Exercise: Move your current Snakefile into the subfolder <code>workflow/rules</code> and rename it to <code>read_mapping.smk</code>. Then create a new Snakefile in <code>workflow/</code> and import <code>read_mapping.smk</code> in it using the <code>include</code> syntax. You should also move the importation of the config file from the modular Snakefile to the main one.</p> Answer <p>First, move and rename the main Snakefile: <pre><code>mv workflow/Snakefile workflow/rules/read_mapping.smk  # Move and rename main Snakefile to modular snakefile\ntouch workflow/Snakefile  # Recreate main Snakefile\n</code></pre></p> <p>Then, add <code>include</code> and <code>configfile</code> statements to the new Snakefile. It should resemble this: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\n</code></pre></p> <p>Finally, remove the config file import (<code>configfile: 'config/config.yaml'</code>) from the modular snakefile (<code>workflow/rules/read_mapping.smk</code>).</p> Relative paths <ul> <li>Include statements are relative to the directory of the Snakefile in which they occur. For example, if the Snakefile is in <code>workflow</code>, then Snakemake will search for included snakefiles in <code>workflow/path/to/other/snakefile</code>, regardless of the working directory</li> <li>You can place snakefiles in a sub-directory without changing input and output paths, because these paths are relative to the working directory</li> <li>However, you will need to edit paths to external scripts and conda environments, because these paths are relative to the snakefile from which they are called (this will be discussed in the last series of exercises)</li> </ul> <p>If you have trouble visualising what an <code>include</code> statement does, you can imagine that the entire content of the included file gets copied into the Snakefile. As a consequence, syntaxes like <code>rules.&lt;rule_name&gt;.output.&lt;output_name&gt;</code> can still be used in modular snakefiles, even if the rule <code>&lt;rule_name&gt;</code> is defined in another snakefile. However, you have to make sure that the snakefile in which <code>&lt;rule_name&gt;</code> is defined is included before the snakefile that uses <code>rules.&lt;rule_name&gt;.output</code>. This is also true for input functions and checkpoints.</p>"},{"location":"course_material/4_optimising_snakemake/#using-a-target-rule-instead-of-a-target-file","title":"Using a target rule instead of a target file","text":"<p>Modularisation also offers a great opportunity to facilitate workflows execution. By default, if no target is given in the command line, Snakemake executes the first rule in the Snakefile. So far, you have always executed the workflow with a target file to avoid this behaviour. But we can actually use this property to make execution easier by writing a pseudo-rule (also called target-rule and usually named rule <code>all</code>) which contains all the desired outputs files as inputs in the Snakefile. This rule will look like this: <pre><code>rule all:\n    input:\n        'path/to/ouput1',\n        'path/to/ouput2',\n        '...'\n</code></pre></p> <p>Exercise: Implement a rule <code>all</code> in your Snakefile to generate the final outputs by default when running <code>snakemake</code> without specifying a target. Then, test your workflow with a dry-run and the <code>-F</code> parameter. How many rules does Snakemake run?</p> Content of a rule <code>all</code> <ul> <li>A rule is not required to have an output nor a <code>shell</code> directive</li> <li>Inputs of rule <code>all</code> should be the final outputs that you want to generate, here those of rule <code>reads_quantification_genes</code></li> </ul> Answer <p><code>reads_quantification_genes</code> is currently creating the last workflow outputs (with the same example as before, <code>results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv</code>). We need to use these files as inputs of rule <code>all</code>: <pre><code># Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        'results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv'\n</code></pre></p> <p>You can launch a dry-run with: <pre><code>snakemake -c 4 -F -p -n\n</code></pre></p> <p>You should see all the rules appearing, including rule <code>all</code>, and the job stats: <pre><code>rule all:\n    input: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n    jobid: 0\n    reason: Forced execution\n    resources: tmpdir=/tmp\n\nJob stats:\njob                           count\n--------------------------  -------\nall                               1\nfastq_trim                        1\nread_mapping                      1\nreads_quantification_genes        1\nsam_to_bam                        1\ntotal                             5\n</code></pre> Snakemake runs 5 rules in total: the 4 of the previous session and the rule <code>all</code>.</p> <p>After several (dry-)runs, you may have noticed that the rule order is not always the same: apart from Snakemake considering the first rule of the workflow as a default target, the order of rules in Snakefile/snakefiles is arbitrary and does not influence the DAG of jobs.</p>"},{"location":"course_material/4_optimising_snakemake/#aggregating-outputs-to-process-lists-of-files","title":"Aggregating outputs to process lists of files","text":"<p>Using a target rule like the one presented in the previous paragraph gives another opportunity to make things easier. In the previous rule <code>all</code>, inputs are still hard-coded\u2026 and you know that this is not an optimal solution, especially if there are many samples to process. The <code>expand()</code> function will solve both problems.</p> <p><code>expand()</code> is used to generate a list of output files by automatically expanding a wildcard expression to several values. In other words, it will replace a wildcard in an expression by all the values of a list, successively. For instance, <code>expand('{sample}.tsv', sample=['A', 'B', 'C'])</code> will create the list of files <code>['A.tsv', 'B.tsv', 'C.tsv']</code>.</p> <p>Exercise: Use an expand syntax to transform rule <code>all</code> to generate a list of final outputs with all the samples. Then, test your workflow with a dry-run and the <code>-F</code> parameter.</p> Two things are required for an expand syntax <ol> <li>A Python list of values that will replace a wildcard; here, a sample list</li> <li>An output path with a wildcard that can be turned into an <code>expand()</code> function to create all the required outputs</li> </ol> Answer <p>First, we need to add a sample list in the Snakefile, before rule <code>all</code>. This list contains all the values that the wildcard will be replaced with: <pre><code>SAMPLES = ['highCO2_sample1', 'highCO2_sample2', 'highCO2_sample3', 'lowCO2_sample1', 'lowCO2_sample2', 'lowCO2_sample3']\n</code></pre></p> <p>Then, we need to transform the rule <code>all</code> inputs to use the <code>expand</code> function: <pre><code>expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=SAMPLES)\n</code></pre></p> <p>The Snakefile should like this: <pre><code># Sample list\nSAMPLES = ['highCO2_sample1', 'highCO2_sample2', 'highCO2_sample3', 'lowCO2_sample1', 'lowCO2_sample2', 'lowCO2_sample3']\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=SAMPLES)\n</code></pre></p> <p>You can launch a dry-run with the same command as before: <pre><code>snakemake -c 4 -F -p -n\n</code></pre></p> <p>You should see rule <code>all</code> requiring 6 inputs and all the other rules appearing 6 times (1 for each sample): <pre><code>rule all:\n    input: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv, results/highCO2_sample2/highCO2_sample2_genes_read_quantification.tsv, results/highCO2_sample3/highCO2_sample3_genes_read_quantification.tsv, results/lowCO2_sample1/lowCO2_sample1_genes_read_quantification.tsv, results/lowCO2_sample2/lowCO2_sample2_genes_read_quantification.tsv, results/lowCO2_sample3/lowCO2_sample3_genes_read_quantification.tsv\n    jobid: 0\n    reason: Forced execution\n    resources: tmpdir=/tmp\n\nJob stats:\njob                           count\n--------------------------  -------\nall                               1\nfastq_trim                        6\nread_mapping                      6\nreads_quantification_genes        6\nsam_to_bam                        6\ntotal                            25\n</code></pre></p> <p>But there is an even better solution! At the moment, samples are defined as a list in the Snakefile. Processing other samples still means having to locate and change some chunks of code. To further improve workflow usability, samples can instead be defined in config files, so they can easily be added, removed, or modified by users without actually modifying code.</p> <p>Exercise: Implement a parameter in the config file to specify sample names and modify the rule <code>all</code> to use this parameter instead of the <code>SAMPLES</code> variable in the <code>expand()</code> syntax.</p> Answer <p>First, we need to add the sample names to the config file: <pre><code># Configuration options of RNAseq-analysis workflow\n\n# Location of genome indices\nindex: 'resources/genome_indices/Scerevisiae_index'\n\n# Location of annotation file\nannotations: 'resources/Scerevisiae.gtf'\n\n# Sample names\nsamples:\n  - highCO2_sample1\n  - highCO2_sample2\n  - highCO2_sample3\n  - lowCO2_sample1\n  - lowCO2_sample2\n  - lowCO2_sample3\n</code></pre></p> <p>Then, we need to remove <code>SAMPLES</code> from the Snakefile and use the config file in <code>expand()</code> instead: <pre><code># Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=config['samples'])\n</code></pre> Here, <code>config['samples']</code> is a Python list containing strings, each string being a sample name. This is because a list of parameters become a list when the config file is parsed. You can launch a dry-run with the same command as before (<code>snakemake -c 4 -F -p -n</code>) and you should see the same jobs.</p> An even more Snakemake-idiomatic solution <p>There is an even better and more Snakemake-idiomatic version of the <code>expand()</code> syntax in rule <code>all</code>: <pre><code>expand(rules.reads_quantification_genes.output.gene_level, sample=config['samples'])\n</code></pre> This entirely removes the need to write output paths, even though it might be less easy to understand at first sight.</p> <p>Exercise: Run the workflow on the other samples and generate the workflow DAG and filegraph. If you implemented parallelisation and multithreading in all the rules, the execution should take less than 10 min in total to process all the samples, otherwise it will be a few minutes longer.</p> Answer <p>You can run the workflow by removing <code>-F</code> and <code>-n</code> from the dry-run command, which makes a very simple command: <pre><code>snakemake -c 4 -p --sdm apptainer\n</code></pre></p> <p>To generate the DAG, you can use: <pre><code>snakemake -c 1 -F -p --dag | dot -Tpng &gt; images/all_samples_dag.png\n</code></pre> <p> </p></p> <p>If needed, open the picture in a new tab to zoom in. Then, you can generate the filegraph with: <pre><code>snakemake -c 1 -F -p --filegraph | dot -Tpng &gt; images/all_samples_filegraph.png\n</code></pre> <p> </p> You probably noticed that these two figures have an extra rule, <code>fastq_qc_sol4</code>. It is the rule implemented in the supplementary exercise below.</p>"},{"location":"course_material/4_optimising_snakemake/#optimising-resource-usage-in-a-workflow-by-multithreading","title":"Optimising resource usage in a workflow by multithreading","text":"<p>This part is about CPU usage in Snakemake. It is quite long, so do it only if you finished all the other exercises.</p> <p>When working with real, larger, datasets, some processes can take a long time to run. Fortunately, computation time can be decreased by running jobs in parallel and using several threads or cores for a single job.</p> <p>Exercise: What are the two things you need to add to a rule to enable multithreading?</p> Answer <p>You need to add:</p> <ol> <li>The <code>threads</code> directive to tell Snakemake that it needs to allocate several threads to this rule</li> <li>The software-specific parameters in the <code>shell</code> directive to tell a software that it can use several threads</li> </ol> <p>If you add only the first element, the software will not be aware of the number of threads allocated to it and will use its default number of threads (usually one). If you add only the second element, Snakemake will allocate only one thread to the software, which means that it will run slowly or crash (the software expects multiple threads but gets one).</p> <p>Usually, you need to read documentation to identify which software can make use of multithreading and which parameters control multithreading. We did it for you to save some time:</p> <ul> <li><code>atropos trim</code>, <code>hisat2</code>, <code>samtools view</code>, and <code>samtools sort</code> can parallelise with the <code>--threads &lt;nb_thread&gt;</code> parameter</li> <li><code>featureCounts</code> can parallelise with the <code>-T &lt;nb_thread&gt;</code> parameter</li> <li><code>samtools index</code> can\u2019t parallelise. Remember that multithreading only applies to software that were developed to this end, Snakemake itself cannot parallelise a software!</li> </ul> <p>Unfortunately, there is no easy way to find the optimal number of threads for a job. It depends on tasks, datasets, software, resources you have at your disposal\u2026 It often takes of few rounds of trial and error to see what works best. We already decided the number of threads to use for each software:</p> <ul> <li>4 threads for <code>hisat2</code></li> <li>2 for all the other software</li> </ul> <p>Exercise: Implement multithreading in a rule of your choice (it\u2019s usually best to start by multithreading the longest job, here <code>read_mapping</code>, but the example dataset is small, so it doesn\u2019t really matter).</p> <code>threads</code> placeholder <p><code>threads</code> can also be replaced by a <code>{threads}</code> placeholder in the <code>shell</code> directive.</p> Answer <p>We implemented multithreading in all the rules so that you can check everything. Feel free to copy this in your Snakefile: <pre><code>rule fastq_trim:\n    '''\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    '''\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    log:\n        'logs/{sample}/{sample}_atropos_trimming.log'\n    threads: 2  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2'\n    shell:\n        '''\n        echo \"Trimming reads in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt; {log}\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" --threads {threads} \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2} &amp;&gt;&gt; {log}\n        echo \"Trimmed files saved in &lt;{output.trim1}&gt; and &lt;{output.trim2}&gt; respectively\" &gt;&gt; {log}\n        echo \"Trimming report saved in &lt;{log}&gt;\" &gt;&gt; {log}\n        '''\n\nrule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    params:\n        index = config['index']\n    threads: 4  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x {params.index} --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n\nrule sam_to_bam:\n    '''\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    '''\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    log:\n        'logs/{sample}/{sample}_mapping_sam_to_bam.log'\n    threads: 2  # Add directive\n    container:\n        'https://depot.galaxyproject.org/singularity/samtools%3A1.21--h50ea8bc_0'\n    shell:\n        '''\n        echo \"Converting &lt;{input.sam}&gt; to BAM format\" &gt; {log}\n        samtools view {input.sam} --threads {threads} -b -o {output.bam} 2&gt;&gt; {log}  \n        echo \"Sorting .bam file\" &gt;&gt; {log}\n        samtools sort {output.bam} --threads {threads} -O bam -o {output.bam_sorted} 2&gt;&gt; {log}  \n        echo \"Indexing sorted .bam file\" &gt;&gt; {log}\n        samtools index -b {output.bam_sorted} -o {output.index} 2&gt;&gt; {log}\n        echo \"Sorted file saved in &lt;{output.bam_sorted}&gt;\" &gt;&gt; {log}\n        '''\n\nrule reads_quantification_genes:\n    '''\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly. The strandedness parameter\n    is determined by get_strandedness().\n    '''\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    threads: 2  # Add directive\n    params:\n        annotations = config['annotations']\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a {params.annotations} -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log} \n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>Exercise: Do you need to change anything in the <code>snakemake</code> command to run the workflow with 4 cores?</p> Answer <p>You need to provide additional cores to Snakemake with the parameter <code>-c 4</code>. Using the same sample as before (<code>highCO2_sample1</code>), the workflow can be run with: <pre><code>snakemake -c 4 -F -p results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv --sdm apptainer\n</code></pre></p> <p>The number of threads allocated to all jobs running at a given time cannot exceed the value specified with <code>--cores</code>, so if you use <code>-c 1</code>, Snakemake will not be able to use multiple threads. Conversely, if you ask for more threads in a rule than what was provided with <code>--cores</code>, Snakemake will cap rule threads at <code>--cores</code> to avoid requesting too many. Another benefit of increasing <code>--cores</code> is to allow Snakemake to run multiple jobs in parallel (for example, here, running two jobs using two threads each).</p> <p>If you run the workflow from scratch with multithreading in all rules, it should take ~6 min, compared to ~10 min before (i.e. a 40% decrease!). This gives you an idea of how powerful multithreading is when datasets and computing power get bigger!</p> Things to keep in mind when using parallel execution <ul> <li>On-screen output from parallel jobs will be mixed, so save any output to log files instead</li> <li>Parallel jobs use more RAM. If you run out then either your OS will swap data to disk (which slows data access), or a process will die (which can crash Snakemake)</li> <li>Parallelising is not without consequences and has a cost. This is a topic too wide for this course, but just know that using too many cores on a dataset that is too small can slow down computation, as explained here</li> </ul>"},{"location":"course_material/5_reproducibility_snakemake/","title":"Being reproducible with Snakemake","text":""},{"location":"course_material/5_reproducibility_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Use an input function to work with an unknown number of inputs</li> <li>Run scripts from other languages (Python and R)</li> <li>Deploy a rule-specific conda environment</li> <li>Deploy a rule-specific Docker/Apptainer container</li> </ul>"},{"location":"course_material/5_reproducibility_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/5_reproducibility_snakemake/#workflow-from-previous-session","title":"Workflow from previous session","text":"<p>If you didn\u2019t finish the previous part or didn\u2019t do the optional exercises, you can restart from fully commented snakefiles, with a supplementary .fastq files quality check rule and multithreading implemented in all rules. You can download all the files (workflow and config) here or copy them after cloning the course repository locally: <pre><code>git clone https://github.com/sib-swiss/containers-snakemake-training.git  # Clone repository\ncd containers-snakemake-training/  # Go in repository\ncp -r docs/solutions/session3 destination_path  # Copy folder where needed; adapt destination_path to required path\n</code></pre></p>"},{"location":"course_material/5_reproducibility_snakemake/#exercises","title":"Exercises","text":"<p>In this series of exercises, you will create the last two rules of the workflow. Each rule will execute a script (one in Python and one in R; don\u2019t worry, this is not a programming course, so we wrote the scripts for you!), and both rules will have dedicated environments that you will need to take into account in the snakefiles.</p> Development and back-up <p>During this session as well, you will modify your Snakefile quite heavily, so it may be a good idea to make back-ups from time to time (with <code>cp</code> or a simple copy/paste) or use a versioning system. As a general rule, if you have a doubt on the code you are developing, do not hesitate to make a back-up beforehand.</p>"},{"location":"course_material/5_reproducibility_snakemake/#creating-a-rule-to-gather-read-counts","title":"Creating a rule to gather read counts","text":"<p>To perform a Differential Expression Analysis (DEA), it is easier to have a single file gathering all the read counts from the different samples. The next rule that you will create will both find the required files and merge them, thanks to an input function and a Python script.</p>"},{"location":"course_material/5_reproducibility_snakemake/#building-the-general-rule-structure","title":"Building the general rule structure","text":"<p>We already wrote the common elements of the rule so that you can focus on the most interesting parts (the missing <code>input</code> and the missing elements at the end): <pre><code>rule count_table:\n    '''\n    This rule merges gene count tables of an assembly into one table.\n    '''\n    input:\n        ?\n    output:\n        table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    threads: 1\n    ?:\n        ?\n    ?:\n        ?\n</code></pre></p> <p>Here, there is no <code>wildcards</code> in the rule: only one file will be created, and its name will not change depending on sample names because we use all the samples to create it.</p> Explicit is better than implicit <p>Even if a software cannot multithread, it is useful to add <code>threads: 1</code> to keep the syntax consistent between rules and clearly state that the software works with a single thread.</p> <p>Exercise: Given that this rule and the next one will be quite different from the previous ones, it is a good idea to implement them in a new snakefile. Create a new file <code>workflow/rules/analyses.smk</code> and copy the previous rule structure in it. Do you need to change anything else to use this rule in your workflow?</p> Answer <p>To actually use this rule, Snakemake needs to be aware that it exists: this is done with the <code>include</code> statement. We need to add the following lines to the main Snakefile (<code>workflow/Snakefile</code>): <pre><code>include: 'rules/analyses.smk'\n</code></pre></p> <p>The Snakefile will look like this: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        expand(rules.reads_quantification_genes.output.gene_level, sample=config['samples'])\n</code></pre></p> <p>Let\u2019s start filling those missing elements!</p>"},{"location":"course_material/5_reproducibility_snakemake/#gathering-the-input-files","title":"Gathering the input files","text":"<p>This task is quite complex: we need a way to identify all the rule inputs and gather them in a Python list. Here, there are only six samples, so in theory, you could list them directly\u2026 However, it isn\u2019t good practice and it quickly becomes un-manageable when the number of sample increases. Fortunately, there is a much more elegant and convenient solution: an input function, which provides the added benefit of scaling up very well.</p> <p>We wrote one for you: <pre><code># Input function used in rule count_table\ndef get_gene_counts(wildcards):\n    '''\n    This function lists count tables from every sample in the config file\n    '''\n    return [f\"results/{sample}/{sample}_genes_read_quantification.tsv\"\n            for sample in config['samples']]\n</code></pre></p> Snakemake wildcards vs Python f-strings <p>This input function is pure Python code: in the return statement, <code>{sample}</code> isn\u2019t a wildcard, it is an f-string variable! This shows that you can natively use basic Python elements in a workflow: Snakemake will still be able understand them. This is because Snakemake was built on top of Python.</p> <p>This function will loop over the list of samples in the config file and replace <code>{sample}</code> with the current sample name of the iteration to create a string which is the output path from the rule <code>reads_quantification_genes</code> of said sample. Then, it will aggregate all the paths in a list and return this list.</p> More on input functions <ul> <li>Input functions take the <code>wildcards</code> global object as single argument</li> <li>You can access wildcard values inside an input function with the syntax <code>{wildcards.wildcards_name}</code></li> <li>Input functions can return lists of files, which will be automatically handled like multiple inputs by Snakemake<ul> <li>Input functions can also return a dictionary; in this case, the function should be called with the <code>unpack()</code> function: <pre><code>input: unpack(&lt;function_name&gt;)\n</code></pre> The dictionary keys will be used as input names and the dictionary values will be used as input values, providing a list of named inputs</li> </ul> </li> </ul> Input functions and output directory <p>Input functions are evaluated before the workflow is executed, so they cannot be used to list the content of an output directory, since it does not exist before the workflow is executed. Instead, you can use a checkpoint to trigger a re-evaluation of the DAG.</p> <p>Exercise: Insert the function <code>get_gene_counts()</code> in <code>workflow/rules/analysis.smk</code> and adapt the input value of <code>count_table</code> accordingly. Do you need to insert the function in a specific location?</p> Answer <p>The first step is to add the input function to the file. However, it needs to appear before the rule <code>count_table</code>, otherwise we will see the error <code>name 'get_gene_counts' is not defined</code>. In other words, the function needs to be defined before Snakemake looks for it when it parses the input. Then, we need to set the function name as the rule input value.</p> <p>The modular snakefile <code>workflow/rules/analysis.smk</code> will resemble this: <pre><code># Input function used in rule count_table\ndef get_gene_counts(wildcards):\n    '''\n    This function lists count tables from every sample in the config file\n    '''\n    return [f\"results/{sample}/{sample}_genes_read_quantification.tsv\"\n            for sample in config['samples']]\n\nrule count_table:\n    '''\n    This rule merges gene count tables of an assembly into one table.\n    '''\n    input:\n        get_gene_counts  # Add input function to rule\n    output:\n        table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    threads: 1\n    ?:\n        ?\n    ?:\n        ?\n</code></pre> You don\u2019t need to use parentheses or specify any argument when you call an input function in the <code>input</code> directive. Doing so would actually change Snakemake behaviour!</p> <p>Now that the rule inputs are defined, we need to set-up the script to process them.</p>"},{"location":"course_material/5_reproducibility_snakemake/#using-a-python-script-in-snakemake","title":"Using a Python script in Snakemake","text":""},{"location":"course_material/5_reproducibility_snakemake/#getting-the-script","title":"Getting the script","text":"<p>The counts will be concatenated thanks to a script called <code>count_table.py</code>. It was written in Python, takes a list of files as input, and produces one output, a tab-separated table containing read counts of the different samples for each gene. You can download it with: <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/docs/solutions/session4/workflow/scripts/count_table.py\n</code></pre></p> <p>Or you can copy it from here:</p> Click here to see a nice Python script! count_table.py<pre><code>'''\nMerge gene counts from all samples of an assembly into a single table.\n'''\n\n\nimport os\nimport pandas as pd  # Non built-in package\nimport sys\n\n\n# Constants\nFIELDS = ['Geneid', 'Reads_quant']\nSTR_TO_REMOVE = '_genes_read_quantification.tsv'\n\n\n# Functions\ndef import_clean(table):\n    print(f'Importing and cleaning quantification data from &lt;{table}&gt;')\n    reads = pd.read_csv(table, sep='\\t', comment='#')\n    reads.rename(columns={reads.columns[-1]: 'Reads_quant'}, inplace=True)\n    print('Sorting &lt;gene&gt; table by Chromosome then Start position')\n    # New columns are simpler and will be used to properly reorder the table\n    print('\\tCreating temporary columns')\n    # Get unique Chr ID using a set\n    reads['Chr_new'] = reads['Chr'].apply(lambda x: ''.join(set(x.split(';'))))\n    # Select start of the first exon\n    reads['Start_new'] = reads['Start'].apply(lambda x: int(x.split(';')[0]))\n    print('\\tSorting table')\n    reads.sort_values(['Chr_new', 'Start_new'], ascending=[True, True],\n                      inplace=True)\n    print('\\tRemoving temporary columns')\n    reads.drop(['Chr_new', 'Start_new'], axis='columns', inplace=True)\n    final_table = reads[FIELDS].set_index('Geneid', drop=True)\n    return final_table\n\n\n# Main code execution\nif __name__ == '__main__':\n\n    with open(snakemake.log[0], 'w') as logfile:\n\n        # Redirect everything from the script to Snakemake log\n        sys.stderr = sys.stdout = logfile\n\n        print('Getting data from snakemake')\n        list_of_files = snakemake.input\n        count_table = snakemake.output.table\n\n        output_dir = os.path.dirname(count_table)\n        os.makedirs(output_dir, exist_ok=True)\n\n        print(f'Initialising global table with &lt;{list_of_files[0]}&gt;')\n        total_table = import_clean(list_of_files[0])\n\n        for file in list_of_files[1:]:\n            print(f'\\tAdding data from &lt;{file}&gt;')\n            tmp_table = import_clean(file)\n            total_table = pd.concat([total_table, tmp_table], axis=1)\n\n        print('Renaming columns')\n        column_titles = [os.path.basename(x).replace(STR_TO_REMOVE, '')\n                         for x in list_of_files]\n        total_table.columns = column_titles\n\n        print(f'Saving final table in &lt;{count_table}&gt;')\n        total_table.to_csv(count_table, sep='\\t', header=True, index=True)\n        print('Done')\n</code></pre> <p>Exercise: Get the script with your favourite method and place it the proper folder according to the official documentation. Where should you store it?</p> Answer <p>Scripts should be gathered in their own dedicated folder: <code>workflow/scripts</code>.</p>"},{"location":"course_material/5_reproducibility_snakemake/#deciding-how-to-run-the-script","title":"Deciding how to run the script","text":"<p>If you remember the presentation, there are two directives that you can use to run external scripts in Snakemake: <code>run</code> and <code>script</code>. While both allow to run Python code, they are not equivalent, so there is a choice to make!</p> <p>Exercise: Check out the script content. Depending on what you find, choose a directive and implement it in place of the last two missing elements (<code>?</code>) of rule <code>count_table</code>.</p> Script path is relative\u2026 <p>\u2026 to the Snakefile calling it. If you followed the recommended workflow structure, modular snakefiles are placed in a <code>rules</code> subfolder (like <code>rules/analysis.smk</code>) and scripts are placed in a <code>scripts</code> subfolder (like <code>scripts/count_table.py</code>). You need to find a path between those two subfolders.</p> Answer <p>There are two things to check before deciding which directive to use:</p> <ol> <li> <p>The script length:</p> <p>If we open the script in a text editor or run <code>wc -l workflow/scripts/count_table.py</code>, we see that it is 67 lines long. It is also quite complex, with function definitions, loops\u2026 This favours the <code>script</code> directive, as it\u2019s better to use <code>run</code> with short and simple code.</p> </li> <li> <p>The use of external packages (packages that are not included in a default Python installation):</p> <p>Another way to put this is: does the script need a special environment to work? If so, then we have to use the <code>script</code> directive, as it is the only one to accommodate for conda environments or containers. This means that this criteria takes precedence over the previous one: if we need to run a short script within a dedicated environment, <code>script</code> is the only way to do it.</p> <p>Here, there are several <code>import</code> statements at the top of the script, including <code>import pandas as pd  # Non built-in package</code>. <code>pandas</code> is a great package, but it is not part of the built-in packages shipped with Python. This means that the script needs a dedicated environment to run and confirm that we need the <code>script</code> directive.</p> </li> </ol> <p>With this in mind, the rule will be: <pre><code>rule count_table:\n    '''\n    This rule merges gene count tables of an assembly into one table.\n    '''\n    input:\n        get_gene_counts\n    output:\n        table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    threads: 1\n    ?:\n        ?\n    script:  # Add script directive\n        '../scripts/count_table.py'  # Add script location relative to rule file\n</code></pre></p> Using the same Python script in and out of Snakemake <p>To avoid code redundancy, it would be ideal to have a script that can be called by Snakemake but also work with standard Python (and be used outside Snakemake). The two main ways to do this are:</p> <ul> <li>Implement the script as a module/package and use this module in Snakemake, for example with a command line interface in <code>shell</code></li> <li>Test whether the <code>snakemake</code> object exists in the script:<ul> <li>If so, the script can process the Snakemake values<ul> <li>When the script is launched by Snakemake, there is an object called <code>snakemake</code> that provides access to the same objects that are available in the <code>run</code> and <code>shell</code> directives (<code>input</code>, <code>output</code>, <code>params</code>, <code>wildcards</code>, <code>log</code>, <code>threads</code>, <code>resources</code>, config). For instance, you can use <code>snakemake.input[0]</code> to access the first input file of a rule, or <code>snakemake.input.input_name</code> to access a named input</li> </ul> </li> <li>If not, the script can use other parameters, for example those coming from command line parsing</li> </ul> </li> </ul>"},{"location":"course_material/5_reproducibility_snakemake/#providing-a-rule-specific-conda-environment","title":"Providing a rule-specific conda environment","text":"<p>Given the presence of a non-default package in the script, we need to find a solution to make it accessible inside the rule. The easiest way to do that is to create a rule-specific conda environment. In Snakemake, you can do this by providing an environment config file (in YAML format) to the rule with the <code>conda</code> directive.</p> <p>(Optional) Exercise: If you have time, you can create your own config file for the environment using the tip on \u2018Environment features\u2019 below. If you need a reminder on how environment files look and work, you can check out slides 4-10 of the presentation (available here). Otherwise, you can directly skip to the answer.</p> Environment features <ul> <li>Environment <code>name</code> is <code>py3.12</code></li> <li>It uses two <code>channels</code>: <code>conda-forge</code> and <code>bioconda</code> (in that order)</li> <li>It requires <code>python</code> with at least version <code>3.12</code></li> <li>It requires <code>pandas</code> with version <code>2.2.3</code> exactly</li> <li>Like with scripts, config files should be stored in their own dedicated folder: <code>workflow/envs</code></li> </ul> Answer <p>The config file, created in <code>workflow/envs/py.yaml</code> should look like this: <pre><code># Environment file to perform data processing with Python\nname: py3.12\nchannels:\n    - conda-forge\n    - bioconda\ndependencies:\n    - python &gt;= 3.12\n    - pandas == 2.2.3\n</code></pre></p> <p>Exercise: Add the conda environment to the rule.</p> Environment file path is relative\u2026 <p>\u2026 to the Snakefile calling it. If you followed the recommended workflow structure, modular snakefiles are placed in a <code>rules</code> subfolder (like <code>rules/analysis.smk</code>) and environment files are placed in a <code>envs</code> subfolder (like <code>envs/py.yaml</code>). You need to find a path between those two subfolders.</p> Answer <p>We need to fill the last two missing elements with the directive name, <code>conda</code>, and its value, the script location: <pre><code>    rule count_table:\n        '''\n        This rule merges gene count tables of an assembly into one table.\n        '''\n        input:\n            get_gene_counts\n        output:\n            table = 'results/total_count_table.tsv'\n        log:\n            'logs/total_count_table.log'\n        threads: 1\n        conda:  # Add conda directive\n            '../envs/py.yaml'  # Add config file location relative to rule file\n        script:\n            '../scripts/count_table.py'\n</code></pre></p> <p>Using conda environments improves reproducibility for many reasons, including version control and the fact that users do not need to manually manage software dependencies. The first workflow execution after adding Conda environments will take more time than usual because <code>snakemake</code> (through <code>conda</code>) has to download and install all the software in the working directory.</p>"},{"location":"course_material/5_reproducibility_snakemake/#adapting-the-snakefile-and-running-the-rule","title":"Adapting the Snakefile and running the rule","text":"<p>All that is left is running the rule to create the table.</p> <p>Exercise: Find the <code>snakemake</code> command you should run to create the desired output (which one is it?) and execute the workflow. Is there anything else to do beforehand?</p> Answer <p>We cannot launch the workflow directly: first, we need to update rule <code>all</code> input to use the output of rule <code>count_table</code>. After this, your Snakefile should be: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        rules.count_table.output.table  # New input matching output of rule `count_table`\n</code></pre></p> <p>Finally, run the workflow with: <pre><code>snakemake -c 4 -p --sdm conda\n</code></pre> Do not forget to add <code>--sdm conda</code>, otherwise Snakemake will not use the environment you provided.</p> <p>You should see the rule <code>count_table</code> executed with 6 files as input. You can also check the log of rule <code>count_table</code> to see if the script worked as intended.</p>"},{"location":"course_material/5_reproducibility_snakemake/#creating-a-rule-to-detect-differentially-expressed-genes-deg","title":"Creating a rule to detect Differentially Expressed Genes (DEG)","text":"<p>The final rule that you will create in this course will use an R script to process the global read count table previously created and detect differentially expressed genes. As such, you will see several common elements between this rule and rule <code>count_table</code> (external scripts, dedicated environments, rule structure\u2026) and the process to implement this rule will also be very similar. However, it will be easier as you won\u2019t need to use an input function.</p>"},{"location":"course_material/5_reproducibility_snakemake/#building-the-general-rule-structure-again","title":"Building the general rule structure (again)","text":"<p>We also wrote the common elements of the rule so that you can focus on the most interesting parts (the missing elements at the end): <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots control graphs (PCA, heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    threads: 2\n    ?:\n        ?\n    ?:\n        ?\n</code></pre></p> <p>Once again, we do not need to use <code>wildcards</code> in this rule, because all the files are precisely defined.</p> <p>Exercise: Given the rule structure above, update the <code>Snakefile</code> so that it creates the final output of the workflow.</p> Answer <p>There is only one thing to update in the target rule input: <pre><code>'''\nMain Snakefile of RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Config file path\nconfigfile: 'config/config.yaml'\n\n# Rules to execute workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule used to launch workflow\nrule all:\n    '''\n    Dummy rule to automatically generate required outputs.\n    '''\n    input:\n        rules.differential_expression.output.deg  # New input matching output of rule `differential_expression`\n</code></pre> Remember that we don\u2019t need to add both outputs of rule <code>differential_expression</code> as inputs of rule <code>all</code>, only one suffices.</p> <p>As mentioned above, we don\u2019t need an input function because the input of rule <code>differential_expression</code> is easy to identify, so we\u2019ll directly focus on a way to run the R script.</p>"},{"location":"course_material/5_reproducibility_snakemake/#using-an-r-script-in-snakemake","title":"Using an R script in Snakemake","text":""},{"location":"course_material/5_reproducibility_snakemake/#downloading-the-script","title":"Downloading the script","text":"<p>The DE analyses will be performed thanks to a script called <code>DESeq2.R</code>. It was written in R, takes a read count table as input, and produces two outputs, a tab-separated table containing DEG (and statistical results) and a .pdf file containing control plots of the analysis. You can download it here or with the command: <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/docs/solutions/session4/workflow/scripts/DESeq2.R\n</code></pre></p> <p>Exercise: Download the script and place it the right folder.</p> Answer <p>You can place this script in the same folder than the Python script, <code>workflow/scripts</code>. There is nothing in the official documentation about placing scripts from different languages in separate folders, but if you use a large number of scripts, it might be worth considering. You could also gather scripts by topic, similarly to <code>.smk</code> files.</p>"},{"location":"course_material/5_reproducibility_snakemake/#running-the-script","title":"Running the script","text":"<p>The next exercise won\u2019t be as guided as the other ones. This is done on purpose as you have seen everything you need to solve it!</p> <p>Exercise: Find a way to run the R script and fill the missing elements in the rule.</p> What do you need to take into account? <ul> <li>The directive you need to run the script</li> <li>The location/path of the script</li> <li>Check whether the script need a special environment<ul> <li>If so, and if you attended the SIB course on Containers, remember a certain Docker image you created during the course</li> <li>If not, use the following container: <code>docker://athiebaut/deseq2:v3</code></li> </ul> </li> </ul> Answer <p>Like with the Python script, there are two problems to solve to run the R script:</p> <ol> <li> <p>Which directive to use?</p> <p>There isn\u2019t much of a choice here\u2026 If you remember the presentation, there is only one way to run non-Python code in Snakemake: the <code>script</code> directive: <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots control graphs (PCA, heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    threads: 2\n    ?:\n        ?\n    script:  # Add script directive\n        '../scripts/DESeq2.R'  # Add script location relative to rule file\n</code></pre></p> </li> <li> <p>Does it use external packages and need a specific environment?</p> <p>If you look at the top of the script, you will see several (11 to be exact!) <code>library()</code> calls. Each of them imports an external package. All of these could be gathered in a conda environment, however when numerous libraries are involved, it is sometimes easier to use a container. During Session 3 of the Containers course (Working with Dockerfiles), you built your own Docker image, called <code>deseq2</code>. This image actually contains everything required by the script:</p> <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots control graphs (PCA, heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    threads: 2\n    container:  # Add container directive\n        'docker://athiebaut/deseq2:v3'  # Try with your own image; if it doesn't work, use this one\n    script:\n        '../scripts/DESeq2.R'\n</code></pre> </li> </ol> Using the same R script in and out of Snakemake <p>Inside the script, an S4 object named <code>snakemake</code>, analogous to the Python one, is available and allows access to Snakemake objects: <code>input</code>, <code>output</code>, <code>params</code>, <code>wildcards</code>, <code>log</code>, <code>threads</code>, <code>resources</code>, and <code>config</code>. Here, the syntax follows that of S4 classes with attributes that are R lists. For example, you can access the first input file with <code>snakemake@input[[1]]</code> (remember that in R, indexing starts at 1). Named objects can be accessed the same way, by providing the name instead of an index: <code>snakemake@input[[\"myfile\"]]</code> to access the input called <code>myfile</code>.</p> <p>Now, all that is left is running the workflow, check its outputs and visualise its DAG!</p>"},{"location":"course_material/5_reproducibility_snakemake/#running-the-workflow","title":"Running the workflow","text":"<p>Exercise: Run the workflow. How many DEGs are detected during the analysis?</p> Answer <p>You can run the workflow with: <pre><code>snakemake -c 4 -p --sdm apptainer\n</code></pre> Do not forget to add <code>--sdm apptainer</code>, otherwise Snakemake will not pull the image and the script will be executed in the default environment (which will most likely lead to a crash).</p> <p>During the run, you should see log messages about Snakemake managing the Docker image: <pre><code>Pulling singularity image docker://athiebaut/deseq2:v3.\n[...]\nActivating singularity image /path/to/snakemake_rnaseq/.snakemake/singularity/8bfdbe93244feb95887ab5d33a705017.simg\n</code></pre></p> <p>To find how many genes are differentially expressed, check out the last output file, <code>results/deg_list.tsv</code>. 10 genes are differentially expressed in total: 4 up-regulated and 6 down-regulated.</p> Containerisation of Conda-based workflows <p>Snakemake can also automatically generate a Dockerfile that contains all required environments in a human readable way. If you want to see how a Snakemake-generated Dockerfile looks like, use: <pre><code>snakemake -c 1 --containerize &gt; Dockerfile\n</code></pre></p> <p>(Optional) Exercise: If you had to re-run the entire workflow from scratch, what command would you use?</p> Answer <p>You can re-run the whole workflow with: <pre><code>snakemake -c 4 -p -F --sdm conda apptainer\n</code></pre></p> <ul> <li><code>-F</code> forces the execution of the entire workflow</li> <li>Remember that you need both Conda and Docker-based environments for this run! You can combine <code>--sdm conda</code> and <code>--sdm apptainer</code> into a single command <code>--sdm conda apptainer</code>. Otherwise, you will lack some software and packages and the workflow will crash!</li> </ul> <p>Exercise: Visualise the DAG of the entire workflow.</p> Answer <p>You can get the DAG with: <pre><code>snakemake -c 1 -p -F --dag | dot -T png &gt; images/total_dag.png\n</code></pre></p> <p>You should get the following DAG (open the picture in a new tab to zoom in):  </p>"},{"location":"course_material/6_hpc_snakemake/","title":"Running Snakemake in an HPC environment","text":""},{"location":"course_material/6_hpc_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand resource usage in each rule with the <code>benchmark</code> directive</li> <li>Optimise resource usage in a workflow with the <code>resources</code> directive</li> <li>Run a Snakemake workflow in an SLURM-based HPC environment</li> <li>Set up workflow-specific configuration profiles</li> </ul>"},{"location":"course_material/6_hpc_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/6_hpc_snakemake/#workflow-from-previous-session","title":"Workflow from previous session","text":"<p>If you didn\u2019t finish the previous part or didn\u2019t do the optional exercises, you can restart from a fully commented workflow with a supplementary .fastq files quality check rule and multithreading implemented in all rules. You can download all the files (workflow and config) here or copy them after cloning the course repository locally: <pre><code>git clone https://github.com/sib-swiss/containers-snakemake-training.git  # Clone repository\ncd containers-snakemake-training/  # Go in repository\ncp -r docs/solutions/session4 destination_path  # Copy folder where needed; adapt destination_path to required path\n</code></pre></p>"},{"location":"course_material/6_hpc_snakemake/#exercises","title":"Exercises","text":"<p>In this last series exercises, you will learn how to get a better idea of the resources used by your workflow and how to optimise their usage, as well as the necessary command line arguments, files and rule directives to run your workflow in an SLURM-based High Performance Computing (HPC) environment. In addition, you will also learn how to set up configuration profiles and further develop the Snakefile to define rule-specific settings.</p>"},{"location":"course_material/6_hpc_snakemake/#benchmarking-a-workflow","title":"Benchmarking a workflow","text":"<p>Knowing how many resources are being used by each job can be very useful to optimise your workflow for both local and remote execution. The <code>benchmark</code> directive allows you to do exactly that. Adding it in your rule definitions will trigger the recording of several metrics for every job generated by your workflow execution. Each job will have an associated benchmark file with the following information:</p> s h: m: s max_rss max_vms max_uss max_pss io_in io_out mean_load cpu_time 31.3048 0:00:31 763.04 904.29 757.89 758.37 1.81 230.18 37.09 11.78 <ul> <li><code>s</code> and <code>h:m:s</code> give the job wall clock time (in seconds and hours-minutes-seconds, respectively), which is the actual time taken from the start of a software to its end. You can use these results to infer a safe value for the <code>runtime</code> keyword</li> <li>Likewise, you can use <code>max_rss</code> (shown in megabytes) to figure out how much memory was used by the job and use this value in the <code>memory</code> keyword</li> </ul> What are the other columns? <p>In case you are wondering about the other columns of the table, the official documentation has detailed explanations about their content.</p> <p>Exercise: Add the and <code>benchmark</code> directive to the rules. You can follow the same logic as you did with the <code>log</code> directive.</p> Benchmarks and wildcards <p><code>benchmark</code> directives must contain the same <code>wildcards</code> as the <code>output</code> directive, here <code>sample</code></p> Answer <p>The code snippet below shows the answer for the <code>reads_quantification_genes</code> rule. You can do the same with the other rules. <pre><code>rule reads_quantification_genes:\n    \"\"\"\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly\n    \"\"\"\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    params:\n        annotations = config['annotations']\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt' # Path to the benchmark file\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a {params.annotations} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>This means that a workflow has to run at least once to produce benchmark values, which leads to a \u201cvicious circle\u201d: to run a workflow, you need benchmark information. But to get them, you need to run the workflow. A good practice is to make a first run with only a small part of the samples and sensible resource requirements to allow for the benchmarking to happen.</p>"},{"location":"course_material/6_hpc_snakemake/#controlling-memory-usage-and-runtime","title":"Controlling memory usage and runtime","text":"<p>Controlling resources such as the amount of memory or runtime of each job is a very good way to optimise resource usage in a workflow. This ensures your instance (computer, HPC cluster\u2026) won\u2019t run out of memory during computation (which could interrupt jobs or even crash the instance) and that your jobs will run in a reasonable amount of time (a job taking more time to run than usual might be a sign that something is going on).</p> Resource usage and schedulers <p>Optimising resource usage is especially important when submitting jobs to a scheduler (for instance on a cluster), as it allows a better and more precise definition of your job priority: jobs with low threads/memory/runtime requirements often have a higher priority than heavy jobs, which means they will often start first.</p> <p>Controlling memory usage and runtime in Snakemake is very simple: you only need to need to use the <code>resources</code> directive with the <code>memory</code> or <code>runtime</code> keywords and in most software, you don\u2019t even need to specify the amount of memory available to the software via a parameter.</p> <p>Here are some suggested values of memory usage in the current workflow:</p> <ul> <li><code>fastq_trim</code>: 500 MB</li> <li><code>read_mapping</code>: 2 GB</li> <li><code>sam_to_bam</code>: 250 MB</li> <li><code>reads_quantification_genes</code>: 500 MB</li> </ul> <p>Exercise: Implement memory usage limits in the workflow.</p> Two ways to declare memory values  <p>There are two ways to declare memory values in <code>resources</code>:</p> <ol> <li><code>mem_&lt;unit&gt; = n</code></li> <li><code>mem = 'n&lt;unit&gt;'</code>: in this case, you must pass a string, so you have to enclose <code>n&lt;unit&gt;</code> with quotes <code>''</code></li> </ol> <p><code>&lt;unit&gt;</code> is a unit in [B, KB, MB, GB, TB, PB, KiB, MiB, GiB, TiB, PiB] and <code>n</code> is a float.</p> Answer <p>We implemented memory usage control in all the rules so that you can check everything. We implemented all the memory usage definitions using <code>mem_mb</code>. Feel free to copy this in your Snakefile: <pre><code>rule fastq_trim:\n    '''\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    '''\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    log:\n        'logs/{sample}/{sample}_atropos_trimming.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_trimming.txt'\n    resources:  # Add directive\n        mem_mb = 500  # Add keyword and value with format 1\n        # mem = '500MB'  # Add keyword and value with format 2\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/atropos%3A1.1.32--py312hf67a6ed_2'\n    shell:\n        '''\n        echo \"Trimming reads in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt; {log}\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" --threads {threads} \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2} &amp;&gt;&gt; {log}\n        echo \"Trimmed files saved in &lt;{output.trim1}&gt; and &lt;{output.trim2}&gt; respectively\" &gt;&gt; {log}\n        echo \"Trimming report saved in &lt;{log}&gt;\" &gt;&gt; {log}\n        '''\n\nrule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    params:\n        index = config['index']\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    resources:\n        mem_mb = 2000\n    threads: 4\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x {params.index} --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n\nrule sam_to_bam:\n    '''\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    '''\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    log:\n        'logs/{sample}/{sample}_mapping_sam_to_bam.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping_sam_to_bam.txt'\n    resources:\n        mem_mb = 250\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/samtools%3A1.21--h50ea8bc_0'\n    shell:\n        '''\n        echo \"Converting &lt;{input.sam}&gt; to BAM format\" &gt; {log}\n        samtools view {input.sam} --threads {threads} -b -o {output.bam} 2&gt;&gt; {log}\n        echo \"Sorting .bam file\" &gt;&gt; {log}\n        samtools sort {output.bam} --threads {threads} -O bam -o {output.bam_sorted} 2&gt;&gt; {log}\n        echo \"Indexing sorted .bam file\" &gt;&gt; {log}\n        samtools index -b {output.bam_sorted} -o {output.index} 2&gt;&gt; {log}\n        echo \"Sorted file saved in &lt;{output.bam_sorted}&gt;\" &gt;&gt; {log}\n        '''\n\nrule reads_quantification_genes:\n    '''\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly. The strandedness parameter\n    is determined by get_strandedness().\n    '''\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    params:\n        annotations = config['annotations']\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt'\n    resources:\n        mem_mb = 500\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/subread%3A2.0.6--he4a0461_2'\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p --countReadPairs \\\n        -B -C --largestOverlap --verbose -F GTF \\\n        -a {params.annotations} -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>In the following section we will see that the values established in <code>resources</code> will be very useful to communicate with the job schedulers.</p>"},{"location":"course_material/6_hpc_snakemake/#setting-up-snakemake-to-send-jobs-via-the-slurm-job-scheduler","title":"Setting up Snakemake to send jobs via the SLURM job scheduler","text":"<p>Running jobs remotely on an HPC environment usually requires interaction with job schedulers such as SLURM. Thankfully, adapting our workflow to send jobs via a scheduler does not require a lot of changes. In this section you will use the <code>cluster-generic</code> plugin to send jobs via SLURM.</p> Snakemake plugins <p>In Snakemake v8+, interaction with job schedulers requires the installation of certain plugins. There are multiple plugins available for different schedulers. You can find them here.</p> <p>In this tutorial, you will use the <code>cluster-generic</code> plugin and use it to configure how to send jobs through SLURM. While there is a SLURM-specific plugin, we will focus on the <code>cluster-generic</code> one because it can be used with any scheduler.</p> <p>Among others, the <code>cluster-generic</code> plugin can take the following settings, which can be passed in different ways:</p> <ul> <li><code>--cluster-generic-submit-cmd</code>: command that will be used to submit each job to the cluster. Since we are using SLURM, the command is <code>sbatch</code>. This string can contain any valid <code>sbatch</code> arguments and use values from the <code>Snakefile</code> (e.g. using <code>threads</code> specified in the Snakefile as a value for the <code>--cpus-per-task</code> argument)</li> <li><code>--cluster-generic-cancel-cmd</code>: command to use to cancel jobs. This is important if you want to stop your workflow while jobs are running while also cancel the running jobs</li> </ul> <p>Then, you will need to tell Snakemake to run the workflow through SLURM. An important parameter when doing so is <code>--jobs</code>, which will tell Snakemake how many jobs it can submit concurrently. In addition, you will also need to specify which executor plugin to use and the required settings: <pre><code>snakemake \\\n    --executor cluster-generic \\\n    --cluster-generic-submit-cmd \\\n        'sbatch &lt;slurm_arguments&gt;' \\\n    --cluster-generic-cancel-cmd 'scancel' \\\n    --jobs 2\n</code></pre></p> <p>The command above will:</p> <ul> <li>Tell Snakemake to use the <code>cluster-generic</code> plugin</li> <li>Use the <code>sbatch</code> command to submit jobs</li> <li>Use the <code>scancel</code> command to cancel submitted jobs if you stop the execution of the workflow</li> <li>Set the maximum number of simultaneously running jobs to 2</li> </ul> <p>As you can see, the command to launch the workflow can get quite long. Thankfully, Snakemake allows the usage of configuration profiles to store all of these settings in a file, increasing reproducibility and also simplifying running your workflow. The next section will show how to set up a config file.</p>"},{"location":"course_material/6_hpc_snakemake/#using-configuration-profiles","title":"Using configuration profiles","text":"<p>Configuration profiles allow you to specify settings regarding the execution of your workflow. Currently, Snakemake supports two types of profiles: global and workflow-specific. In this tutorial we will focus on workflow-specific profiles.</p> <p>Exercise: Create a workflow profile named <code>slurm_profile</code> by:</p> <ol> <li>Creating a directory named <code>slurm_profile</code> in the project directory.</li> <li>Creating a <code>config.yaml</code> file inside <code>slurm_profile</code>.</li> </ol> File structure example <p>Your file structure should resemble this: <pre><code>\u2502\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 config.yaml\n\u2502\u2500\u2500 slurm_profile\n\u2502   \u2514\u2500\u2500 config.yaml\n\u2514\u2500\u2500 workflow\n    \u2502\u2500\u2500 Snakefile\n    \u2502\u2500\u2500 envs\n    \u2502   \u2502\u2500\u2500 tool1.yaml\n    \u2502   \u2514\u2500\u2500 tool2.yaml\n    \u2502\u2500\u2500 rules\n    \u2502   \u2502\u2500\u2500 module1.smk\n    \u2502   \u2514\u2500\u2500 module2.smk\n    \u2514\u2500\u2500 scripts\n        \u2502\u2500\u2500 script1.py\n        \u2514\u2500\u2500 script2.R\n</code></pre></p> <p>The <code>config.yaml</code> file inside the <code>slurm_profile</code> directory can contain multiple settings related to the execution of the workflow. Below you can find an example of configuration file: <pre><code>executor: cluster-generic\ncluster-generic-submit-cmd: 'sbatch --job-name={rule}_{wildcards} --cpus-per-task={threads}'\njobs: 2\n</code></pre></p> <p>Exercise: What do <code>--job-name={rule}_{wildcards}</code> and <code>--cpus-per-task={threads}</code> do?</p> Answer <p>These settings allow binding information defined in the workflow to SLURM arguments such as <code>--job-name</code> or <code>--cpus-per-task</code>. For example, a rule where <code>threads: 2</code>, <code>--cpus-per-task={threads}</code> will become <code>--cpus-per-task=2</code>, indicating SLURM that the job should have 2 cpus available.</p> <p>Exercise: Add the required SLURM argument to:</p> <ul> <li>Log the SLURM output of each job to a file in <code>slurm_logs/{rule}/{rule}_{wildcards}.log</code></li> <li>Use the thread number specified in each rule during job submission</li> <li>Use the <code>scancel</code> command to cancel running jobs</li> </ul> SLURM arguments <p>You can find the SLURM argument to use by running <code>sbatch -h</code> or by checking this SLURM cheatsheet.</p> Make sure <code>slurm_logs</code> exists! <p>In older SLURM versions, the directory <code>slurm_logs</code> needed to exist before running the workflow! Therefore, in order to be able to save the logs into a directory, you need to have created the folder before running Snakemake. There are two ways to go about this:</p> <ul> <li>Manually create the folder before running the workflow</li> <li>Adding the directory creation command in <code>cluster-generic-submit-cmd</code>. This option ensures that no problems arise if you are running the workflow for the very first time</li> </ul> Answer <p>Your config file should look like this: <pre><code>executor: cluster-generic\ncluster-generic-submit-cmd:\n    \"mkdir -p slurm_logs/{rule} &amp;&amp;\n    sbatch\n        --job-name={rule}_{wildcards}\n        --cpus-per-task={threads}\n        --output=slurm_logs/{rule}/{rule}_{wildcards}.log\"\ncluster-generic-cancel-cmd: scancel\n</code></pre> Here\u2019s a quick breakdown of the arguments:</p> <ul> <li><code>mkdir -p slurm_logs/{rule}</code>: create directory where logs will be saved</li> <li><code>--job-name={rule}_{wildcards}</code>: set job name to rule name and wildcards</li> <li><code>--cpus-per-task={threads}</code>: set number of CPUs to number of threads defined in the rule</li> <li><code>--output=slurm_logs/{rule}/{rule}_{wildcards}.log</code>: set output file for job logs to <code>slurm_logs/{rule}/{rule}_{wildcards}.log</code></li> <li><code>cluster-generic-cancel-cmd: scancel</code>: set command to cancel jobs to <code>scancel</code></li> </ul>"},{"location":"course_material/6_hpc_snakemake/#passing-rule-specific-resources-to-the-job-scheduler","title":"Passing rule-specific resources to the job scheduler","text":"<p>As shown before, it is possible to specify rule-specific memory usage to have better control over how the workflow uses the available resources with the <code>resources</code> directive: <pre><code>rule myrule:\n    input:\n        ...\n    output:\n        ...\n    resources:\n        mem_mb = 100\n    shell:\n        \"\"\"\n        cp {input} {output};\n        \"\"\"\n</code></pre></p> <p>This will ensure that jobs spawn from rule <code>myrule</code> never use more than 100 MB of RAM. Note that the jobs will crash if they surpass this limit, so try to account for some wiggle room when you set resources.</p> <p>Exercise: Update the configuration profile to set the amount of memory per job to the values specified in each rule through the <code>resources</code> directive. To simplify things, make sure all rule memory requirements are set in megabytes.</p> <p>Think about the SLURM parameter you need to use and how to set its value</p> Answer <p>The SLURM parameter to use is <code>--mem</code>. Its value should be set to the <code>mem_mb</code> keyword defined in the <code>resources</code> directive: <pre><code>executor: cluster-generic\ncluster-generic-submit-cmd:\n    \"mkdir -p logs/{rule} &amp;&amp;\n    sbatch\n        --job-name={rule}_{wildcards}\n        --cpus-per-task={threads}\n        --output=slurm_logs/{rule}/{rule}_{wildcards}.log\n        --mem={resources.mem_mb}\"\ncluster-generic-cancel-cmd: scancel\n</code></pre></p> <p>In addition to remote execution parameters, a configuration profile allows to set anything that can be passed through the command line. This can significantly simplify launching a workflow if there are a lot of arguments to pass when to run it. For example, the extra lines below indicate that we should run a maximum of 2 jobs at a time and to use conda and singularity: <pre><code>executor: cluster-generic\ncluster-generic-submit-cmd:\n  \"mkdir -p slurm_logs/{rule} &amp;&amp;\n  sbatch\n    --job-name={rule}-{wildcards}\n    --cpus-per-task={threads}\n    --output=slurm_logs/{rule}/{rule}_{wildcards}.log\n    --mem={resources.mem_mb}\"\ncluster-generic-cancel-cmd: scancel\njobs: 2\nsoftware-deployment-method:\n  - conda\n  - apptainer\n</code></pre></p> <p>Exercise: Add the <code>jobs</code> and <code>software-deployment-method</code> parameters to your configuration profile.</p>"},{"location":"course_material/6_hpc_snakemake/#adapting-the-workflow-to-the-available-resources","title":"Adapting the workflow to the available resources","text":"<p>Before launching a workflow, it is very important to quantify the resources available in the machine supposed to run it. This information is typically provided by the HPC user guide. In our case, you can use the following commands to know the total number of cores and the amount of memory available in the server: <pre><code>nproc --all    # Number of cores\nfree -g        # Amount of memory in GB in row \"Mem\" and column \"total\"\n</code></pre></p> <p>Exercise: Based on the available resources, figure out if you need to update any of the rule resources.</p> Answer <p>The cluster has 48 CPUs and 96 GB of memory, so there is no need to change the resource settings. It is also more than enough to run several jobs in parallel without any problem.</p> Always check the available resources <p>A very important role of schedulers is to ensure that running jobs do not surpass the total amount of available resources. When a job is asking for more resources than a machine has (e.g. the job asks for 50 CPUs, but the machine only has 20), the HPC (if configured properly) will prevent Snakemake from submitting the job and the workflow will fail. However, if the HPC is not configured properly or if the workflow is running on another type of instance without proper safety checks, Snakemake will be able launch the job but will display the message <code>Waiting for more resources.</code> to indicate that the job is waiting until enough resources are available for it to run, which can never happen. In this case, Snakemake will get stuck and never manage to launch the job.</p>"},{"location":"course_material/6_hpc_snakemake/#final-exercise","title":"Final exercise","text":"<p>Exercise: To conclude, run the workflow in the HPC environment with the following command: <pre><code>snakemake --profile slurm_profile\n</code></pre></p> <p>You can then see what jobs are being run by using the <code>squeue</code> command in combination with <code>watch</code> to check the status of your workflow on regular intervals (here, 10s): <pre><code>watch -n 10 squeue -u &lt;username&gt;\n</code></pre></p> <p>This will give you information such as the job id, the job status, how long is has been running for, and the reason if the status is <code>PD</code> (pending).</p> JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 91 local read_map user1 PD 0:00 1 (Resources) 90 local read_map user1 R 0:30 1 localhost <p>Congratulations, you made it to the end! You are now able to create a Snakemake workflow, make it reproducible thanks to Conda and Docker/Apptainer and even run it in an HPC! This is a great time to get yourself a coffee/tea and celebrate!  </p> <p>To make things even better, have a look at some additional concepts and Snakemake\u2019s best practices!</p>"},{"location":"course_material/7_debugging_snakemake/","title":"Additional concepts and workflow design/debugging","text":""},{"location":"course_material/7_debugging_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand the inner workings and order of operations in Snakemake</li> <li>Efficiently design and debug a Snakemake workflow</li> <li>Use flags to delete or protect outputs</li> <li>Use flags to consider a directory as output</li> </ul>"},{"location":"course_material/7_debugging_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/7_debugging_snakemake/#designing-a-workflow","title":"Designing a workflow","text":"<p>There are many ways to design a new workflow, but these few pieces of advice will be useful in most cases:</p> <ul> <li>Start with a pen and paper: try to find out how many rules you will need and how they depend on each other. In other terms, start by sketching the DAG of your workflow!<ul> <li>Remember that Snakemake has a bottom-up approach (it goes from the final outputs to the first input), so it may be easier for you to work in that order as well and write your last rule first</li> <li>Determine which rules (if any) aggregate or split inputs and create input functions accordingly (the topic is tackled in this series of exercises)</li> </ul> </li> <li>Make sure your input and output directives are right before worrying about anything else, especially the shell sections. There is no point in executing commands with the wrong inputs/outputs!<ul> <li>Remember that Snakemake builds the DAG before running the shell commands, so you can use the <code>-n/--dry-run/--dryrun</code> parameter to test the workflow before running it. You can even do that without writing all the shell commands!</li> </ul> </li> <li>List any parameters or settings that might need to be adjusted later</li> <li>Choose meaningful and easy-to-understand names for your rules, inputs, outputs, parameters, <code>wildcards</code>\u2026 to make your Snakefile as readable as possible. This is true for every script, piece of code, variable\u2026 and Snakemake is no exception! Have a look at The Zen of Python for more information</li> </ul>"},{"location":"course_material/7_debugging_snakemake/#debugging-a-workflow","title":"Debugging a workflow","text":"<p>It is very likely you will see bugs and errors the first time you try to run a new Snakefile: don\u2019t be discouraged, this is normal!</p>"},{"location":"course_material/7_debugging_snakemake/#order-of-operations-in-snakemake","title":"Order of operations in Snakemake","text":"<p>The topic was approached when we discussed DAGs, but to efficiently debug a workflow, it is worth taking a deeper look at what Snakemake does when you execute the command <code>snakemake -c 1 &lt;target&gt;</code>. There are 3 main phases:</p> <ol> <li>Prepare to run:<ol> <li>Read all the rule definitions from the Snakefile</li> </ol> </li> <li>Resolve the DAG (happens when Snakemake says \u2018Building DAG of jobs\u2019):<ol> <li>Check what output(s) are required</li> <li>Look for matching input(s) by looking at the outputs of all the rules</li> <li>Fill in the <code>wildcards</code> to determine the exact input(s) of the matching rule(s)</li> <li>Check whether this(these) input(s) is(are) available; if not, repeat Step 2 until everything is resolved</li> </ol> </li> <li>Run:<ol> <li>If needed, create the output(s) folder path</li> <li>If needed, remove the outdated output(s)</li> <li>Fill in the placeholders and run the shell commands</li> <li>Check that the commands ran without errors and produced the expected output(s)</li> </ol> </li> </ol>"},{"location":"course_material/7_debugging_snakemake/#debugging-advice","title":"Debugging advice","text":"<p>Sometimes, Snakemake will give you a precise error report, but other times\u2026 less so. Try to identify which phase of execution failed (see previous paragraph on order of operations) and double-check the most common error causes for that phase:</p> <ol> <li>Parsing phase failures (phase 1):<ul> <li>Syntax errors, among which (but not limited to):<ul> <li>Missing commas/colons/semicolons</li> <li>Unbalanced quotes/brackets/parenthesis</li> <li>Wrong indentation</li> <li>These errors can be easily solved using a text editor with Python/Snakemake text colouring</li> </ul> </li> <li>Failure to evaluate expressions<ul> <li>Problems in functions (<code>expand()</code>, input functions\u2026) in input/output directives</li> <li>Python logic added outside of rules</li> </ul> </li> <li>Other problems with rule definition<ul> <li>Invalid rule names/directives</li> <li>Invalid wildcard names</li> <li>Mismatched <code>wildcards</code></li> </ul> </li> </ul> </li> <li>DAG building failures (phase 2, before Snakemake tries to run any job):<ul> <li>Failure to determine the target</li> <li>Ambiguous rules making the same output(s)</li> <li>On the contrary, no rule making the required output(s)</li> <li>Circular dependency (violating the \u2018Acyclic\u2019 property of a DAG).</li> <li>Write-protected output(s)</li> </ul> </li> <li>DAG running failures (phase 3, <code>--dry-run</code> works and builds the DAG, but the jobs execution fails):<ul> <li>Shell command returning non-zero status</li> <li>Missing output file(s) after the commands have run</li> <li>Reference to a <code>$shell_variable</code> before it was set</li> <li>Use of a wrong/unknown placeholder inside <code>{ }</code></li> <li>When a job fails, Snakemake reports an error, deletes all output file(s) for that job (because of potential corruption), and stops</li> </ul> </li> </ol>"},{"location":"course_material/7_debugging_snakemake/#using-non-conventional-outputs","title":"Using non-conventional outputs","text":"<p>This part is an extra-exercise about non-conventional Snakemake outputs. It is quite long, so do it only if you finished the rest of the course beforehand.</p> <p>Snakemake has several built-in utilities to assign properties to outputs that are deemed \u2018special\u2019. These properties are listed in the table below:</p> Property Syntax Function Temporary <code>temp('file.txt')</code> File is deleted as soon as it is not required by a future jobs Protected <code>protected('file.txt')</code> File cannot be overwritten after job ends (useful to prevent erasing a file by mistake, for example files requiring heavy computation) Ancient <code>ancient('file.txt')</code> Ignore file timestamp and assume file is older than any outputs: file will not be re-created when re-running workflow, except when <code>--force</code> options are used Directory <code>directory('directory')</code> Output is a directory instead of a file (use <code>touch()</code> instead if possible) Touch <code>touch('file.txt')</code> Create an empty flag file <code>file.txt</code> regardless of shell commands (only if commands finished without errors) <p>The next paragraphs will show how to use some of these properties.</p>"},{"location":"course_material/7_debugging_snakemake/#removing-and-safeguarding-outputs","title":"Removing and safeguarding outputs","text":"<p>This part shows a few examples using <code>temp()</code> and <code>protected()</code> flags.</p> <p>Exercise: Can you think of a convenient use of the <code>temp()</code> flag?</p> Answer <p><code>temp()</code> is extremely useful to automatically remove intermediary outputs that are no longer needed.</p> <p>Exercise: In your workflow, identify outputs that are intermediary and mark them as temporary with <code>temp()</code>.</p> Answer <p>Unsorted .bam and .sam outputs seem like great candidates to be marked as temporary. One could argue that trimmed .fastq files could also be marked as temporary, but we won\u2019t bother with them here. For clarity, only lines that changed are shown below:</p> <ul> <li> <p>rule <code>read_mapping</code>: <pre><code>output:\n    sam = temp('results/{sample}/{sample}_mapped_reads.sam'),\n</code></pre></p> </li> <li> <p>rule <code>sam_to_bam</code>: <pre><code>output:\n    bam = temp('results/{sample}/{sample}_mapped_reads.bam'),\n</code></pre></p> </li> </ul> Advantages and drawbacks of <code>temp()</code> <p>On one hand, removing temporary outputs is a great way to save storage space. If you look at the size of your current <code>results/</code> folder (<code>du -bchd0 results/</code>), you will notice that it drastically increased. Just removing these two files would allow to save ~1 GB. While it may not seem like much, remember that you usually have much bigger files and many more samples!</p> <p>On the other hand, using temporary outputs might force you to re-run more jobs than necessary if an input changes, so think carefully before using <code>temp()</code>.</p> <p>Exercise: On the contrary, is there a file of your workflow that you would like to protect? If so, mark it with <code>protected()</code>.</p> Answer <p>Sorted .bam files from rule <code>sam_to_bam</code> seem like good candidates for protection: <pre><code>output:\n    bam_sorted = protected('results/{sample}/{sample}_mapped_reads_sorted.bam'),\n</code></pre> If you set this output as protected, be careful when you want to re-run your workflow to recreate the file!</p>"},{"location":"course_material/7_debugging_snakemake/#using-an-output-directory-the-fastqc-example","title":"Using an output directory: the FastQC example","text":"<p>FastQC is a program designed to spot potential problems in high-througput sequencing datasets. It is a very popular tool, notably because it is fast and does not require a lot of configuration. It runs a set of analyses on one or more sequence files in FASTQ or BAM format and produces a quality report with plots that summarise the results. It highlights potential problems that might require a closer look in your dataset.</p> <p>As such, it would be interesting to run FastQC on the original and trimmed .fastq files to check whether trimming actually improved read quality. FastQC can be run interactively or in batch mode, during which it saves results as an HTML file and a ZIP file. You will later see that running FastQC in batch mode is a bit harder than it looks.</p> Data types and FastQC <p>FastQC does not differentiate between sequencing techniques and as such can be used to look at libraries coming from a large number of experiments (Genomic Sequencing, ChIP-Seq, RNAseq, BS-Seq etc\u2026).</p> <p>If you run <code>fastqc -h</code>, you will notice something a bit surprising, but not unusual in bioinformatics: <pre><code>    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it. If this option is not set then the\n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n\n   -d --dir         Selects a directory to be used for temporary files written when\n                    generating report images. Defaults to system temp directory if\n                    not specified.\n</code></pre></p> <p>Two files are produced for each .fastq file and these files appear in the same directory as the input file: FastQC does not allow to specify the output file names! However, you can set an alternative output directory, even though it needs to be manually created before FastQC is run.</p> <p>There are different solutions to this problem:</p> <ol> <li>Work with the default file names produced by FastQC and leave the reports in the same directory as the input files</li> <li>Create the outputs in a new directory and leave the reports with their default name</li> <li>Create the outputs in a new directory and tell Snakemake that the directory itself is the output</li> <li>Force a naming convention by manually renaming the FastQC output files within the rule</li> </ol> <p>It would be too long to test all four solutions, so you will work on the 3<sup>rd</sup> or the 4<sup>th</sup> solution. Here is a brief summary of solutions 1 and 2:</p> <ol> <li>This could work, but it\u2019s better not to put reports in the same directory as input sequences. As a general principle, when writing Snakemake rules, we prefer to be in charge of the output names and to have all the files linked to a sample in the same directory</li> <li>This involves manually constructing the output directory path to use with the <code>-o</code> parameter, which works but isn\u2019t very convenient</li> </ol> <p>The first part of the FastQC command is: <pre><code>fastqc --format fastq --threads {threads} --outdir {folder_path} --dir {folder_path} &lt;input_fastq1&gt; &lt;input_fastq2&gt;\n</code></pre></p> Explanation of FastQC parameters <ul> <li><code>-t/--threads</code>: specify how many files can be processed simultaneously. Here, it will be 2 because inputs are paired-end files</li> <li><code>-o/--outdir</code>: create output files in specified output directory</li> <li><code>-d/--dir</code>: select a directory to be used for temporary files</li> </ul> <p>Choose between solution 3 or 4 and implement it. You will find more information and help below.</p> Solution 3Solution 4 <p>This option is equivalent to tell Snakemake not to worry about individual files at all and consider an entire directory as the rule output.</p> <p>Exercise: Implement a single rule to run FastQC on both the original and trimmed .fastq files (four files in total) using directories as ouputs with the <code>directory()</code> flag.</p> <ul> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0</code></li> </ul> Answer <p>This makes the rule definition quite \u2018simple\u2019 compared to solution 4: <pre><code>rule fastq_qc_sol3:\n    '''\n    This rule performs a QC on paired-end .fastq files before and after trimming.\n    '''\n    input:\n        reads1 = rules.fastq_trim.input.reads1,\n        reads2 = rules.fastq_trim.input.reads2,\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        before_trim = directory('results/{sample}/fastqc_reports/before_trim/'),\n        after_trim = directory('results/{sample}/fastqc_reports/after_trim/')\n    log:\n        'logs/{sample}/{sample}_fastqc.log'\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0'\n    shell:\n        '''\n        echo \"Creating output directory &lt;{output.before_trim}&gt;\" &gt; {log}\n        mkdir -p {output.before_trim} 2&gt;&gt; {log}  # FastQC doesn't create output directories so we have to do it manually\n        echo \"Performing QC of reads before trimming in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {output.before_trim} \\\n        --dir {output.before_trim} {input.reads1} {input.reads2} &amp;&gt;&gt; {log}\n        echo \"Results saved in &lt;{output.before_trim}&gt;\" &gt;&gt; {log}\n        echo \"Creating output directory &lt;{output.after_trim}&gt;\" &gt;&gt; {log}\n        mkdir -p {output.after_trim} 2&gt;&gt; {log}  # FastQC doesn't create output directories so we have to do it manually\n        echo \"Performing QC of reads after trimming in &lt;{input.trim1}&gt; and &lt;{input.trim2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {output.after_trim} \\\n        --dir {output.after_trim} {input.trim1} {input.trim2} &amp;&gt;&gt; {log}\n        echo \"Results saved in &lt;{output.after_trim}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <code>.snakemake_timestamp</code> <p>When <code>directory()</code> is used, Snakemake creates an empty file called <code>.snakemake_timestamp</code> in the output directory. This is the marker file it uses to know whether it needs to re-run the rule producing the directory.</p> <p>Overall, this rule works quite well and allows for an easy rule definition. However, in this case, individual files are not explicitly named as outputs and this may cause problems to chain rules later. Also, remember that some software won\u2019t give you any control at all over the outputs, which is why you need a back-up plan, i.e. solution 4: the most powerful solution is still to use shell commands to move and/or rename files to names you want. Also, the Snakemake developers advise to use <code>directory()</code> only as a last resort and to use <code>touch()</code> instead.</p> <p>This option amounts to let FastQC follows its default behaviour but manually rename the files afterwards to obtain the exact outputs we require.</p> <p>Exercise: Implement a single rule to run FastQC on both the original and trimmed .fastq files (four files in total) and rename the files created by FastQC to the desired output names using the <code>mv &lt;old_name&gt; &lt;new_name&gt;</code> command.</p> <ul> <li>The container image can be found at <code>https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0</code></li> </ul> Answer <p>This makes the rule definition (much) more complicated than solution 3: <pre><code>rule fastq_qc_sol4:\n    '''\n    This rule performs a QC on paired-end .fastq files before and after trimming.\n    '''\n    input:\n        reads1 = rules.fastq_trim.input.reads1,\n        reads2 = rules.fastq_trim.input.reads2,\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        # QC before trimming\n        html1_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_1.html',  # Forward-read report in HTML format before trimming\n        zipfile1_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_1.zip',  # Forward-read report in ZIP format before trimming\n        html2_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_2.html',  # Reverse-read report in HTML format before trimming\n        zipfile2_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_2.zip',  # Reverse-read report in ZIP format before trimming\n        # QC after trimming\n        html1_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_1.html',  # Forward-read report in HTML format after trimming\n        zipfile1_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_1.zip',  # Forward-read report in ZIP format after trimming\n        html2_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_2.html',  # Forward-read report in HTML format after trimming\n        zipfile2_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_2.zip'  # Forward-read report in ZIP format after trimming\n    params:\n        wd = 'results/{sample}/fastqc_reports/',  # Temporary directory to store files before renaming\n        # QC before trimming\n        html1_before = 'results/{sample}/fastqc_reports/{sample}_1_fastqc.html',  # Default FastQC output name for forward-read report in HTML format before trimming\n        zipfile1_before = 'results/{sample}/fastqc_reports/{sample}_1_fastqc.zip',  # Default FastQC output name for forward-read report in ZIP format before trimming\n        html2_before = 'results/{sample}/fastqc_reports/{sample}_2_fastqc.html',  # Default FastQC output name for reverse-read report in HTML format before trimming\n        zipfile2_before = 'results/{sample}/fastqc_reports/{sample}_2_fastqc.zip',  # Default FastQC output name for reverse-read report in ZIP format before trimming\n        # QC after trimming\n        html1_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_1_fastqc.html',# Default FastQC output name for forward-read report in HTML format after trimming\n        zipfile1_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_1_fastqc.zip',# Default FastQC output name for forward-read report in ZIP format after trimming\n        html2_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_2_fastqc.html',# Default FastQC output name for reverse-read report in HTML format after trimming\n        zipfile2_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_2_fastqc.zip'# Default FastQC output name for reverse-read report in ZIP format after trimming\n    log:\n        'logs/{sample}/{sample}_fastqc.log'\n    threads: 2\n    container:\n        'https://depot.galaxyproject.org/singularity/fastqc%3A0.12.1--hdfd78af_0'\n    shell:\n        '''\n        echo \"Creating output directory &lt;{params.wd}&gt;\" &gt; {log}\n        mkdir -p {params.wd} 2&gt;&gt; {log}  # FastQC doesn't create output directories so we have to do it manually\n        echo \"Performing QC of reads before trimming in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {params.wd} \\\n        --dir {params.wd} {input.reads1} {input.reads2} &amp;&gt;&gt; {log}\n        echo \"Renaming results from original fastq analysis\" &gt;&gt; {log}  # Renames files because we can't choose fastqc output\n        mv {params.html1_before} {output.html1_before} 2&gt;&gt; {log}\n        mv {params.zipfile1_before} {output.zipfile1_before} 2&gt;&gt; {log}\n        mv {params.html2_before} {output.html2_before} 2&gt;&gt; {log}\n        mv {params.zipfile2_before} {output.zipfile2_before} 2&gt;&gt; {log}\n        echo \"Performing QC of reads after trimming in &lt;{input.trim1}&gt; and &lt;{input.trim2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {params.wd} \\\n        --dir {params.wd} {input.trim1} {input.trim2} &amp;&gt;&gt; {log}\n        echo \"Renaming results from trimmed fastq analysis\" &gt;&gt; {log}  # Renames files because we can't choose fastqc output\n        mv {params.html1_after} {output.html1_after} 2&gt;&gt; {log}\n        mv {params.zipfile1_after} {output.zipfile1_after} 2&gt;&gt; {log}\n        mv {params.html2_after} {output.html2_after} 2&gt;&gt; {log}\n        mv {params.zipfile2_after} {output.zipfile2_after} 2&gt;&gt; {log}\n        echo \"Results saved in &lt;results/{wildcards.sample}/fastqc_reports/&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>This solution is very long and much more complicated than the other one. However, it makes up for its complexity by allowing a total control on what is happening: with this method, we can choose where temporary files are saved as well as output names. It could have been shortened by using <code>-o .</code> to tell FastQC to create files in the current working directory instead of a specific one, but this would have created another problem: if we run multiple jobs in parallel, then Snakemake may try to produce files from different jobs at the same temporary destination. In this case, the different Snakemake instances would be trying to write to the same temporary files at the same time, overwriting each other and corrupting the output files.</p> <p>Three interesting things are happening in both versions of this rule:</p> <ul> <li>Similarly to outputs, it is possible to refer to inputs of a rule directly in another rule with the syntax <code>rules.&lt;rule_name&gt;.input.&lt;input_name&gt;</code></li> <li>FastQC doesn\u2019t create output directories by itself (other programs might insist that output directories do not already exist), so you have to manually create it with <code>mkdir</code> in the <code>shell</code> directive before running FastQC<ul> <li>Overall, most software will create the required directories for you; FastQC is an exception</li> </ul> </li> <li>When using output files, Snakemake creates the missing folders by itself. However, when using a <code>directory(</code>) output, Snakemake will not create the directory. Otherwise, this would guarantee the rule to succeed every time, independently of the <code>shell</code> directive status: as soon as the directory is created, the rule succeeds, even if the <code>shell</code> directive fails. The current behaviour prevents Snakemake from continuing the workflow when a command may have failed. This also shows that solution 3, albeit simple, can be risky</li> </ul> Controlling execution flow <p>If you want to make sure that a certain rule is executed before another, you can write the outputs of the first rule as inputs of the second one, even if the rule doesn\u2019t use them. For example, you could force the execution of FastQC before mapping reads with one more line in rule <code>read_mapping</code>: <pre><code>rule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq onto a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2,\n        fastqc = rules.fastq_qc_sol4.output.html1_before  # This single line will force the execution of FastQC before read mapping\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    params:\n        index = 'resources/genome_indices/Scerevisiae_index'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    threads: 4\n    container:\n        'https://depot.galaxyproject.org/singularity/hisat2%3A2.2.1--hdbdd923_6'\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x {params.index} --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p>"}]}
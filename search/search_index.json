{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course website","text":""},{"location":"#teachers","title":"Teachers","text":"<ul> <li>Geert van Geest  </li> <li>Antonin Thi\u00e9baut  </li> <li>Damir Zhakparov</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Geert van Geest  </li> <li>Antonin Thi\u00e9baut  </li> <li>Patricia Palagi  </li> </ul>"},{"location":"#attribution","title":"Attribution","text":"<p>This course is partly inspired by the Carpentries Docker course and the official Snakemake tutorial.</p>"},{"location":"#license-copyright","title":"License &amp; copyright","text":"<p>License: CC BY-SA 4.0</p> <p>Copyright: SIB Swiss Institute of Bioinformatics</p>"},{"location":"#material","title":"Material","text":"<ul> <li>This website</li> <li>Google doc (through mail)</li> </ul>"},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>After this course, you will be able to:</p> <ul> <li>Understand the basic concepts and terminology associated with virtualization with containers</li> <li>Customize, store, manage and share containerized environments with Docker</li> <li>Use Apptainer to run containers on a shared computer environment (e.g. a HPC cluster)</li> <li>Understand the basic concepts and terminology associated with workflow management systems</li> <li>Create a computational workflow that uses containers and package managers with Snakemake</li> </ul>"},{"location":"#learning-outcomes-explained","title":"Learning outcomes explained","text":"<p>To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter (found at Course material) starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes we will use lectures, exercises, polls and group work. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.</p>"},{"location":"#exercises","title":"Exercises","text":"<p>Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. All answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.</p>"},{"location":"#asking-questions","title":"Asking questions","text":"<p>During lectures, you are encouraged to raise your hand if you have questions.</p> <p>A main source of communication will be our slack channel. Ask background questions that interest you personally at #background. During the exercises, e.g. if you are stuck or don\u2019t understand what is going on, use the slack channel #q-and-a.  This channel is not only meant for asking questions but also for answering questions of other participants. If you are replying to a question, use the \u201creply in thread\u201d option:</p> <p>The teachers will review the answers, and add/modify if necessary. If you\u2019re really stuck and need specific tutor support, write the teachers or helpers personally.</p> <p>To summarise:</p> <ul> <li>During lectures: raise hand</li> <li>Personal interest questions: #background on slack</li> <li>During exercises: raise hand/#q-and-a on slack</li> </ul>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1-containers","title":"Day 1 - Containers","text":"Block Start End subject Block 1 9:00 AM 10:30 AM Introduction to containers 10:30 AM 11:00 AM BREAK Block 2 11:00 AM 12:30 PM Managing containers and images 12:30 PM 1:30 PM BREAK Block 3 1:30 PM 3:00 PM Working with <code>dockerfiles</code> 3:00 PM 3:30 PM BREAK Block 4 3:30 PM 5:00 PM Running containers with apptainer"},{"location":"course_schedule/#day-2-snakemake","title":"Day 2 - Snakemake","text":"Block Start End subject Block 1 9:00 AM 10:30 AM Introduction to Snakemake 10:30 AM 11:00 AM BREAK Block 2 11:00 AM 12:30 PM Generalising a Snakemake workflow 12:30 PM 1:30 PM BREAK Block 3 1:30 PM 3:00 PM Decorating a Snakemake workflow 3:00 PM 3:30 PM BREAK Block 4 3:30 PM 4:30 PM Snakemake, package managers and containers 4:30 PM 5:00 PM Wrap-up &amp; Open Q&amp;A"},{"location":"precourse/","title":"Precourse preparations","text":""},{"location":"precourse/#unix","title":"UNIX","text":"<p>As is stated in the course prerequisites at the announcement web page. We expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here. If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial.</p>"},{"location":"precourse/#software","title":"Software","text":"<p>Install Docker on your local computer and create an account on dockerhub. You can find instructions here. Note that you need admin rights to install and use Docker, and if you are installing Docker on Windows, you need a recent Windows version. You should also have a modern code editor installed, like Sublime Text or VScode.</p> <p>If working with Windows</p> <p>During the course exercises you will be mainly interacting with docker through the command line. Although windows powershell is suitable for that, it is easier to follow the exercises if you have UNIX or \u2018UNIX-like\u2019 terminal. You can get this by using WSL2. Make sure you install the latest versions before installing docker. </p> <p>If installing Docker is a problem</p> <p>During the course, we can give only limited support for installation issues. If you do not manage to install Docker before the course, you can still do almost all exercises on Play with Docker. A Docker login is required.</p> <p>In addition to your local computer, we will be working on an Amazon Web Services (AWS)  Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through <code>ssh</code> with a username, key and IP address. All participants will be granted access to a personal home directory.</p>"},{"location":"course_material/day1/apptainer/","title":"Running containers with apptainer","text":""},{"location":"course_material/day1/apptainer/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Login to a remote machine with <code>ssh</code></li> <li>Use <code>apptainer pull</code> to convert an image from dockerhub to the \u2018apptainer image format\u2019 (<code>.sif</code>)</li> <li>Execute a apptainer container</li> <li>Explain the difference in default mounting behaviour between <code>docker</code> and <code>apptainer</code></li> <li>Use <code>apptainer shell</code> to generate an interactive shell inside a <code>.sif</code> image</li> <li>Search and use images with both <code>docker</code> and <code>apptainer</code> from bioconda</li> </ul>"},{"location":"course_material/day1/apptainer/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Apptainer documentation</li> <li>Apptainer hub</li> <li>An article on Docker vs Apptainer</li> <li>Using conda and containers with snakemake</li> </ul>"},{"location":"course_material/day1/apptainer/#exercises","title":"Exercises","text":""},{"location":"course_material/day1/apptainer/#login-to-remote","title":"Login to remote","text":"<p>If you are enrolled in the course, you have received an e-mail with an IP, username, private key and password. To do the Apptainer exercises we will login to a remote server. Below you can find instructions on how to login.</p> <p>VScode is a code editor that can be used to edit files and run commands locally, but also on a remote server. In this subchapter we will set up VScode to work remotely.</p> <p>If not working with VScode</p> <p>If you are not working with VScode, you can login to the remote server with the following command:</p> <pre><code>ssh -i key_username.pem\n</code></pre> <p>If you want to edit files directly on the server, you can mount a directory with <code>sshfs</code>. </p> <p>Required installations</p> <p>For this exercise it is easiest if you use VScode. In addition you would need to have followed the instructions to set up remote-ssh:</p> <ul> <li>OpenSSH compatible client. This is usually pre-installed on your OS. You can check whether the command <code>ssh</code> exists. </li> <li>The Remote-SSH extension. To install, open VSCode and click on the extensions icon (four squares) on the left side of the window. Search for <code>Remote-SSH</code> and click on <code>Install</code>.</li> </ul> Windowsmac OS/Linux <p>Open a PowerShell and <code>cd</code> to the directory where you have stored your private key. After that, move it to <code>~\\.ssh</code>:</p> <pre><code>mv .\\key_username.pem ~\\.ssh\n</code></pre> <p>Open a terminal, and <code>cd</code> to the directory where you have stored your private key. After that, change the file permissions of the key and move it to <code>~/.ssh</code>:</p> <pre><code>chmod 400 key_username.pem\nmv key_username.pem ~/.ssh\n</code></pre> <p>Open VScode and click on the green or blue button in the bottom left corner. Select <code>Connect to Host...</code>, and then on <code>Configure SSH Host...</code>. Specify a the location for the config file. Use the same directory as where your keys are stored (so <code>~/.ssh</code>). A skeleton config file will be provided. Edit it, so it looks like this (replace <code>username</code> with your username, and specify the correct IP at <code>HostName</code>):</p> WindowsMacOS/Linux <pre><code>Host sib_course_remote\n    User username\n    HostName 123.456.789.123\n    IdentityFile ~\\.ssh\\key_username.pem\n</code></pre> <pre><code>Host sib_course_remote\n    User username\n    HostName 123.456.789.123\n    IdentityFile ~/.ssh/key_username.pem\n</code></pre> <p>Save and close the config file. Now click again the green or blue button in the bottom left corner. Select <code>Connect to Host...</code>, and then on <code>sib_course_remote</code>. You will be asked which operating system is used on the remote. Specify \u2018Linux\u2019. </p> <p>At first login, initiate conda (for tomorrow):</p> <pre><code>/opt/miniconda3/bin/conda init\n</code></pre> <p>Close this terminal and open a new one, and you are good to go!</p>"},{"location":"course_material/day1/apptainer/#pulling-an-image","title":"Pulling an image","text":"<p>Apptainer can take several image formats (e.g. a <code>docker</code> image), and convert them into it\u2019s own <code>.sif</code> format. Unlike <code>docker</code> this image doesn\u2019t live in a local image cache, but it\u2019s stored as an actual file.</p> <p>Exercise: On the remote server, pull the docker image that has the adjusted default <code>CMD</code> that we have pushed to dockerhub in this exercise (<code>ubuntu-figlet-df:v3</code>) with <code>apptainer pull</code>. The syntax is:</p> <pre><code>apptainer pull docker://[USER NAME]/[IMAGE NAME]:[TAG]\n</code></pre> Answer <p><pre><code>apptainer pull docker://[USER NAME]/ubuntu-figlet:v3\n</code></pre> This will result in a file called <code>ubuntu-figlet_v3.sif</code></p> <p>Note</p> <p>If you weren\u2019t able to push the image in the previous exercises to your docker hub, you can use <code>geertvangeest</code> as username to pull the image.</p>"},{"location":"course_material/day1/apptainer/#executing-an-image","title":"Executing an image","text":"<p>These <code>.sif</code> files can be run as standalone executables:</p> <pre><code>./ubuntu-figlet_v3.sif\n</code></pre> <p>Note</p> <p>This is shorthand for:</p> <pre><code>apptainer run ubuntu-figlet_v3.sif\n</code></pre> <p>And you can overwrite the default command like this:</p> <pre><code>apptainer run [IMAGE NAME].sif [COMMAND]\n</code></pre> <p>Note</p> <p>In this case, you can also use</p> <pre><code>./[IMAGE NAME].sif [COMMAND]\n</code></pre> <p>However, most applications require <code>apptainer run</code>. Especially if you want to provide options like <code>--bind</code> (for mounting directories). </p> <p>Exercise: Run the <code>.sif</code> file without a command, and with a command that runs <code>figlet</code>. Do you get expected output? Do the same for the R image you\u2019ve created in the previous chapter.</p> <p>Entrypoint and apptainer</p> <p>The <code>daterange</code> image has an entrypoint set, and <code>apptainer run</code> does not overwrite it. In order to ignore both the entrypoint and cmd use <code>apptainer exec</code>.  </p> Answer <p>Running it without a command (<code>./ubuntu-figlet_v3.sif</code>) should give:</p> <p><pre><code>__  __         _                                                 _        _\n|  \\/  |_   _  (_)_ __ ___   __ _  __ _  ___  __      _____  _ __| | _____| |\n| |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| |\n| |  | | |_| | | | | | | | | (_| | (_| |  __/  \\ V  V / (_) | |  |   &lt;\\__ \\_|\n|_|  |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___|   \\_/\\_/ \\___/|_|  |_|\\_\\___(_)\n       |___/                     |___/\n</code></pre> Which is the default command that we changed in the <code>Dockerfile</code>.</p> <p>Running with a another <code>figlet</code> command:</p> <pre><code>./ubuntu-figlet_v3.sif figlet 'Something else'\n</code></pre> <p>Should give:</p> <pre><code>____                       _   _     _                    _\n/ ___|  ___  _ __ ___   ___| |_| |__ (_)_ __   __ _    ___| |___  ___\n\\___ \\ / _ \\| '_ ` _ \\ / _ \\ __| '_ \\| | '_ \\ / _` |  / _ \\ / __|/ _ \\\n___) | (_) | | | | | |  __/ |_| | | | | | | | (_| | |  __/ \\__ \\  __/\n|____/ \\___/|_| |_| |_|\\___|\\__|_| |_|_|_| |_|\\__, |  \\___|_|___/\\___|\n                                             |___/\n</code></pre> <p>Pulling the <code>deseq2</code> image:</p> <pre><code>apptainer pull docker://[USER NAME]/deseq2:v1\n</code></pre> <p>Running it without command:</p> <pre><code>./deseq2.sif\n</code></pre> <p>Running with a command:</p> <pre><code>./deseq2.sif --rows 100\n</code></pre> <p>To overwrite both entrypoint and the command:</p> <pre><code>apptainer exec deseq2.sif test_deseq2.R --rows 200\n</code></pre>"},{"location":"course_material/day1/apptainer/#mounting-with-apptainer","title":"Mounting with Apptainer","text":"<p>Apptainer is also different from Docker in the way it handles mounting. By default, Apptainer binds your home directory and a number of paths in the root directory to the container. This results in behaviour that is almost like if you are working on the directory structure of the host.  </p> <p>If your directory is not mounted by default</p> <p>It depends on the apptainer settings whether most directories are mounted by default to the container. If your directory is not mounted, you can do that with the <code>--bind</code> option of <code>apptainer exec</code>:</p> <pre><code>apptainer exec --bind /my/dir/to/mount/ [IMAGE NAME].sif [COMMAND]\n</code></pre> <p>Running the command <code>pwd</code> (full name of current working directory) will therefore result in a path on the host machine:</p> <pre><code>./ubuntu-figlet_v3.sif pwd\n</code></pre> <p>Exercise: Run the above command. What is the output? How would the output look like if you would run a similar command with Docker?</p> <p>Hint</p> <p>A similar Docker command would look like (run this on your local computer):</p> <pre><code>docker run --rm ubuntu-figlet:v3 pwd\n</code></pre> Answer <p>The output of <code>./ubuntu-figlet_v3.sif pwd</code> is the current directory on the host: i.e. <code>/home/username</code> if you have it in your home directory. The output of <code>docker run --rm ubuntu-figlet:v3 pwd</code> (on the local host) would be <code>/</code>, which is the default workdir (root directory) of the container. As we did not mount any host directory, this directory exists only within the container (i.e. separated from the host).</p>"},{"location":"course_material/day1/apptainer/#interactive-shell","title":"Interactive shell","text":"<p>If you want to debug or inspect an image, it can be helpful to have a shell inside the container. You can do that with <code>apptainer shell</code>:</p> <pre><code>apptainer shell ubuntu-figlet_v3.sif\n</code></pre> <p>Note</p> <p>To exit the shell type <code>exit</code>.</p> <p>Exercise: Can you run <code>figlet</code> inside this shell?</p> Answer <p>Yes: <pre><code>Apptainer&gt; figlet test\n _            _\n| |_ ___  ___| |_\n| __/ _ \\/ __| __|\n| ||  __/\\__ \\ |_\n \\__\\___||___/\\__|\n</code></pre></p> <p>During the lecture you have learned that apptainer takes over the user privileges of the user on the host. You can get user information with command like <code>whoami</code>, <code>id</code>, <code>groups</code> etc.</p> <p>Exercise: Run the <code>figlet</code> container interactively. Do you have the same user privileges as if you were on the host? How is that with <code>docker</code>?</p> Answer <p>A command like <code>whoami</code> will result in your username printed at stdout:</p> <pre><code>Apptainer&gt; whoami\nmyusername\nApptainer&gt; id\nuid=1030(myusername) gid=1031(myusername) groups=1031(myusername),1001(condausers)\nApptainer&gt; groups\nmyusername condausers\n</code></pre> <p>With apptainer, you have the same privileges inside the apptainer container as on the host. If you do this in the docker container (based on the same image), you\u2019ll get output like this:</p> <pre><code>root@a3d6e59dc19d:/# whoami\nroot\nroot@a3d6e59dc19d:/# groups\nroot\nroot@a3d6e59dc19d:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre>"},{"location":"course_material/day1/apptainer/#a-bioinformatics-example-extra","title":"A bioinformatics example (extra)","text":"<p>All bioconda packages also have a pre-built container. Have a look at the bioconda website, and search for <code>fastqc</code>. In the search results, click on the appropriate record (i.e. package \u2018fastqc\u2019). Now, scroll down and find the namespace and tag for the latest fastqc image. Now we can pull it with apptainer like this:</p> <pre><code>apptainer pull docker://quay.io/biocontainers/fastqc:0.11.9--hdfd78af_1\n</code></pre> <p>Let\u2019s test the image. Download some sample reads first:</p> <pre><code>mkdir reads\ncd reads\nwget https://introduction-containers.s3.eu-central-1.amazonaws.com/ecoli_reads.tar.gz\ntar -xzvf ecoli_reads.tar.gz\nrm ecoli_reads.tar.gz\n</code></pre> <p>Now you can simply run the image as an executable preceding the commands you would like to run within the container. E.g. running <code>fastqc</code> would look like:</p> <pre><code>cd\n./fastqc_0.11.9--hdfd78af_1.sif fastqc ./reads/ecoli_*.fastq.gz\n</code></pre> <p>This will result in <code>html</code> files in the directory <code>./reads</code>. These are quality reports for the sequence reads. If you\u2019d like to view them, you can download them with <code>scp</code> or e.g. FileZilla, and view them with your local browser.</p>"},{"location":"course_material/day1/dockerfiles/","title":"Working with dockerfiles","text":""},{"location":"course_material/day1/dockerfiles/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Build an image based on a dockerfile</li> <li>Use the basic dockerfile syntax</li> <li>Change the default command of an image and validate the change</li> <li>Map ports to a container to display interactive content through a browser</li> </ul>"},{"location":"course_material/day1/dockerfiles/#material","title":"Material","text":"<ul> <li>Official <code>Dockerfile</code> reference</li> <li>Ten simple rules for writing dockerfiles</li> </ul>"},{"location":"course_material/day1/dockerfiles/#exercises","title":"Exercises","text":"<p>To make your images shareable and adjustable, it\u2019s good practice to work with a <code>Dockerfile</code>. This is a script with a set of instructions to build your image from an existing image.</p>"},{"location":"course_material/day1/dockerfiles/#basic-dockerfile","title":"Basic <code>Dockerfile</code>","text":"<p>You can generate an image from a <code>Dockerfile</code> using the command <code>docker build</code>. A <code>Dockerfile</code> has its own syntax for giving instructions. Luckily, they are rather simple. The script always contains a line starting with <code>FROM</code> that takes the image name from which the new image will be built. After that you usually want to run some commands to e.g. configure and/or install software. The instruction to run these commands during building starts with <code>RUN</code>.  In our <code>figlet</code> example that would be:</p> <pre><code>FROM ubuntu:jammy-20230308\nRUN apt-get update\nRUN apt-get install figlet\n</code></pre> <p>On writing reproducible <code>Dockerfiles</code></p> <p>At the <code>FROM</code> statement in the above <code>Dockerfile</code> you see that we have added a specific tag to the image (i.e. <code>jammy-20230308</code>). We could also have written:</p> <pre><code>FROM ubuntu\nRUN apt-get update\nRUN apt-get install figlet\n</code></pre> <p>This will automatically pull the image with the tag <code>latest</code>. However, if the maintainer of the <code>ubuntu</code> images decides to tag another <code>ubuntu</code> version as <code>latest</code>, rebuilding with the above <code>Dockerfile</code> will not give you the same result. Therefore it\u2019s always good practice to add the (stable) tag to the image in a <code>Dockerfile</code>. More rules on making your <code>Dockerfiles</code> more reproducible here.</p> <p>Exercise: Create a file on your computer called <code>Dockerfile</code>, and paste the above instruction lines in that file. Make the directory containing the <code>Dockerfile</code> your current directory. Build a new image based on that <code>Dockerfile</code> with:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build .\n</code></pre> <pre><code>docker build --platform amd64 .\n</code></pre> <p>If using an Apple M1 chip (newer Macs)</p> <p>If you are using a computer with an Apple M1 chip, you have the less common ARM system architecture, which can limit transferability of images to (more common) <code>x86_64/AMD64</code> machines. When building images on a Mac with an M1 chip (especially if you have sharing in mind), it\u2019s best to specify the <code>--platform amd64</code> flag. </p> <p>The argument of <code>docker build</code></p> <p>The command <code>docker build</code> takes a directory as input (providing <code>.</code> means the current directory). This directory should contain the <code>Dockerfile</code>, but it can also contain more of the build context, e.g. (python, R, shell) scripts that are required to build the image.</p> <p>What has happened? What is the name of the build image?</p> Answer <p>A new image was created based on the <code>Dockerfile</code>. You can check it with: <code>docker image ls</code>, which gives something like:</p> <pre><code>REPOSITORY                        TAG       IMAGE ID       CREATED             SIZE\n&lt;none&gt;                            &lt;none&gt;    92c980b09aad   7 seconds ago       101MB\nubuntu-figlet                     latest    e08b999c7978   About an hour ago   101MB\nubuntu                            latest    f63181f19b2f   30 hours ago        72.9MB\n</code></pre> <p>It has created an image without a name or tag. That\u2019s a bit inconvenient.</p> <p>Exercise: Build a new image with a specific name. You can do that with adding the option <code>-t</code> to <code>docker build</code>. Before that, remove the nameless image.</p> <p>Hint</p> <p>An image without a name is usually a \u201cdangling image\u201d. You can remove those with <code>docker image prune</code>.</p> Answer <p>Remove the nameless image with <code>docker image prune</code>.</p> <p>After that, rebuild an image with a name:</p> x86_64 / AMD64ARM (MacOS M1 chip) <pre><code>docker build -t ubuntu-figlet:v2 .\n</code></pre> <pre><code>docker build --platform amd64 -t ubuntu-figlet:v2 .\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#using-cmd","title":"Using <code>CMD</code>","text":"<p>As you might remember the second positional argument of <code>docker run</code> is a command (i.e. <code>docker run IMAGE [CMD]</code>). If you leave it empty, it uses the default command. You can change the default command in the <code>Dockerfile</code> with an instruction starting with <code>CMD</code>. For example:</p> <pre><code>FROM ubuntu:jammy-20230308\nRUN apt-get update\nRUN apt-get install figlet\nCMD figlet My image works!\n</code></pre> <p>Exercise: Build a new image based on the above <code>Dockerfile</code>. Can you validate the change using <code>docker image inspect</code>? Can you overwrite this default with <code>docker run</code>?</p> Answer <p>Copy the new line to your <code>Dockerfile</code>, and build the new image like this:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t ubuntu-figlet:v3 .\n</code></pre> <pre><code>docker build --platform amd64 -t ubuntu-figlet:v3 .\n</code></pre> <p>The command <code>docker inspect ubuntu-figlet:v3</code> will give:</p> <pre><code>\"Cmd\": [\n    \"/bin/sh\",\n    \"-c\",\n    \"figlet My image works!\"\n]\n</code></pre> <p>So the default command (<code>/bin/bash</code>) has changed to <code>figlet My image works!</code></p> <p>Running the image (with clean-up (<code>--rm</code>)):</p> <pre><code>docker run --rm ubuntu-figlet:v3\n</code></pre> <p>Will result in:</p> <pre><code>__  __         _                                                 _        _\n|  \\/  |_   _  (_)_ __ ___   __ _  __ _  ___  __      _____  _ __| | _____| |\n| |\\/| | | | | | | '_ ` _ \\ / _` |/ _` |/ _ \\ \\ \\ /\\ / / _ \\| '__| |/ / __| |\n| |  | | |_| | | | | | | | | (_| | (_| |  __/  \\ V  V / (_) | |  |   &lt;\\__ \\_|\n|_|  |_|\\__, | |_|_| |_| |_|\\__,_|\\__, |\\___|   \\_/\\_/ \\___/|_|  |_|\\_\\___(_)\n       |___/                     |___/\n</code></pre> <p>And of course you can overwrite the default command:</p> <pre><code>docker run --rm ubuntu-figlet:v3 figlet another text\n</code></pre> <p>Resulting in:</p> <pre><code>_   _                 _            _\n__ _ _ __   ___ | |_| |__   ___ _ __  | |_ _____  _| |_\n/ _` | '_ \\ / _ \\| __| '_ \\ / _ \\ '__| | __/ _ \\ \\/ / __|\n| (_| | | | | (_) | |_| | | |  __/ |    | ||  __/&gt;  &lt;| |_\n\\__,_|_| |_|\\___/ \\__|_| |_|\\___|_|     \\__\\___/_/\\_\\\\__|\n</code></pre> <p>Two flavours of <code>CMD</code></p> <p>You have seen in the output of <code>docker inspect</code> that docker translates the command (i.e. <code>figlet \"my image works!\"</code>) into this: <code>[\"/bin/sh\", \"-c\", \"figlet 'My image works!'\"]</code>. The notation we used in the <code>Dockerfile</code> is the shell notation while the notation with the square brackets (<code>[]</code>) is the exec-notation. You can use both notations in your <code>Dockerfile</code>. Altough the shell notation is more readable, the exec notation is directly used by the image, and therefore less ambiguous.</p> <p>A <code>Dockerfile</code> with shell notation:</p> <pre><code>FROM ubuntu:jammy-20230308\nRUN apt-get update\nRUN apt-get install figlet\nCMD figlet My image works!\n</code></pre> <p>A <code>Dockerfile</code> with exec notation:</p> <pre><code>FROM ubuntu:jammy-20230308\nRUN apt-get update\nRUN apt-get install figlet\nCMD [\"/bin/sh\", \"-c\", \"figlet My image works!\"]\n</code></pre> <p>Exercise: Now push our created image (with a version tag) to docker hub. We will use it later for the <code>apptainer</code> exercises.</p> Answer <pre><code>docker tag ubuntu-figlet:v3 [USER NAME]/ubuntu-figlet:v3\ndocker push [USER NAME]/ubuntu-figlet:v3\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#build-an-image-for-your-own-script","title":"Build an image for your own script","text":"<p>Often containers are built for a specific purpose. For example, you can use a container to ship all dependencies together with your developed set of scripts/programs. For that you will need to add your scripts to the container. That is quite easily done with the instruction <code>COPY</code>. However, in order to make your container more user-friendly, there are several additional instructions that can come in useful. We will treat the most frequently used ones below. Depending on your preference, either choose R or Python below. </p> <p>In the exercises will use a script called <code>test_deseq2.R</code>. This script will:</p> <ul> <li>Load the <code>DESeq2</code> and <code>optparse</code> packages</li> <li>Load some additional packages to test their installations. We will use those packages later on in the course.</li> <li>Create and parse an option called <code>--rows</code> with <code>optparse</code></li> <li>Create a dummy count matrix</li> <li>Run DESeq2 on the dummy count matrix</li> <li>Print the results to stdout</li> </ul> <p>You can download it here, or copy-paste it:</p> test_deseq2.R<pre><code>#!/usr/bin/env Rscript\n\n# load packages required for this script\nwrite(\"Loading packages required for this script\", stderr())\nsuppressPackageStartupMessages({\n    library(DESeq2)\n    library(optparse)\n})\n\n# load dependency packages for testing installations\nwrite(\"Loading dependency packages for testing installations\", stderr())\nsuppressPackageStartupMessages({\n    library(apeglm)\n    library(IHW)\n    library(limma)\n    library(data.table)\n    library(ggplot2)\n    library(ggrepel)\n    library(pheatmap)\n    library(RColorBrewer)\n    library(scales)\n    library(stringr)\n})\n\n# parse options with optparse\noption_list &lt;- list(\n    make_option(c(\"--rows\"),\n        type = \"integer\",\n        help = \"Number of rows in dummy matrix [default = %default]\",\n        default = 100\n    )\n)\n\nopt_parser &lt;- OptionParser(\n    option_list = option_list,\n    description = \"Runs DESeq2 on dummy data\"\n)\nopt &lt;- parse_args(opt_parser)\n\n# create a random dummy count matrix\ncnts &lt;- matrix(rnbinom(n = opt$row * 10, mu = 100, size = 1 / 0.5), ncol = 10)\ncond &lt;- factor(rep(1:2, each = 5))\n\n# object construction\ndds &lt;- DESeqDataSetFromMatrix(cnts, DataFrame(cond), ~cond)\n\n# standard analysis\ndds &lt;- DESeq(dds)\nres &lt;- results(dds)\n\n# print results to stdout\nprint(res)\n</code></pre> <p>After you have downloaded it, make sure to set the permissions to executable:</p> <p><pre><code>chmod +x test_deseq2.R\n</code></pre> It is a relatively simple script that runs DESeq2 on a dummy dataset. An example for execution would be:</p> <pre><code>./test_deseq2.R --rows 100\n</code></pre> <p>Here, <code>--rows</code> is a optional arguments that specifies the number of rows generated in the input count matrix. When running the script, it will return a bunch of messages and at the end an overview of differential gene expression analysis results:</p> <pre><code>     baseMean log2FoldChange     lfcSE         stat    pvalue      padj\n    &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\n1     66.1249       0.281757  0.727668     0.387206  0.698604  0.989804\n2     76.9682       0.305763  0.619209     0.493796  0.621451  0.989804\n3     64.7843      -0.694525  0.479445    -1.448603  0.147448  0.931561\n4    123.0252       0.631247  0.688564     0.916758  0.359269  0.931561\n5     93.2002      -0.453430  0.686043    -0.660936  0.508653  0.941951\n...       ...            ...       ...          ...       ...       ...\n96    64.0177    0.757585137  0.682683  1.109718054  0.267121  0.931561\n97   114.3689   -0.580010850  0.640313 -0.905823841  0.365029  0.931561\n98    79.9620    0.000100617  0.612442  0.000164288  0.999869  0.999869\n99    92.6614    0.563514308  0.716109  0.786910869  0.431334  0.939106\n100   96.4410   -0.155268696  0.534400 -0.290547708  0.771397  0.989804\n</code></pre> <p>From the script you can see it has <code>DESeq2</code> and <code>optparse</code> as dependencies. If we want to run the script inside a container, we would have to install them. We do this in the <code>Dockerfile</code> below. We give it the following instructions:</p> <ul> <li>use the r2u base image version jammy</li> <li>install the package <code>DESeq2</code>, <code>optparse</code> and some additional packages we will need later on. We perform the installations with <code>install2.r</code>, which is a helper command that is present inside most rocker images. More info here. </li> <li>copy the script <code>test_deseq2.R</code> to <code>/opt</code> inside the container:</li> </ul> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nCOPY test_deseq2.R /opt \n</code></pre> <p>Note</p> <p>In order to use <code>COPY</code>, the file that needs to be copied needs to be in the same directory as the <code>Dockerfile</code> or one of its subdirectories.</p> <p>R image stack</p> <p>The most used R image stack is from the rocker project. It contains many different base images (e.g. with shiny, Rstudio, tidyverse etc.). It depends on the type of image whether installations with <code>apt-get</code> or <code>install2.r</code> are possible. To understand more about how to install R packages in different containers, check it this cheat sheet, or visit rocker-project.org.</p> <p>Exercise: Download the <code>test_deseq2.R</code> and build the image with <code>docker build</code>. Name the image <code>deseq2</code>. After that, start an interactive session and execute the script inside the container. </p> <p>Hint</p> <p>Make an interactive session with the options <code>-i</code> and <code>-t</code> and use <code>/bin/bash</code> as the command. </p> Answer <p>Build the container:</p> x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t deseq2 .\n</code></pre> <pre><code>docker build --platform amd64 -t deseq2 .\n</code></pre> <p>Run the container:</p> <pre><code>docker run -it --rm deseq2 /bin/bash\n</code></pre> <p>Inside the container we look up the script:</p> <pre><code>cd /opt\nls\n</code></pre> <p>This should return <code>test_deseq2.R</code>. </p> <p>Now you can execute it from inside the container:</p> <pre><code>./test_deseq2.R --rows 100\n</code></pre> <p>That\u2019s kind of nice. We can ship our R script inside our container. However, we don\u2019t want to run it interactively every time. So let\u2019s make some changes to make it easy to run it as an executable. For example, we can add <code>/opt</code> to the global <code>$PATH</code> variable with <code>ENV</code>. </p> <p>The <code>$PATH</code> variable</p> <p>The path variable is a special variable that consists of a list of path seperated by colons (<code>:</code>). These paths are searched if you are trying to run an executable. More info this topic at e.g. wikipedia. </p> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nCOPY test_deseq2.R /opt \n\nENV PATH=/opt:$PATH\n</code></pre> <p>Note</p> <p>The <code>ENV</code> instruction can be used to set any variable. </p> <p>Exercise: Rebuild the image and start an interactive bash session inside the new image. Is the path variable updated? (i.e. can we execute <code>test_deseq2.R</code> from anywhere?)</p> Answer <p>After re-building we start an interactive session:</p> <pre><code>docker run -it --rm deseq2 /bin/bash\n</code></pre> <p>The path is upated, <code>/opt</code> is appended to the beginning of the variable:</p> <pre><code>echo $PATH\n</code></pre> <p>returns:</p> <pre><code>/opt:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n</code></pre> <p>Now you can try to execute it from the root directory (or any other):</p> <pre><code>test_deseq2.R\n</code></pre> <p>Instead of starting an interactive session with <code>/bin/bash</code> we can now more easily run the script non-interactively:</p> <pre><code>docker run --rm deseq2 test_deseq2.R --rows 100\n</code></pre> <p>Now it will directly print the output of <code>test_deseq2.R</code> to stdout. </p> <p>In the case you want to pack your script inside a container, you are building a container specifically for your script, meaning you almost want the container to behave as the program itself. In order to do that, you can use <code>ENTRYPOINT</code>. <code>ENTRYPOINT</code> is similar to <code>CMD</code>, but has two important differences:</p> <ul> <li><code>ENTRYPOINT</code> can not be overwritten by the positional arguments (i.e. <code>docker run image [CMD]</code>), but has to be overwritten by <code>--entrypoint</code>. </li> <li>The positional arguments (or <code>CMD</code>) are pasted to the <code>ENTRYPOINT</code> command. This means that you can use <code>ENTRYPOINT</code> as the executable and the positional arguments (or <code>CMD</code>) as the options. </li> </ul> <p>Let\u2019s try it out:</p> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nCOPY test_deseq2.R /opt \n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"test_deseq2.R\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--rows\", \"100\"]\n</code></pre> <p>Exercise: Re-build, and run the container non-interactively without any positional arguments. After that, try to pass a different number of rows to <code>--rows</code>. How do the commands look?</p> Answer <p>Just running the container non-interactively would be:</p> <pre><code>docker run --rm deseq2\n</code></pre> <p>Passing a different argument (i.e. overwriting <code>CMD</code>) would be:</p> <pre><code>docker run --rm deseq2 --rows 200\n</code></pre> <p>Here, the container behaves as the executable itself to which you can pass arguments. </p> <p>Most containerized applications need multiple build steps. Often, you want to perform these steps and executions in a specific directory. Therefore, it can be in convenient to specify a working directory. You can do that with <code>WORKDIR</code>. This instruction will set the default directory for all other instructions (like <code>RUN</code>, <code>COPY</code> etc.). It will also change the directory in which you will land if you run the container interactively.</p> <pre><code>FROM rocker/r2u:jammy\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nWORKDIR /opt\n\nCOPY test_deseq2.R .\n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"test_deseq2.R\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--rows\", \"100\"]\n</code></pre> <p>Exercise: build the image, and start the container interactively. Has the default directory changed? After that, push the image to dockerhub, so we can use it later with the apptainer exercises.</p> <p>Note</p> <p>You can overwrite <code>ENTRYPOINT</code> with <code>--entrypoint</code> as an argument to <code>docker run</code>. </p> Answer <p>Running the container interactively would be:</p> <pre><code>docker run -it --rm --entrypoint /bin/bash deseq2\n</code></pre> <p>Which should result in a terminal looking something like this:</p> <pre><code>root@9a27da455fb1:/opt#\n</code></pre> <p>Meaning that indeed the default directory has changed to <code>/opt</code></p> <p>Pushing it to dockerhub: </p> <pre><code>docker tag deseq2 [USER NAME]/deseq2:v1\ndocker push [USER NAME]/deseq2:v1\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#get-information-on-your-image-with-docker-inspect","title":"Get information on your image with <code>docker inspect</code>","text":"<p>We have used <code>docker inspect</code> already in the previous chapter to find the default <code>Cmd</code> of the ubuntu image. However we can get more info on the image: e.g. the entrypoint, environmental variables, cmd, workingdir etc., you can use the <code>Config</code> record from the output of <code>docker inspect</code>. For our image this looks like:</p> <pre><code>\"Config\": {\n    \"Hostname\": \"\",\n    \"Domainname\": \"\",\n    \"User\": \"\",\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"AttachStderr\": false,\n    \"Tty\": false,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Env\": [\n        \"PATH=/opt:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n        \"LC_ALL=en_US.UTF-8\",\n        \"LANG=en_US.UTF-8\",\n        \"DEBIAN_FRONTEND=noninteractive\",\n        \"TZ=UTC\"\n    ],\n    \"Cmd\": [\n        \"--rows\",\n        \"100\"\n    ],\n    \"ArgsEscaped\": true,\n    \"Image\": \"\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"/opt\",\n    \"Entrypoint\": [\n        \"test_deseq2.R\"\n    ],\n    \"OnBuild\": null,\n    \"Labels\": {\n        \"maintainer\": \"Dirk Eddelbuettel &lt;edd@debian.org&gt;\",\n        \"org.label-schema.license\": \"GPL-2.0\",\n        \"org.label-schema.vcs-url\": \"https://github.com/rocker-org/\",\n        \"org.label-schema.vendor\": \"Rocker Project\"\n    }\n}\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#adding-metadata-to-your-image","title":"Adding metadata to your image","text":"<p>You can annotate your <code>Dockerfile</code> and the image by using the instruction <code>LABEL</code>. You can give it any key and value with <code>&lt;key&gt;=&lt;value&gt;</code>. However, it is recommended to use the Open Container Initiative (OCI) keys.</p> <p>Exercise: Annotate our <code>Dockerfile</code> with the OCI keys on the creation date, author and description. After that, check whether this has been passed to the actual image with <code>docker inspect</code>. </p> <p>Note</p> <p>You can type <code>LABEL</code> for each key-value pair, but you can also have it on one line by seperating the key-value pairs by a space, e.g.:</p> <pre><code>LABEL keyx=\"valuex\" keyy=\"valuey\"\n</code></pre> Answer <p>The <code>Dockerfile</code> would look like:</p> <pre><code>FROM rocker/r2u:jammy\n\nLABEL org.opencontainers.image.created=\"2023-04-12\" \\\n        org.opencontainers.image.authors=\"Geert van Geest\" \\\n        org.opencontainers.image.description=\"Container with DESeq2 and friends\"\n\nRUN install2.r \\\n    DESeq2 \\\n    optparse \\\n    apeglm \\\n    IHW \\\n    limma \\\n    data.table \\\n    ggrepel \\\n    pheatmap \\\n    stringr\n\nWORKDIR /opt\n\nCOPY test_deseq2.R .\n\nENV PATH=/opt:$PATH\n\n# note that if you want to be able to combine the two\n# both ENTRYPOINT and CMD need to written in the exec form\nENTRYPOINT [\"test_deseq2.R\"]\n\n# default option (if positional arguments are not specified)\nCMD [\"--rows\", \"100\"]\n</code></pre> <p>The <code>Config</code> record in the output of <code>docker inspect</code> was updated with:</p> <pre><code>    \"Labels\": {\n        \"org.opencontainers.image.authors\": \"Geert van Geest\",\n        \"org.opencontainers.image.created\": \"2023-04-12\",\n        \"org.opencontainers.image.description\": \"Container with DESeq2 and friends\",\n        \"org.opencontainers.image.licenses\": \"GPL-2.0-or-later\",\n        \"org.opencontainers.image.source\": \"https://github.com/rocker-org/rocker\",\n        \"org.opencontainers.image.vendor\": \"Rocker Project\"\n    }\n</code></pre>"},{"location":"course_material/day1/dockerfiles/#building-an-image-with-a-browser-interface","title":"Building an image with a browser interface","text":"<p>In this exercise, we will use a different base image (<code>rocker/rstudio:4</code>), and we\u2019ll install the same packages. Rstudio server is a nice browser interface that you can use for a.o. programming in R. With the image we are creating we will be able to run Rstudio server inside a container.  Check out the <code>Dockerfile</code>:</p> <pre><code>FROM rocker/rstudio:4\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y libz-dev\n\nRUN install2.r \\\n    optparse \\\n    BiocManager\n\nRUN R -q -e 'BiocManager::install(\"biomaRt\")'\n</code></pre> <p>This will create an image from the existing <code>rstudio</code> image. It will also install <code>libz-dev</code> with <code>apt-get</code>, <code>BiocManager</code> with <code>install2.r</code> and <code>DESeq2</code> with an R command. Despite we\u2019re installing the same packages, the installation steps need to be different from the <code>r-base</code> image. This is because in the <code>rocker/rstudio</code> images R is installed from source, and therefore you can\u2019t install packages with <code>apt-get</code>. More information on how to install R packages in R containers in this cheat sheet, or visit rocker-project.org.</p> <p>Installation will take a while</p> <p>The installation of CRAN packages will go relatively quickly, because can use the binary packages supplied by Posit Public Package Manager. However, the installation of Bioconductor packages will take a while, because they need to be installed from source. </p> <p>If you don\u2019t have time, you can skip the DESeq2 installation by removing the last line of the <code>Dockerfile</code>.</p> <p>Exercise: Build an image based on this <code>Dockerfile</code> and give it a meaningful name.</p> Answer x86_64 / AMD64ARM64 (MacOS M1 chip) <pre><code>docker build -t rstudio-server .\n</code></pre> <pre><code>docker build --platform amd64 -t rstudio-server .\n</code></pre> <p>You can now run a container from the image. However, you will have to tell docker where to publish port 8787 from the docker container with <code>-p [HOSTPORT:CONTAINERPORT]</code>. We choose to publish it to the same port number:</p> <pre><code>docker run --rm -it -p 8787:8787 rstudio-server\n</code></pre> <p>Networking</p> <p>More info on docker container networking here</p> <p>By running the above command, a container will be started exposing rstudio server at port 8787 at localhost. You can approach the instance of Rstudio server by typing <code>localhost:8787</code> in your browser. You will be asked for a password. You can find this password in the terminal from which you have started the container.</p> <p>We can make this even more interesting by mounting a local directory to the container running the Rstudio image:</p> <pre><code>docker run \\\n-it \\\n--rm \\\n-p 8787:8787 \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/home/rstudio/working_dir \\\nrstudio-server\n</code></pre> <p>By doing this you have a completely isolated and shareable R environment running Rstudio server, but with your local files available to it. Pretty neat right? </p>"},{"location":"course_material/day1/introduction_containers/","title":"Introduction to containers","text":""},{"location":"course_material/day1/introduction_containers/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Discriminate between an image and a container</li> <li>Run a docker container from dockerhub interactively</li> <li>Validate the available containers and their status</li> </ul>"},{"location":"course_material/day1/introduction_containers/#material","title":"Material","text":"<p>General introduction:</p> <p> Download the presentation</p> <p>Introduction to containers:</p> <p> Download the presentation</p>"},{"location":"course_material/day1/introduction_containers/#exercises","title":"Exercises","text":"<p>We recommend using a code editor like VScode or Sublime text. If you don\u2019t know which one to chose, take VScode as we can provide most support for this editor. </p> <p>If working on Windows</p> <p>If you are working on Windows, it is easiest to work with WSL2. With VScode use the WSL extension. Make sure you install the latest versions before you install docker. In principle, you can also use a native shell like PowerShell, but this might result into some issues with bind mounting directories.</p> <p>Work in projects</p> <p>We recommend to work in a project folder. This will make it easier to find your files and to share them with others. You can create a project folder anywhere on your computer. For example, you can create a folder <code>projects</code> in your home directory and then create a subfolder <code>docker-snakemake-course</code> in it. You can then open this folder in VScode.</p> <p>Let\u2019s create our first container from an existing image. We do this with the image <code>ubuntu</code>, generating an environment with a minimal installation of ubuntu.  </p> <pre><code>docker run -it ubuntu\n</code></pre> <p>This will give you an interactive shell into the created container (this interactivity was invoked by the options <code>-i</code> and <code>-t</code>) .</p> <p>Exercise: Check out the operating system of the container by typing <code>cat /etc/os-release</code> in the container\u2019s shell. Are we really in an ubuntu environment?</p> Answer <p>Yes:</p> <pre><code>root@27f7d11608de:/# cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"20.04.1 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.1 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n</code></pre> <p>Where does the image come from?</p> <p>If the image <code>ubuntu</code> was not on your computer yet, <code>docker</code> will search and try to get them from dockerhub, and download it.</p> <p>Exercise: Run the command <code>whoami</code> in the docker container. Who are you?</p> Answer <p>The command <code>whoami</code> returns the current user. In the container <code>whoami</code> will return <code>root</code>. This means you are the <code>root</code> user i.e. within the container you are admin and can basically change anything.  </p> <p>Check out the container panel at the Docker dashboard (the Docker gui) or open another host terminal and type:</p> <pre><code>docker container ls -a\n</code></pre> <p>Exercise: What is the container status?</p> Answer <p>In Docker dashboard you can see that the shell is running:</p> <p> </p> <p>The output of <code>docker container ls -a</code> is:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND       CREATED         STATUS         PORTS     NAMES\n27f7d11608de   ubuntu    \"/bin/bash\"   7 minutes ago   Up 6 minutes             great_moser\n</code></pre> <p>Also showing you that the <code>STATUS</code> is <code>Up</code>.</p> <p>Now let\u2019s install some software in our <code>ubuntu</code> environment. We\u2019ll install some simple software called <code>figlet</code>. Type into the container shell:</p> <pre><code>apt-get update\napt-get install figlet\n</code></pre> <p>This will give some warnings</p> <p>This installation will give some warnings. It\u2019s safe to ignore them.</p> <p>Now let\u2019s try it out. Type into the container shell:</p> <pre><code>figlet 'SIB courses are great!'\n</code></pre> <p>Now you have installed and used software <code>figlet</code> in an <code>ubuntu</code> environment (almost) completely separated from your host computer. This already gives you an idea of the power of containerization.</p> <p>Exit the shell by typing <code>exit</code>. Check out the container panel of Docker dashboard or type:</p> <pre><code>docker container ls -a\n</code></pre> <p>Exercise: What is the container status?</p> Answer <p><code>docker container ls -a</code> gives:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND       CREATED          STATUS                     PORTS     NAMES\n27f7d11608de   ubuntu    \"/bin/bash\"   15 minutes ago   Exited (0) 8 seconds ago             great_moser\n</code></pre> <p>Showing that the container has exited, meaning it\u2019s not running.</p>"},{"location":"course_material/day1/managing_docker/","title":"Managing containers and images","text":""},{"location":"course_material/day1/managing_docker/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Explain the concept of layers in the context of docker containers and images</li> <li>Use the command line to restart and re-attach to an exited container</li> <li>Create a new image with <code>docker commit</code></li> <li>List locally available images with <code>docker image ls</code></li> <li>Run a command inside a container non-interactively</li> <li>Use <code>docker image inspect</code> to get more information on an image</li> <li>Use the command line to prune dangling images and stopped containers</li> <li>Rename and tag a docker image</li> <li>Push a newly created image to dockerhub</li> <li>Use the option <code>--mount</code> to bind mount a host directory to a container</li> </ul>"},{"location":"course_material/day1/managing_docker/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Overview of how docker works</li> <li>More on bind mounts</li> <li>Docker volumes in general</li> </ul>"},{"location":"course_material/day1/managing_docker/#exercises","title":"Exercises","text":""},{"location":"course_material/day1/managing_docker/#restarting-an-exited-container","title":"Restarting an exited container","text":"<p>If you would like to go back to your container with the <code>figlet</code> installation, you could try to run again:</p> <pre><code>docker run -it ubuntu\n</code></pre> <p>Exercise: Run the above command. Is your <code>figlet</code> installation still there? Why?</p> <p>Hint</p> <p>Check the status of your containers:</p> <pre><code>docker container ls -a\n</code></pre> Answer <p>No, the installation is gone. Another container was created from the same ubuntu image, without the <code>figlet</code> installation. Running the command <code>docker container ls -a</code> results in:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND       CREATED              STATUS                     PORTS     NAMES\n8d7c4c611b70   ubuntu    \"/bin/bash\"   About a minute ago   Up About a minute                    kind_mendel\n27f7d11608de   ubuntu    \"/bin/bash\"   27 minutes ago       Exited (0) 2 minutes ago             great_moser\n</code></pre> <p>In this case the container <code>great_moser</code> contains the <code>figlet</code> installation. But we have exited that container. We created a new container (<code>kind_mendel</code> in this case) with a fresh environment created from the original <code>ubuntu</code> image.</p> <p>To restart your first created container, you\u2019ll have to look up its name. You can find it in the Docker dashboard, or with <code>docker container ls -a</code>.</p> <p>Container names</p> <p>The container name is the funny combination of two words separated by <code>_</code>, e.g.: <code>nifty_sinoussi</code>. Alternatively you can use the container ID (the first column of the output of <code>docker container ls</code>)</p> <p>To restart a container you can use:</p> <pre><code>docker start [CONTAINER NAME]\n</code></pre> <p>And after that to re-attach to the shell:</p> <pre><code>docker attach [CONTAINER NAME]\n</code></pre> <p>And you\u2019re back in the container shell.</p> <p>Exercise: Run the <code>docker start</code> and <code>docker attach</code> commands for the container that is supposed to contain the <code>figlet</code> installation. Is the installation of <code>figlet</code> still there?</p> Answer <p>yes:</p> <pre><code>figlet 'try some more text!'\n</code></pre> <p>Should give you output.</p> <p><code>docker attach</code> and <code>docker exec</code></p> <p>In addition to <code>docker attach</code>, you can also \u201cre-attach\u201d a container with <code>docker exec</code>. However, these two are quite different. While <code>docker attach</code> gets you back to your stopped shell process, <code>docker exec</code> creates a new one (more information on stackoverflow). The command <code>docker exec</code> enables you therefore to have multiple shells open in the same container. That can be convenient if you have one shell open with a program running in the foreground, and another one for e.g. monitoring. An example for using <code>docker exec</code> on a running container:</p> <pre><code>docker exec -it [CONTAINER NAME] /bin/bash\n</code></pre> <p>Note that  <code>docker exec</code> requires a CMD, it doesn\u2019t use the default.</p>"},{"location":"course_material/day1/managing_docker/#creating-a-new-image","title":"Creating a new image","text":"<p>You can store your changes and create a new image based on the <code>ubuntu</code> image like this:</p> <pre><code>docker commit [CONTAINER NAME] ubuntu-figlet\n</code></pre> <p>Exercise: Run the above command with the name of the container containing the <code>figlet</code> installation. Check out <code>docker image ls</code>. What have we just created?</p> Answer <p>A new image called <code>ubuntu-figlet</code> based on the status of the container. The output of <code>docker image ls</code> should look like:</p> <pre><code>REPOSITORY                        TAG       IMAGE ID       CREATED         SIZE\nubuntu-figlet                     latest    e08b999c7978   4 seconds ago   101MB\nubuntu                            latest    f63181f19b2f   29 hours ago    72.9MB\n</code></pre> <p>Now you can generate a new container based on the new image:</p> <pre><code>docker run -it ubuntu-figlet\n</code></pre> <p>Exercise: Run the above command. Is the <code>figlet</code> installation in the created container?</p> Answer <p>yes</p>"},{"location":"course_material/day1/managing_docker/#commands","title":"Commands","text":"<p>The second positional argument of <code>docker run</code> can be a command followed by its arguments. So, we could run a container non-interactively (without <code>-it</code>), and just let it run a single command:</p> <pre><code>docker run ubuntu-figlet figlet 'non-interactive run'\n</code></pre> <p>Resulting in just the output of the <code>figlet</code> command.</p> <p>In the previous exercises we have run containers without a command as positional argument. This doesn\u2019t mean that no command has been run, because the container would do nothing without a command. The default command is stored in the image, and you can find it by <code>docker image inspect [IMAGE NAME]</code>.  </p> <p>Exercise: Have a look at the output of <code>docker image inspect</code>, particularly at <code>\"Config\"</code> (ignore <code>\"ContainerConfig\"</code> for now). What is the default command (<code>CMD</code>) of the ubuntu image?</p> Answer <p>Running <code>docker image inspect ubuntu</code> gives (amongst other information):</p> <pre><code>\"Cmd\": [\n    \"/bin/bash\"\n],\n</code></pre> <p>In the case of the ubuntu the default command is <code>bash</code>, returning a shell in <code>bash</code> (i.e. Bourne again shell). Adding the options <code>-i</code> and <code>-t</code> (<code>-it</code>) to your <code>docker run</code> command will therefore result in an interactive <code>bash</code> shell. You can modify this default behaviour. More on that later, when we will work on Dockerfiles.</p> <p>The difference between <code>Config</code> and <code>ContainerConfig</code></p> <p>The configuration at <code>Config</code> represents the image, the configuration at <code>ContainerConfig</code> the last step during the build of the image, i.e. the last layer. More info e.g. at this post at stackoverflow.</p>"},{"location":"course_material/day1/managing_docker/#removing-containers","title":"Removing containers","text":"<p>In the meantime, with every call of <code>docker run</code> we have created a new container (check your containers with <code>docker container ls -a</code>). You probably don\u2019t want to remove those one-by-one. These two commands are very useful to clean up your Docker cache:</p> <ul> <li><code>docker container prune</code>: removes stopped containers</li> <li><code>docker image prune</code>: removes dangling images (i.e. images without a name)</li> </ul> <p>So, remove your stopped containers with:</p> <pre><code>docker container prune\n</code></pre> <p>Unless you\u2019re developing further on a container, or you\u2019re using it for an analysis, you probably want to get rid of it once you have exited the container. You can do this with adding <code>--rm</code> to your <code>docker run</code> command, e.g.:</p> <pre><code>docker run --rm ubuntu-figlet figlet 'non-interactive run'\n</code></pre>"},{"location":"course_material/day1/managing_docker/#pushing-to-dockerhub","title":"Pushing to dockerhub","text":"<p>Now that we have created our first own docker image, we can store it and share it with the world on docker hub. Before we get there, we first have to (re)name and tag it.</p> <p>Before pushing an image to dockerhub, <code>docker</code> has to know to which user and which repository the image should be added. That information should be in the name of the image, like this: <code>user/imagename</code>. We can rename an image with <code>docker tag</code> (which is a bit of misleading name for the command). So we could push to dockerhub like this:</p> <pre><code>docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet\ndocker push [USER NAME]/ubuntu-figlet\n</code></pre> <p>If on Linux</p> <p>If you are on Linux and haven\u2019t connected to docker hub before, you will have login first. To do that, run:</p> <pre><code>docker login\n</code></pre> <p>How docker makes money</p> <p>All images pushed to dockerhub are open to the world. With a free account you can have one image on dockerhub that is private. Paid accounts can have more private images, and are therefore popular for commercial organisations. As an alternative to dockerhub, you can store images locally with <code>docker save</code>.</p> <p>We didn\u2019t specify the tag for our new image. That\u2019s why <code>docker tag</code> gave it the default tag called <code>latest</code>. Pushing an image without a tag will overwrite the current image with the tag <code>latest</code> (more on (not) using <code>latest</code> here). If you want to maintain multiple versions of your image, you will have to add a tag, and push the image with that tag to dockerhub:</p> <pre><code>docker tag ubuntu-figlet [USER NAME]/ubuntu-figlet:v1\ndocker push [USER NAME]/ubuntu-figlet:v1\n</code></pre>"},{"location":"course_material/day1/managing_docker/#mounting-a-directory","title":"Mounting a directory","text":"<p>For many analyses you do calculations with files or scripts that are on your host (local) computer. But how do you make them available to a docker container? You can do that in several ways, but here we will use bind-mount. You can bind-mount a directory with <code>-v</code> (<code>--volume</code>) or <code>--mount</code>. Most old-school <code>docker</code> users will use <code>-v</code>, but <code>--mount</code> syntax is easier to understand and now recommended, so we will use the latter here:</p> <pre><code>docker run \\\n--mount type=bind,source=/host/source/path,target=/path/in/container \\\n[IMAGE]\n</code></pre> <p>The target directory will be created if it does not yet exist. The source directory should exist.</p> <p>MobaXterm users</p> <p>You can specify your local path with the Windows syntax (e.g. <code>C:\\Users\\myusername</code>). However, you will have to use forward slashes (<code>/</code>) instead of backward slashes (<code>\\</code>). Therefore, mounting a directory would look like:</p> <pre><code>docker run \\\n--mount type=bind,source=C:/Users/myusername,target=/path/in/container \\\n[IMAGE]\n</code></pre> <p>Do not use autocompletion or variable substitution (e.g. <code>$PWD</code>) in MobaXterm, since these point to \u2018emulated\u2019 paths, and are not passed properly to the docker command.</p> <p>Using docker from Windows PowerShell</p> <p>Most of the syntax for <code>docker</code> is the same for both PowerShell and UNIX-based systems. However, there are some differences, e.g. in Windows, directories in file paths are separated by <code>\\</code> instead of <code>/</code>. Also, line breaks are not escaped by <code>\\</code> but by `.</p> <p>Exercise: Mount a host (local) directory to a target directory <code>/working_dir</code> in a container created from the <code>ubuntu-figlet</code> image and run it interactively. Check whether the target directory has been created.</p> Answer <p>e.g. on Mac OS this would be:</p> <pre><code>docker run \\\n-it \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/working_dir/ \\\nubuntu-figlet\n</code></pre> <p>This creates a directory called <code>working_dir</code> in the root directory (<code>/</code>):</p> <pre><code>root@8d80a8698865:/# ls\nbin   dev  home  lib32  libx32  mnt  proc  run   srv  tmp  var\nboot  etc  lib   lib64  media   opt  root  sbin  sys  usr  working_dir\n</code></pre> <p>This mounted directory is both available for the host (locally) and for the container. You can therefore e.g. copy files in there, and write output generated by the container.</p> <p>Exercise: Write the output of <code>figlet \"testing mounted dir\"</code> to a file in <code>/working_dir</code>. Check whether it is available on the host (locally) in the source directory.</p> <p>Hint</p> <p>You can write the output of <code>figlet</code> to a file like this: <pre><code>figlet 'some string' &gt; file.txt\n</code></pre></p> Answer <pre><code>root@8d80a8698865:/# figlet 'testing mounted dir' &gt; /working_dir/figlet_output.txt\n</code></pre> <p>This should create a file in both your host (local) source directory and the target directory in the container called <code>figlet_output.txt</code>.</p> <p>Using files on the host</p> <p>This of course also works the other way around. If you would have a file on the host with e.g. a text, you can copy it into your mounted directory, and it will be available to the container.</p>"},{"location":"course_material/day1/managing_docker/#managing-permissions-extra","title":"Managing permissions (extra)","text":"<p>Depending on your system, the user ID and group ID will be taken over from the user inside the container. If the user inside the container is root, this will be root. That\u2019s a bit inconvenient if you just want to run the container as a regular user (for example in certain circumstances your container could write in <code>/</code>). To do that, use the <code>-u</code> option, and specify the group ID and user ID like this:</p> <pre><code>docker run -u [uid]:[gid]\n</code></pre> <p>So, e.g.:</p> <pre><code>docker run \\\n-it \\\n-u 1000:1000 \\\n--mount type=bind,source=/Users/myusername/working_dir,target=/working_dir/ \\\nubuntu-figlet\n</code></pre> <p>If you want docker to take over your current uid and gid, you can use:</p> <pre><code>docker run -u \"$(id -u):$(id -g)\"\n</code></pre> <p>This behaviour is different on MacOS and MobaXterm</p> <p>On MacOS and in the local shell of MobaXterm the uid and gid are taken over from the user running the container (even if you set <code>-u</code> as 0:0), i.e. your current ID. More info on stackoverflow.</p> <p>Exercise: Start an interactive container based on the <code>ubuntu-figlet</code> image, bind-mount a local directory and take over your current <code>uid</code> and <code>gid</code>. Write the output of a <code>figlet</code> command to a file in the mounted directory. Who and which group owns the file inside the container? And outside the container? Answer the same question but now run the container without setting <code>-u</code>.</p> Answer LinuxMacOSMobaXterm <p>Running <code>ubuntu-figlet</code> interactively while taking over <code>uid</code> and <code>gid</code> and mounting my current directory:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data -u \"$(id -u):$(id -g)\" ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>I have no name!@e808d7c36e7c:/$ id\nuid=1000 gid=1000 groups=1000\n</code></pre> <p>So, I have taken over uid 1000 and gid 1000.</p> <pre><code>I have no name!@e808d7c36e7c:/$ cd /data\nI have no name!@e808d7c36e7c:/data$ figlet 'uid set' &gt; uid_set.txt\nI have no name!@e808d7c36e7c:/data$ ls -lh\n-rw-r--r-- 1 1000 1000 0 Mar  400 13:37 uid_set.txt\n</code></pre> <p>So the file belongs to user 1000, and group 1000.</p> <p>Outside container:</p> <pre><code>ubuntu@ip-172-31-33-21:~$ ls -lh\n-rw-r--r-- 1 ubuntu ubuntu 400 Mar  5 13:37 uid_set.txt\n</code></pre> <p>Which makes sense:</p> <pre><code>ubuntu@ip-172-31-33-21:~$ id\nuid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu)\n</code></pre> <p>Running <code>ubuntu-figlet</code> interactively without taking over <code>uid</code> and <code>gid</code>:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>root@fface8afb220:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre> <p>So, uid and gid are <code>root</code>.</p> <pre><code>root@fface8afb220:/# cd /data\nroot@fface8afb220:/data# figlet 'uid unset' &gt; uid_unset.txt\nroot@fface8afb220:/data# ls -lh\n-rw-r--r-- 1 1000 1000 400 Mar  5 13:37 uid_set.txt\n-rw-r--r-- 1 root root 400 Mar  5 13:40 uid_unset.txt\n</code></pre> <p>Outside container:</p> <pre><code>ubuntu@ip-172-31-33-21:~$ ls -lh\n-rw-r--r-- 1 ubuntu ubuntu 0 Mar  5 13:37 uid_set.txt\n-rw-r--r-- 1 root   root   0 Mar  5 13:40 uid_unset.txt\n</code></pre> <p>So, the uid and gid 0 (root:root) are taken over.</p> <p>Running <code>ubuntu-figlet</code> interactively while taking over <code>uid</code> and <code>gid</code> and mounting my current directory:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data -u \"$(id -u):$(id -g)\" ubuntu-figlet\n</code></pre> Inside container:</p> <p><pre><code>I have no name!@e808d7c36e7c:/$ id\nuid=503 gid=20(dialout) groups=20(dialout)\n</code></pre> So, the container has taken over uid 503 and group 20</p> <pre><code>I have no name!@e808d7c36e7c:/$ cd /data\nI have no name!@e808d7c36e7c:/data$ figlet 'uid set' &gt; uid_set.txt\nI have no name!@e808d7c36e7c:/data$ ls -lh\n-rw-r--r--  1 503 dialout    400 Mar  5 13:11 uid_set.txt\n</code></pre> <p>So the file belongs to user 503, and the group <code>dialout</code>.</p> <p>Outside container:</p> <pre><code>mac-34392:~ geertvangeest$ ls -lh\n-rw-r--r--   1 geertvangeest  staff     400B Mar  5 14:11 uid_set.txt\n</code></pre> <p>Which are the same as inside the container:</p> <pre><code>mac-34392:~ geertvangeest$ echo \"$(id -u):$(id -g)\"\n503:20\n</code></pre> <p>The <code>uid</code> 503 was nameless in the docker container. However the group 20 already existed in the ubuntu container, and was named <code>dialout</code>.</p> <p>Running <code>ubuntu-figlet</code> interactively without taking over <code>uid</code> and <code>gid</code>:</p> <p><pre><code>docker run -it --mount type=bind,source=$PWD,target=/data ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>root@fface8afb220:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre> <p>So, inside the container I am <code>root</code>. Creating new files will lead to ownership of <code>root</code> inside the container:</p> <pre><code>root@fface8afb220:/# cd /data\nroot@fface8afb220:/data# figlet 'uid unset' &gt; uid_unset.txt\nroot@fface8afb220:/data# ls -lh\n-rw-r--r--  1 503 dialout    400 Mar  5 13:11 uid_set.txt\n-rw-r--r--  1 root root    400 Mar  5 13:25 uid_unset.txt\n</code></pre> <p>Outside container:</p> <pre><code>mac-34392:~ geertvangeest$ ls -lh\n-rw-r--r--   1 geertvangeest  staff     400B Mar  5 14:11 uid_set.txt\n-rw-r--r--   1 geertvangeest  staff     400B Mar  5 14:15 uid_unset.txt\n</code></pre> <p>So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.</p> <p>Running <code>ubuntu-figlet</code> interactively while taking over <code>uid</code> and <code>gid</code> and mounting to a  specfied directory:</p> <p><pre><code>docker run -it --mount type=bind,source=C:/Users/geert/data,target=/data -u \"$(id -u):$(id -g)\" ubuntu-figlet\n</code></pre> Inside container:</p> <p><pre><code>I have no name!@e808d7c36e7c:/$ id\nuid=1003 gid=513 groups=513\n</code></pre> So, the container has taken over uid 1003 and group 513</p> <pre><code>I have no name!@e808d7c36e7c:/$ cd /data\nI have no name!@e808d7c36e7c:/data$ figlet 'uid set' &gt; uid_set.txt\nI have no name!@e808d7c36e7c:/data$ ls -lh\n-rw-r--r--  1 1003 513    400 Mar  5 13:11 uid_set.txt\n</code></pre> <p>So the file belongs to user 1003, and the group 513.</p> <p>Outside container:</p> <pre><code>/home/mobaxterm/data$ ls -lh\n-rwx------   1 geert  UserGrp     400 Mar  5 14:11 uid_set.txt\n</code></pre> <p>Which are the same as inside the container:</p> <pre><code>/home/mobaxterm/data$ echo \"$(id -u):$(id -g)\"\n1003:513\n</code></pre> <p>Running <code>ubuntu-figlet</code> interactively without taking over <code>uid</code> and <code>gid</code>:</p> <p><pre><code>docker run -it --mount type=bind,source=C:/Users/geert/data,target=/data ubuntu-figlet\n</code></pre> Inside container:</p> <pre><code>root@fface8afb220:/# id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre> <p>So, inside the container I am <code>root</code>. Creating new files will lead to ownership of <code>root</code> inside the container:</p> <pre><code>root@fface8afb220:/# cd /data\nroot@fface8afb220:/data# figlet 'uid unset' &gt; uid_unset.txt\nroot@fface8afb220:/data# ls -lh\n-rw-r--r--  1 1003 503    400 Mar  5 13:11 uid_set.txt\n-rw-r--r--  1 root root    400 Mar  5 13:25 uid_unset.txt\n</code></pre> <p>Outside container:</p> <pre><code>/home/mobaxterm/data$ ls -lh\n-rwx------   1 geert  UserGrp     400 Mar  5 14:11 uid_set.txt\n-rwx------   1 geert  UserGrp     400 Mar  5 14:15 uid_unset.txt\n</code></pre> <p>So, the uid and gid 0 (root:root) are not taken over. Instead, the uid and gid of the user running docker were used.</p>"},{"location":"course_material/day2/1_guidelines/","title":"General guidelines","text":""},{"location":"course_material/day2/1_guidelines/#workshop-goal","title":"Workshop goal","text":"<p>Over the course of the workshop, you will implement and improve a workflow to trim bulk RNAseq reads, align them to a genome, perform some quality checks (QC), count mapped reads, and identify Differentially Expressed Genes (DEG). The goal of the workshop is that after the last series of exercises, you will have implemented a simple workflow with commonly used Snakemake features. You will be able to use this workflow as a reference to implement your own workflows in the future.</p>"},{"location":"course_material/day2/1_guidelines/#software","title":"Software","text":"<p>All the software needed in this workflow is either:</p> <ul> <li>Already installed in the <code>snake_course</code> conda environment</li> <li>Already installed in a Docker container</li> <li>Will be installed via a conda environment during today\u2019s exercises</li> </ul> <p>All information of this course is based on the official documentation for Snakemake version <code>7.32.3</code>. </p>"},{"location":"course_material/day2/1_guidelines/#exercises","title":"Exercises","text":"<p>Each series of exercises is divided into multiple questions. We first provide a general explanation on the context behind each question; we then explicitly describe the task and provide details when they are required. We also provide hints that should help you with the most challenging parts of some questions. You should first try to solve the problems without using these hints! Do not hesitate to modify and overwrite your code from previous questions when specified in an exercise, as the solutions for each series of exercises are provided. If something is not clear at any point, please call us and we will do our best to answer your questions. You can also check the official Snakemake documentation for more information.</p>"},{"location":"course_material/day2/2_introduction_snakemake/","title":"Introduction to Snakemake","text":""},{"location":"course_material/day2/2_introduction_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand the structure of a Snakemake workflow</li> <li>Write rules and Snakefiles to produce the desired outputs</li> <li>Chain rules together</li> <li>Run a Snakemake workflow</li> </ul>"},{"location":"course_material/day2/2_introduction_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/2_introduction_snakemake/#structuring-a-workflow","title":"Structuring a workflow","text":"<p>It is advised to implement your code in a directory called <code>workflow</code> (more information about this in the next series of exercises). You are free to choose the names and location of files for the different steps of your workflow, but, for now, we recommend that you at least group all outputs from the workflow in a <code>results</code> folder within the <code>workflow</code> directory.</p> <p>A small reminder about conda environment</p> <p>If you try to run a command and get an error such as <code>Command 'snakemake' not found</code>, you are probably not in the right environment. To list them, use <code>conda env list</code>. Then activate the right environment with <code>conda activate &lt;env_name&gt;</code>. You can deactivate an environment with <code>conda deactivate</code>. To list the packages installed in an environment, activate it and use <code>conda list</code>.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#exercises","title":"Exercises","text":"<p>This series of exercises will bear no biological meaning, on purpose: it is designed to show you the fundamentals of Snakemake.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#creating-a-basic-rule","title":"Creating a basic rule","text":"<p>Rules are the basic blocks of a Snakemake workflow. A rule is like a recipe indicating how to produce a specific output . The actual application of a rule to create an output is called a job. A rule is defined in a Snakefile with the keyword <code>rule</code> and contains directives which indicate the rule\u2019s properties.</p> <p>To create the simplest rule possible, we need at least two directives:</p> <ul> <li><code>output</code>: path of the output file for this rule</li> <li><code>shell</code>: shell commands to execute in order to generate the output</li> </ul> <p>We will see other directives later in the course.</p> <p>Exercise: The following example shows the minimal syntax to implement a rule. What do you think it does? Does it create a file? If so, how is it called?</p> <pre><code>rule first_step:\n    output:\n        'results/first_step.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/first_step.txt'\n</code></pre> Answer <p>This rule uses the <code>echo</code> shell command to print the line <code>Hello world!</code> in an output file called <code>first_step.txt</code>, located in the <code>results</code> folder.</p> <p>Rules are defined and written in a file called Snakefile (note the capital <code>S</code> and the absence of extension in the filename). This file should be located at the root of the workflow directory (here, <code>workflow/Snakefile</code>).</p> <p>Exercise: Create a Snakefile and copy the previous rule in it. Because the Snakemake language is built on top of Python, spaces and indents are essential, so do not forget to keep the indentation as is and use space characters in the indents instead of tabs.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#executing-a-workflow-with-a-precise-output","title":"Executing a workflow with a precise output","text":"<p>It is now time to execute your first worklow! To do this, you need to tell Snakemake what is your target, i.e. what is the output that you want to generate.</p> <p>Exercise: Execute the workflow with <code>snakemake --cores 1 &lt;target&gt;</code>. What value should you use for <code>&lt;target&gt;</code>? Once Snakemake execution is finished, can you locate the output file?</p> Answer <ul> <li>Execute the workflow: <code>snakemake --cores 1 results/first_step.txt</code></li> <li>Visualise the content of the <code>results</code> folder: <code>ls -alh results/</code></li> <li>Check the output content: <code>cat results/first_step.txt</code></li> </ul> <p>Note that during the execution of the workflow, Snakemake automatically created the missing folder (<code>results/</code>) in the output path. If several folders are missing (for example, <code>test1/test2/test3/first_step.txt</code>), Snakemake will create all of them.</p> <p>Exercise: Re-run the exact same command. What happens?</p> Answer <p>Nothing! We get a message saying that Snakemake did not run anything:</p> <pre><code>Building DAG of jobs...\nNothing to be done (all requested files are present and up to date).\n</code></pre> <p>By default, Snakemake only runs a job if:</p> <ul> <li>A target file explicitly requested in the <code>snakemake</code> command is missing</li> <li>An intermediate file is missing and is required to produce a target file</li> <li>It detects when input files that have been modified more recently than output files, based on their modification dates. In this case, Snakemake will generate again the existing outputs.</li> </ul> <p>We can change this behaviour and force the re-run of a specific target by using the <code>-f</code> option: <code>snakemake --cores 1 -f results/first_step.txt</code> or force recreate ALL the outputs of the workflow using the <code>-F</code> option: <code>snakemake --cores 1 -F</code>. In practice, we can also alter Snakemake (re-)run policy, but we will not cover this topic in the course (see \u2013rerun-triggers option in Snakemake\u2019s CLI help and this git issue for more information).</p> <p>In the previous example, the values of the two rule directives are strings. For the <code>shell</code> directive (we will see other types of directive values later in the course), long string can be written on multiple lines for clarity, simply using a set of quotes for each line:</p> <pre><code>rule first_step:\n    output:\n        'results/first_step.txt'\n    shell:\n        'echo \"I want to print a very very very very very very '\n        'very very very very long string in my output\" &gt; results/first_step.txt'\n</code></pre> <p>Here, Snakemake will simply concatenate the two lines (paste each line one after the other) and execute the resulting command.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#adding-an-input-directive","title":"Adding an input directive","text":"<p>The next directive used by most rules is <code>input</code>. It indicates the path to a file that is required by the rule to generate the output. In the following example, we modified the previous rule to use the file previously created <code>results/first_step.tsv</code> as an input, and copy this file to <code>results/second_step.txt</code>:</p> <pre><code>rule second_step:\n    input:\n        'results/first_step.txt'\n    output:\n        'results/second_step.txt'\n    shell:\n        'cp results/first_step.txt results/second_step.txt'\n</code></pre> <p>Note that with this rule definition, Snakemake will not run if <code>results/first_step.tsv</code> does not exist!</p> <p>Exercise: Modify your first rule to add an <code>input</code> directive and execute the workflow. Check that the output was created and that the files are identical. If you get a <code>Missing input files for rule &lt;rule_name&gt;</code> error, that means that the input file is missing and cannot be created. How can you solve this problem?</p> Answer <ul> <li>Execute the workflow: <code>snakemake --cores 1 results/second_step.txt</code></li> <li>Visualise the content of the <code>results</code> folder: <code>ls -alh results/</code></li> <li>Check that the files are identical: <code>diff results/first_step.txt results/second_step.txt</code></li> <li>If the input file is missing, you can create it with <code>echo \"Hello world!\" &gt; results/first_step.txt</code> and then execute the workflow. We will see later why this happened and how to avoid it!</li> </ul>"},{"location":"course_material/day2/2_introduction_snakemake/#creating-a-workflow-with-several-rules","title":"Creating a workflow with several rules","text":"<p>Creating one Snakefile per rule does not seem like a good solution, so let\u2019s try to improve this.</p> <p>Exercise: Delete the <code>results/</code> folder, copy the two previous rules (<code>first_step</code> and <code>second_step</code>) in the same Snakefile (place the <code>first_step</code> rule first) and try to run the workflow without specifying an output. What happens?</p> Answer <ul> <li>Delete the <code>results</code> folder: using the graphic interface or <code>rm -rf results/</code></li> <li>Execute the workflow without output: <code>snakemake --cores 1</code></li> </ul> <p>Only the first output, <code>results/first_step.txt</code>, is created. During its execution, Snakemake tries to generate a specific output called target and resolve all dependencies based on this target. A target can be any output that can be generated by any rule in the workflow. When you do not specify a target, the default one is the output of the first rule in the Snakefile, here <code>results/first_step.txt</code> of <code>rule first_step</code>.</p> <p>Exercise: With this in mind, instead of one target, use a space-separated list of targets in your command, to generate multiple targets. Use the <code>-F</code> to force the re-run of the whole workflow or delete your <code>results/</code> folder beforehand.</p> Answer <ul> <li>Delete the <code>results</code> folder: using the graphic interface or <code>rm -rf results/</code></li> <li>Execute the workflow with multiple targets: <code>snakemake --cores 1 results/first_step.txt results/second_step.txt</code></li> </ul> <p>We should now see Snakemake execute the 2 rules and produce both targets/outputs.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#chaining-rules","title":"Chaining rules","text":"<p>Once again, writing all the outputs in the <code>snakemake</code> command does not look like a good solution: it is very time-consuming, error-prone (and annoying)! Imagine what happens when your workflow generate tens of outputs?! Fortunately, there is a way to simplify this, which relies on rules dependency.</p> <p>The core principle of Snakemake\u2019s execution is to compute a Directed Acyclic Graph (DAG) that summarizes dependencies between all the inputs and outputs required to generate the final desired outputs. For each job, starting from the jobs generating the final outputs, Snakemake checks if the required inputs exist. If they do not, the software looks for a rule that generates these inputs. This process is repeated until all dependencies are resolved. This is why Snakemake is said to have a \u2018bottom-up\u2019 approach: it starts from the last outputs and go back to the first inputs.</p> <p>Hint</p> <p>Your Snakefile should look like this:</p> <pre><code>rule first_step:\n    output:\n        'results/first_step.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/first_step.txt'\n\nrule second_step:\n    input:\n        'results/first_step.txt'\n    output:\n        'results/second_step.txt'\n    shell:\n        'cp results/first_step.txt results/second_step.txt'\n</code></pre> <p>Exercise: Delete the <code>results/</code> folder, identify your final output(s) and execute the workflow specifying only this(these) output(s) in the command.</p> Answer <ul> <li>Delete the <code>results</code> folder: using the graphic interface or <code>rm -rf results/</code></li> <li>Execute the workflow: <code>snakemake --cores 1 results/second_step.txt</code></li> <li>Visualise the content of the <code>results</code> folder: <code>ls -alh results/</code></li> </ul> <p>You should now see Snakemake executing the two rules and producing both outputs. To generate the output <code>results/second_step.txt</code>, Snakemake requires the input <code>results/first_step.txt</code>. Before the workflow is executed, this file does not exist, therefore, Snakemake looks for a rule that generates <code>results/first_step.txt</code>, in this case the rule <code>first_step</code>. The process is then repeated for <code>first_step</code>. In this case, the rule does not require any input, so all dependencies are resolved, and Snakemake can generate the DAG.</p>"},{"location":"course_material/day2/2_introduction_snakemake/#important-notes-on-rules-dependency","title":"Important notes on rules dependency","text":""},{"location":"course_material/day2/2_introduction_snakemake/#rules-must-produce-unique-outputs","title":"Rules must produce unique outputs","text":"<p>Because of the rules dependency process, by default, an output can only be generated by a single rule. Otherwise, Snakemake cannot decide which rule to use to generate this output, and the rules are considered ambiguous. In practice, there are ways to deal with ambiguous rules, but they should be avoided as much as possible and we will not cover them in this course (see the relevant section in the official documentation for more information).</p>"},{"location":"course_material/day2/2_introduction_snakemake/#rules-dependency-can-be-written-more-easily","title":"Rules dependency can be written more easily","text":"<p>It is possible to refer to the output of a rule directly in another rule with the syntax <code>rules.&lt;rule_name&gt;.output</code>. Note that you don\u2019t need quotes around this statement, because it is a Snakemake object. The following example implements this syntax for the two rule defined above:</p> <pre><code>rule first_step:\n    output:\n        'results/first_step.txt'\n    shell:\n        'echo \"Hello world!\" &gt; results/first_step.txt'\n\nrule second_step:\n    input:\n        rules.first_step.output\n    output:\n        'results/second_step.txt'\n    shell:\n        'cp results/first_step.txt results/second_step.txt'\n</code></pre> <p>This method has several advantages, among which:</p> <ul> <li>It limits the risk of error because we do not have to write the same filename at several locations</li> <li>A change in output name will be automatically propagated to rules that depend on it, i.e. the name only has to be changed once</li> <li>This makes the code much clearer and easier to understand: with this syntax, we instantly know the object type (<code>rule</code>), how/where it is created (<code>first_step</code>), and what it is (<code>output</code>)</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/","title":"Making a more general-purpose Snakemake workflow","text":""},{"location":"course_material/day2/3_generalising_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Create rules with multiple inputs and outputs</li> <li>Make the code shorter and more general by using placeholders and wildcards</li> <li>Optimise the memory usage of a workflow and checking its performances</li> <li>Visualise a workflow DAG</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"course_material/day2/3_generalising_snakemake/#data-origin","title":"Data origin","text":"<p>The data we will use during the exercises was produced in this work. Briefly, the team studied the transcriptional response of a strain of baker\u2019s yeast, Saccharomyces cerevisiae, facing environments with different amount of CO<sub>2</sub>. To this end, they performed 150 bp paired-end sequencing of mRNA-enriched samples. Detailed information on all the samples are available here, but just know that for the purpose of the course, we selected 6 samples (3 replicates per condition, low and high CO<sub>2</sub>) and down-sampled them to 1 million read-pairs each to reduce computation times.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#exercises","title":"Exercises","text":"<p>One of the aims of today\u2019s course is to develop a basic, yet efficient, workflow to analyse RNAseq data. This workflow takes reads coming from RNA sequencing as inputs and outputs a list of genes that are differentially expressed between two conditions. The files containing the reads are in FASTQ format and the output will be a tab-separated file containing a list of genes with expression changes, results of statistical tests\u2026</p> <p>In this series of exercises, we will create the \u2018backbone\u2019 of the workflow, i.e. the rules that are the most computationally expensive, namely:</p> <ul> <li>A rule to trim poor-quality reads</li> <li>A rule to map the trimmed reads on a reference genome</li> <li>A rule to convert and sort files from the SAM format to the BAM format</li> <li>A rule to count the reads mapping on each gene</li> </ul> <p>At the end of this series of exercises, the DAG of your workflow should look like this:</p> <p> </p> Rulegraph of the workflow at the end of the session <p>Designing and debugging a workflow</p> <p>If you have problems designing your Snakemake workflow or debugging it, you can find some help here.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#general-instructions-and-reminders","title":"General instructions and reminders","text":"<p>In each rule, you should try (as much as possible) to:</p> <ul> <li>Choose meaningful rule names</li> <li>Use rules dependency, with the syntax <code>rules.&lt;rule_name&gt;.output</code><ul> <li>If you use numbered outputs, the syntax becomes <code>rules.&lt;rule_name&gt;.output[n]</code> (with <code>n</code> starting at 0)</li> <li>If you use named outputs, the syntax becomes <code>rules.&lt;rule_name&gt;.output.&lt;output_name&gt;</code></li> </ul> </li> <li>Use placeholders</li> <li>Use wildcards<ul> <li>Choose meaningful wildcard names</li> <li>The <code>output</code>, <code>log</code>, and <code>benchmark</code> directives must have the same wildcard names!</li> <li>You can use the same wildcard names in multiple rules for consistency and readability, but Snakemake will treat them as independent wildcards and their values will not be shared: rules are self-contained and wildcards are local to each rule (see a very nice summary on wildcards)</li> </ul> </li> <li>Use multiple inputs/outputs (when needed/possible)</li> <li>Create a log file with the <code>log</code> directive</li> <li>Create a benchmark file with the <code>benchmark</code> directive</li> </ul> <p>If you have a doubt, do not hesitate to test your workflow logic with a dry-run (the <code>-n</code> flag): <code>snakemake --cores 1 -n &lt;target&gt;</code>. Snakemake will then display all the jobs required to generate the target. To obtain additional information on why a specific job is necessary, run Snakemake with the <code>-r</code> flag (which can be -and usually is- combined with <code>-n</code>): <code>snakemake --cores 1 -n -r &lt;target&gt;</code>. For each job, Snakemake will print a reason field explaining why the job was required. To visualize the exact command executed by each job (with the placeholders and wildcards replaced by their values), run snakemake with the <code>-p</code> flag: <code>snakemake --cores 1 -n -r -p &lt;target&gt;</code>.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#downloading-the-data-and-setting-up-the-directory-structure","title":"Downloading the data and setting up the directory structure","text":"<p>In this part, we will download the data and start building the directory structure of our workflow according to the official recommendations. We already starting doing so in the previous series of exercises and ultimately, it should resemble this:</p> <pre><code>\u2502\u2500\u2500 .gitignore\n\u2502\u2500\u2500 README.md\n\u2502\u2500\u2500 LICENSE.md\n\u2502\u2500\u2500 benchmarks\n\u2502   \u2502\u2500\u2500 sample1.fastq\n\u2502   \u2514\u2500\u2500 sample2.fastq\n\u2502\u2500\u2500 config\n\u2502   \u2502\u2500\u2500 config.yaml\n\u2502   \u2514\u2500\u2500 some-sheet.tsv\n\u2502\u2500\u2500 data\n\u2502   \u2502\u2500\u2500 sample1.fastq\n\u2502   \u2514\u2500\u2500 sample2.fastq\n\u2502\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 rulegraph.svg\n\u2502\u2500\u2500 logs\n\u2502   \u2502\u2500\u2500 sample1.log\n\u2502   \u2514\u2500\u2500 sample2.log\n\u2502\u2500\u2500 results\n\u2502   \u2502\u2500\u2500 sample1\n\u2502   \u2502   \u2514\u2500\u2500 sample1.bam\n\u2502   \u2502\u2500\u2500 sample2\n\u2502   \u2502   \u2514\u2500\u2500 sample2.bam\n\u2502   \u2514\u2500\u2500 DEG_list.tsv\n\u2502\u2500\u2500 resources\n\u2502   \u2502\u2500\u2500 Scerevisiae.fasta\n\u2502   \u2514\u2500\u2500 Scerevisiae.gtf\n\u2514\u2500\u2500 workflow\n    \u2502\u2500\u2500 envs\n    \u2502   \u2502\u2500\u2500 tool1.yaml\n    \u2502   \u2514\u2500\u2500 tool2.yaml\n    \u2502\u2500\u2500 rules\n    \u2502   \u2502\u2500\u2500 module1.smk\n    \u2502   \u2514\u2500\u2500 module2.smk\n    \u2502\u2500\u2500 scripts\n    \u2502   \u2502\u2500\u2500 script1.py\n    \u2502   \u2514\u2500\u2500 script2.R\n    \u2514\u2500\u2500 Snakefile\n</code></pre> <p>For now, the main thing to remember is that the workflow code goes into a subfolder called <code>workflow</code> and the rest is mostly input/output files, except for the <code>config</code> subfolder, which will be explained later. All output files generated in the workflow should be stored under <code>results/</code>.</p> <p>Now, let\u2019s download the data, uncompress it and build the first part of the directory structure.</p> <pre><code>wget https://apollo.vital-it.ch/trackvis/snakemake_rnaseq.tar.gz  # Download the data\ntar -xvf snakemake_rnaseq.tar.gz  # Uncompress the archive\nrm snakemake_rnaseq.tar.gz  # Delete the archive\ncd snakemake_rnaseq/  # Start developing in a new folder\n</code></pre> <p>In this new folder, you should now see 2 subfolders:</p> <ul> <li><code>data/</code>, which contains the data to analyse</li> <li><code>resources/</code>, which contains retrieved resources, here the assembly, the genome indices and the annotation file of S. cerevisiae. It may also contain small resources delivered along with the workflow via git</li> </ul> <p>Let\u2019s create another subfolder, this time to host all the files containing the code, as well as the Snakefile:</p> <pre><code>mkdir workflow  # Create a new folder\ntouch workflow/Snakefile  # Create an empty Snakefile\n</code></pre> <p>The Snakefile marks the entrypoint of the workflow. It will be automatically discovered when running Snakemake from the root of the structure, here <code>snakemake_rnaseq/</code>. We can also tell Snakemake to use a specific Snakefile with the <code>-s</code> flag: <code>snakemake --cores 1 -s &lt;Snakefile_path&gt; &lt;target&gt;</code>, but it is highly discouraged as it hampers reproducibility.</p> <p>If you followed the general instructions, Snakemake should create all the other missing folders by itself (except one that you will discover at the end of this series of exercises), so it is now time to create the rules mentioned earlier. Have a look here for a few pieces of advice on workflow design.</p> <p>\u2018bottom-up\u2019 or \u2018top-down\u2019 development?</p> <p>Even if it is often easier to start from the final outputs and work backwards to the first inputs, the next exercises are presented in the opposite direction (first inputs to last outputs) to make the session easier to understand. That being said, feel free to work and develop your code in the order you prefer!</p> <p>Even if we asked you to use wildards, do not try to process all the samples yet. Choose and work with one sample (which means two .fastq files because reads are paired-end) in this series of exercises. We will see an efficient way to process list of files in the next series of exercises.</p>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-trim-reads","title":"Creating a rule to trim reads","text":"<p>Usually, the first step in dealing with sequencing data is to improve the reads quality by removing low quality bases, stretches of As and Ns and reads that are too short.</p> <p>Adapters trimming</p> <p>In theory, trimming also removes sequencing adapters, but we will not do it here to keep computation time low and avoid having to parse other files to extract the adapter sequences.</p> <p>Exercise: Implement a rule to trim the reads contained in .fastq files using atropos.</p> <p>Hint</p> <ul> <li>You can find information on how to use atropos and its parameters with <code>atropos trim -h</code></li> <li>The files to trim are located in <code>data/</code></li> <li>The base of the trimming command is <code>atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\"</code><ul> <li>If you are interested in what these options mean, see below for an explanation</li> </ul> </li> <li>The paths of the files to trim (i.e. input files, in FASTQ format) are specified with the options <code>-pe1</code> (first read) and <code>-pe2</code> (second read)</li> <li>The paths of the trimmed files (i.e. output files, also in FASTQ format) are specified with the options <code>-o</code> (first read) and <code>-p</code> (second read)</li> <li>atropos outputs some information as well as its trimming report in the terminal (stdout to be exact); do not forget to redirect these information to the log file with <code>&gt;&gt; {log}</code></li> </ul> <p>Please give it a try before looking at the answer!</p> Answer <p>This is one way of writing this rule, but definitely not the only way! This is true for all the rules presented here.</p> <pre><code>rule fastq_trim:\n    '''\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    '''\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    log:\n        'logs/{sample}/{sample}_atropos_trimming.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_trimming.txt'\n    resources:\n        mem_mb = 500\n    shell:\n        '''\n        echo \"Trimming reads in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt; {log}\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2} &amp;&gt;&gt; {log}\n        echo \"Trimmed files saved in &lt;{output.trim1}&gt; and &lt;{output.trim2}&gt; respectively\" &gt;&gt; {log}\n        echo \"Trimming report saved in &lt;{log}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre> <p>Note the three things that are happening here:</p> <ol> <li>We used the <code>{sample}</code> wildcards twice in the output paths. This is because we prefer to have all the files linked to a sample in the same directory</li> <li>We added a memory limit for this job: 500 MB. Because we have limited resources in this server compared to a High Performance Computing cluster (HPC), this will help Snakemake to better allocate resources and parallelise jobs. You can determine the maximum amount of memory used by a rule thanks to the max_rss column in a benchmark result (results are shown in MB). More information here</li> <li>We used a backslash <code>\\</code> to split a very long line in smaller lines. This is purely \u2018cosmetic\u2019, to avoid very long lines that are painful to read, copy\u2026</li> </ol> <p>Paths in Snakemake</p> <p>All the paths in the Snakefile are relative to the working directory in which the <code>snakemake</code> command is executed.</p> <ul> <li>If you execute Snakemake in <code>snakemake_rnaseq/</code>, the relative path to the input files in the rule is <code>data/&lt;sample&gt;.fastq</code></li> <li>If you execute Snakemake in <code>snakemake_rnaseq/workflow/</code>, the relative path to the input files in the rule is <code>../data/&lt;sample&gt;.fastq</code></li> </ul> <p>Exercise: If you had to run the workflow by specifying only one output, what command would you use?</p> Answer <p><code>snakemake --cores 1 -r -p results/highCO2_sample1/highCO2_sample1_atropos_trimmed_1.fastq</code></p> <p>If you run it now, don\u2019t forget to have a look at the log and benchmark files!</p>"},{"location":"course_material/day2/3_generalising_snakemake/#atropos-options","title":"atropos options","text":"<ul> <li><code>-q 20,20</code>: trim low-quality bases from 5\u2019, 3\u2019 ends of each read before adapter removal</li> <li><code>--minimum-length 25</code>: discard trimmed reads that are shorter than 25 bp</li> <li><code>--trim-n</code>: trim N\u2019s on ends of reads</li> <li><code>--preserve-order</code>: preserve order of reads in input files</li> <li><code>--max-n 10</code>: discard reads with more than 10 N</li> <li><code>--no-cache-adapters</code>: do not cache adapters list as \u2018.adapters\u2019 in the working directory</li> <li><code>-a \"A{{20}}\" -A \"A{{20}}\"</code>: remove series of 20 As in the adapter sequence (<code>-a</code> for the first read of the pair, <code>-A</code> for the second one)<ul> <li>The usual command-line syntax is <code>-a \"A{20}\"</code>. Here, brackets were doubled to prevent Snakemake from interpreting <code>{20}</code> as a wildcard</li> </ul> </li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-map-trimmed-reads-on-a-reference-genome","title":"Creating a rule to map trimmed reads on a reference genome","text":"<p>Once we have trimmed reads, the next step is to map those reads onto a reference assembly, here S. cerevisiae strain S288C, to eventually obtain read counts. The assembly used in this exercise is RefSeq GCF_000146045.2 and was retrieved via the NCBI genome website.</p> <p>Exercise: Implement a rule to map the trimmed reads on the S. cerevisiae assembly using HISAT2.</p> <p>HISAT2 genome index</p> <p>To align reads to a genome, HISAT2 relies on a graph-based index. We built the genome index for you, using the command <code>hisat2-build -p 24 -f Scerevisiae.fasta resources/genome_indices/Scerevisiae_index</code>. <code>-p</code> is the number of threads to use, <code>-f</code> is the genomic sequence in FASTA format and <code>Scerevisiae_index</code> is the global name shared by all the index files.</p> <p>Hint</p> <ul> <li>You can find information on how to use HISAT2 and its parameters with <code>hisat2 -h</code></li> <li>The base of the mapping command is <code>hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal</code><ul> <li>If you are interested in what these options mean, see below for an explanation</li> </ul> </li> <li>The path of the genome indices (i.e. input files, in binary format) is specified with the option <code>-x</code>. The files have a shared title of <code>resources/genome_indices/Scerevisiae_index</code>, which is the value you need to use for <code>-x</code></li> <li>The paths of the trimmed files (i.e. input files) are specified with the options <code>-1</code> (first read) and <code>-2</code> (second read)</li> <li>The path of the mapped reads file (i.e. output file, in SAM format) is specified with the option <code>-S</code> (do not forget the .sam extension to the filename)</li> <li>The path of the mapping report (i.e. output file, in text format) is specified with the option <code>--summary-file</code></li> <li>HISAT2 also outputs information in the terminal (stderr to be exact); do not forget to redirect these to the log file with <code>2&gt;&gt; {log}</code></li> <li>This step is the longest of the workflow. With the current settings, it should take ~6 min to complete. If you decide to run it now, you should launch it and start working on the next rules</li> </ul> <p>Please give it a try before looking at the answer!</p> Answer <pre><code>rule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq on a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    resources:\n        mem_gb = 2\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x resources/genome_indices/Scerevisiae_index \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre> <p>Exercise: If you had to run the workflow by specifying only one output, what command would you use?</p> Answer <p><code>snakemake --cores 1 -r -p results/highCO2_sample1/highCO2_sample1_mapped_reads.sam</code></p> <p>If you run it now, don\u2019t forget to have a look at the log and benchmark files!</p>"},{"location":"course_material/day2/3_generalising_snakemake/#hisat2-options","title":"HISAT2 options","text":"<ul> <li><code>--dta</code>: report alignments tailored for transcript assemblers</li> <li><code>--fr</code>: set alignment of -1, -2 mates to forward/reverse (position of reads in a pair relatively to each other)</li> <li><code>--no-mixed</code>: remove unpaired alignments for paired reads</li> <li><code>--no-discordant</code>: remove discordant alignments for paired reads</li> <li><code>--time</code>: print wall-clock time taken by search phases</li> <li><code>--new-summary</code>: print alignment summary in a new style</li> <li><code>--no-unal</code>: suppress SAM records for reads that failed to align</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-convert-and-sort-sam-files-to-bam","title":"Creating a rule to convert and sort SAM files to BAM","text":"<p>HISAT2 only outputs mapped reads in the SAM format. However, most downstream analysis tools use the BAM format, which is the compressed binary version of the SAM format and, as such, is much smaller, easier to manipulate and transfer and allows a faster data retrieval. Additionally, many analyses require that BAM files are sorted by genomic coordinates and indexed, because sorted BAM files can be processed much more easily and quickly than unsorted ones.</p> <p>Alignment data files</p> <p>More information on alignment data files and other formats on the official github repository of the formats.</p> <p>Exercise: Implement a single rule to:</p> <ol> <li>Convert SAM files to BAM using Samtools</li> <li>Sort the BAM files using Samtools</li> <li>Index the sorted BAM files using Samtools</li> </ol> <p>Hint</p> <ul> <li>You can find information on how to use Samtools and its parameters with <code>samtools --help</code></li> <li>You need to write 3 commands that will be executed sequentially: the output of command 1 will be the input of command 2 etc\u2026<ul> <li>No panic! These commands are pretty simple and do not use many options!</li> </ul> </li> <li>To convert SAM format to the BAM format, use the command <code>samtools view &lt;input_file&gt; -b -o &lt;output_file&gt;</code></li> <li>To sort a BAM file, use the command <code>samtools sort &lt;input_file&gt; -O bam -o &lt;output_file&gt;</code></li> <li>To index a BAM file, use the command <code>samtools index -b &lt;input_file&gt; -o &lt;output_file&gt;</code><ul> <li>The index must have the exact same name than its associated BAM file, except it finishes with the extension <code>.bam.bai</code> instead of <code>.bam</code></li> </ul> </li> <li>If you are interested in what these options mean, see below for an explanation</li> <li>To catch potential information and errors, do not forget to redirect <code>stderr</code> to the log file with <code>2&gt;&gt; {log}</code></li> </ul> <p>Please give it a try before looking at the answer!</p> Answer <pre><code>rule sam_to_bam:\n    '''\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    '''\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    log:\n        'logs/{sample}/{sample}_mapping_sam_to_bam.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping_sam_to_bam.txt'\n    resources:\n        mem_mb = 250\n    shell:\n        '''\n        echo \"Converting &lt;{input.sam}&gt; to BAM format\" &gt; {log}\n        samtools view {input.sam} -b -o {output.bam} 2&gt;&gt; {log}\n        echo \"Sorting BAM file\" &gt;&gt; {log}\n        samtools sort {output.bam} -O bam -o {output.bam_sorted} 2&gt;&gt; {log}\n        echo \"Indexing the sorted BAM file\" &gt;&gt; {log}\n        samtools index -b {output.bam_sorted} -o {output.index} 2&gt;&gt; {log}\n        echo \"Sorted file saved in &lt;{output.bam_sorted}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre> <p>Exercise: If you had to run the workflow by specifying only one output, what command would you use?</p> Answer <p><code>snakemake --cores 1 -r -p results/highCO2_sample1/highCO2_sample1_mapped_reads_sorted.bam</code></p> <p>If you run it now, don\u2019t forget to have a look at the log and benchmark files!</p>"},{"location":"course_material/day2/3_generalising_snakemake/#samtools-options","title":"Samtools options","text":"<ul> <li>samtools view<ul> <li><code>-b</code>: flag to tell Samtools to create an output in BAM format</li> <li><code>-o</code>: path of the output file</li> </ul> </li> <li>samtools view<ul> <li><code>-O bam</code>: flag to tell Samtools to create an output in BAM format</li> <li><code>-o</code>: path of the output file</li> </ul> </li> <li>samtools index<ul> <li><code>-b</code>: flag to tell Samtools to create an index in BAI format</li> </ul> </li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#creating-a-rule-to-count-mapped-reads","title":"Creating a rule to count mapped reads","text":"<p>Most of the analyses happening downstream the alignment step, including Differential Expression Analyses, are starting off read counts, either by exon or gene. However, we are still missing those counts!</p> <p>Counting reads on exons/genes</p> <p>To count reads mapping on genomic features, we first need a definition of those features. In this case, we picked one of the best-known model organism, S. cerevisiae, which has been annotated for a long time. These annotations are easily available on the NCBI or the Saccharomyces Genome Database. If your organism has not been annotated yet, there are ways to work around this problem, but this is an entirely different field that we won\u2019t discuss here!</p> <p>Chromosome names</p> <p>If you are working with genome sequences and annotations from different sources, remember that they must contain the chromosome names, otherwise counting will not work.</p> <p>Exercise: Implement a rule to count the reads mapping on each gene of the S. cerevisiae genome using featureCounts.</p> <p>Hint</p> <ul> <li>You can find information on how to use featureCounts and its parameters with <code>featureCounts -h</code></li> <li>The base of the mapping command is <code>featureCounts -t exon -g gene_id -s 2 -p -B -C --largestOverlap --verbose -F GTF</code><ul> <li>If you are interested in what these options mean, see below for an explanation</li> </ul> </li> <li>The path of the file containing the annotations (i.e. input files, in GTF format) is specified with the <code>-a</code> option. This file is located at <code>resources/Scerevisiae.gtf</code><ul> <li>There are two main annotations format: GTF and GFF. The former is lighter and easier to work with, so that is the one we will use</li> </ul> </li> <li>The paths of the sorted BAM file(s) (i.e. input file(s)) are not specified with an option, they are simply added at the end of the command</li> <li>The path of the file containing the count results (i.e. output file, in tsv format) is specified with the option <code>-o</code><ul> <li>featureCounts will also output a separate file (in tsv format) including summary statistics of counting results, with the name <code>&lt;output_name&gt;</code>.summary. For example, if the output is <code>test.tsv</code>, the summary will be printed in <code>test.tsv.summary</code>. Do not forget this output in your rule</li> </ul> </li> <li>featureCounts also outputs information in the terminal (stderr to be exact); do not forget to redirect these to the log file with <code>2&gt;&gt; {log}</code></li> </ul> <p>Please give it a try before looking at the answer!</p> Answer <pre><code>rule reads_quantification_genes:\n    '''\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly.\n    '''\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt'\n    resources:\n        mem_mb = 500\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre>"},{"location":"course_material/day2/3_generalising_snakemake/#featurecounts-options","title":"featureCounts options","text":"<ul> <li><code>-t</code>: specify on which feature type to count the reads</li> <li><code>-g</code>: specify if and how to gather feature counts. Here, reads are counted by exon (<code>-t</code>) and the exon counts are gathered by genes \u2018meta-features\u2019 (<code>-g</code>)</li> <li><code>-s</code>: perform strand-specific read counting<ul> <li>Strandedness is determined by looking at the mRNA library preparation kit. It can also be determined a posteriori with scripts such as infer_experiment.py from the RSeQC package</li> </ul> </li> <li><code>-p</code>: count fragments instead of reads.<ul> <li>If you don\u2019t use this option with paired-end reads, featureCounts won\u2019t be able to assign the read-pairs to features</li> </ul> </li> <li><code>-B</code>: only count read pairs that have both ends aligned</li> <li><code>-C</code>: do not count read pairs that have their two ends mapping to different chromosomes or mapping on the same chromosome but on different strands</li> <li><code>--largestOverlap</code>: assign reads to the meta-feature/feature that has the largest number of overlapping bases</li> <li><code>-F</code>: specify format of the provided annotation file</li> <li><code>--verbose</code>: output verbose information, such as unmatched chromosome/contig names</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#running-the-workflow","title":"Running the workflow","text":"<p>Exercise: If you have not done it after each step, it is now time to run the entire workflow on your sample of choice. What command will you use to run it?</p> Answer <p>Because all the rules are chained together, you only need to specify one of the final outputs to trigger the execution of all the previous rules:</p> <p><code>snakemake --cores 1 -F -r -p results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv</code>.</p> <p>You can add the <code>-F</code> flag to force an entire re-run. The entire run should take about ~10 min to complete.</p> <p>Exercise: Check Snakemake\u2019s log in <code>.snakemake/log/</code>. Is everything as you expected, especially the wildcard values, input and output names etc\u2026?</p> Answer <p><code>cat .snakemake/log/&lt;latest_log&gt;</code></p>"},{"location":"course_material/day2/3_generalising_snakemake/#visualising-the-dag-of-the-workflow","title":"Visualising the DAG of the workflow","text":"<p>We have now implemented and run the main steps of our workflow. It is always a good idea to visualise the whole process to check for errors and inconsistencies. Snakemake\u2019s has a built-in workflow visualisation feature to do this.</p> <p>Exercise: Visualise the entire workflow\u2019s Directed Acyclic Graph using the <code>--dag</code> flag. Do you need to specify a target?</p> <p>Hint</p> <ul> <li>Try to follow the official recommendations on workflow structure, which states that images are supposed to go in the <code>images/</code> subfolder</li> <li>Snakemake prints a DAG in text format, so we need to use the <code>dot</code> command to transform it into a picture</li> <li>Save the result as a PNG picture</li> </ul> Answer <p>If we run the command without target: <code>snakemake --cores 1 --dag -F | dot -Tpng &gt; images/dag.png</code>, we will get a <code>Target rules may not contain wildcards.</code> error, which means we need to add a target. Same as before, it makes sense to use one of the final outputs to get the entire workflow: <code>snakemake --cores 1 --dag -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpng &gt; images/dag.png</code>.</p> <p>But once again, we will get an error: <code>BrokenPipeError: [Errno 32] Broken pipe</code>. This is because we are piping the command output to a folder (<code>images/</code>) that does not exist yet The folder is not created by Snakemake because it isn\u2019t handled as part of an actual run. So we have to create the folder before generating the DAG:</p> <pre><code>mkdir images\nsnakemake --cores 1 --dag -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpng &gt; images/dag.png\n</code></pre> <p>Some explanations on the command:</p> <ul> <li><code>-F</code>: force to show the entire worklow and ensures all jobs are shown. You can also use <code>-f &lt;target&gt;</code> to show fewer jobs</li> <li><code>dot</code>: tool that is a part of the graphviz package and is used to draw hierarchical or layered drawings of directed graphs, i.e. graphs in which edges (arrows) have a direction</li> <li><code>-T</code>: control the image format. Available formats are listed here</li> </ul> <p>DAG aspect</p> <p>If you already computed all the outputs of the workflow, steps in the DAG will have dotted lines. To visualise the DAG before running the workflow, add <code>-F/--forceall</code> to the snakemake command to force the execution of all jobs.</p> <p>DAG = dry-run</p> <p>The <code>--dag</code> flag implicitly activates the <code>--dry-run/--dryrun/-n</code> option, which means that no jobs are executed during the plot creation.</p> <p>There are actually 3 types of DAG:</p> <ul> <li>A DAG, created with the <code>--dag</code> option</li> <li>A filegraph, created with the <code>--filegraph</code> option</li> <li>A rulegraph, created with the <code>--rulegraph</code> option</li> </ul> <p>Exercise: Generate the filegraph and rulegraph of your workflow. Feel free to try different pictures format. What are the differences between the plots?</p> Answer <ul> <li>Generate the rulegraph: <code>snakemake --cores 1 --rulegraph -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tpdf &gt; images/rulegraph.pdf</code></li> <li>Generate the filegraph: <code>snakemake --cores 1 --filegraph -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv | dot -Tjpg &gt; images/filegraph.jpg</code></li> </ul> <p>You should obtain the 3 following figures:</p> DAG, rulegraph and filegraph (respectively) of the workflowat the end of the session <p>The differences between these plots are:</p> <ul> <li><code>--dag</code>: dependency graph of all the jobs</li> <li><code>--filegraph</code>: dependency graph of rules with inputs and outputs (rule appears once, with wildcards)</li> <li><code>--rulegraph</code>: dependency graph of rules (rule appears once)</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#designing-a-snakemake-workflow-and-debugging-it","title":"Designing a Snakemake workflow\u2026 and debugging it!","text":""},{"location":"course_material/day2/3_generalising_snakemake/#designing-a-workflow","title":"Designing a workflow","text":"<p>There are many ways to design a new workflow, but these few pieces of advice will be useful in most cases:</p> <ul> <li>Start with a pen and paper: try to find out how many rules you will need and how they depend on each other. In other terms, start by sketching the DAG of your workflow!<ul> <li>Remember that Snakemake has a bottom-up approach (it goes from the final outputs to the first input), so it may be easier for you to work in that order as well and write your last rule first</li> <li>Determine which rules (if any) aggregate or split inputs and create input functions accordingly (we will see how these functions work in session 4)  </li> </ul> </li> <li>Make sure your input and output directives are right before worrying about anything else, especially the shell sections.<ul> <li>Remember that Snakemake builds the DAG before running the shell commands, so you can use the <code>--dryrun</code> option to test the workflow before running it. You can even do that without writing all the shell commands!</li> </ul> </li> <li>List any parameters or settings that might need to be adjusted</li> <li>Choose meaningful and easy-to-understand names for your inputs, outputs, parameters, wildcards\u2026 to make your Snakefile as readable as possible. This is true for every script, piece of code, variable etc\u2026 and Snakemake is no exception! Have a look at The Zen of Python for more information</li> </ul>"},{"location":"course_material/day2/3_generalising_snakemake/#debugging-a-workflow","title":"Debugging a workflow","text":"<p>It is very likely you will see bugs and errors the first time you try to run a new Snakefile: don\u2019t be discouraged, this is normal!</p> <p>Order of operations in Snakemake</p> <p>The topic was tackled when DAGs were mentioned, but to efficiently debug a workflow, it is worth taking a deeper look at what Snakemake does when you execute the command <code>snakemake --cores 1 &lt;target&gt;</code>. There are 3 main phases:</p> <ol> <li>Prepare to run:<ol> <li>Read all the rule definitions from the Snakefile</li> </ol> </li> <li>Resolve the DAG (when Snakemake says \u2018Building DAG of jobs\u2019):<ol> <li>Check what output(s) are required</li> <li>Look for a matching rule by looking at the outputs of all the rules</li> <li>Fill in the wildcards to determine the input of the matching rule</li> <li>Check whether this input is available; if not, repeat Step 2 until everything is resolved</li> </ol> </li> <li>Run:<ol> <li>If needed, create the folder for the output(s)</li> <li>If needed, remove the outdated output(s)</li> <li>Run the shell command with the placeholders replaced</li> <li>Check that the command ran without errors and produced the expected output(s)</li> </ol> </li> </ol> <p>Debugging advice</p> <p>Sometimes, Snakemake will give you a precise error report, but other times less so. Try to identify which phase of execution failed (see previous paragraph on order of operations) and double-check the most common error causes for that phase:</p> <ol> <li>Parsing phase failures (phase 1):<ul> <li>Syntax errors, among which (but not limited to):<ul> <li>This errors can be easily solved using a text editor with Python/Snakemake text colouring</li> <li>Missing commas/colons/semicolons</li> <li>Unbalanced quotes/brackets/parenthesis</li> <li>Wrong indentation</li> </ul> </li> <li>Failure to evaluate expressions<ul> <li>Problems in functions (<code>expand()</code>, input functions\u2026) in input/output directives</li> <li>Python logic added outside of rules</li> </ul> </li> <li>Other problems with rule definition<ul> <li>Invalid rule names/directives</li> <li>Invalid wildcard names</li> <li>Mismatched wildcards</li> </ul> </li> </ul> </li> <li>DAG building failures (phase 2, before Snakemake tries to run any job):<ul> <li>Failure to determine the target</li> <li>Ambiguous rules making the same output(s)</li> <li>On the contrary, no rule making the required output(s)</li> <li>Circular dependency (violating the \u2018Acyclic\u2019 property of a DAG).</li> <li>Write-protected output(s)</li> </ul> </li> <li>DAG running failures (phase 3, <code>--dry-run</code> works and builds the DAG, but the real execution fails):<ul> <li>When a job fails, Snakemake reports an error, deletes all output file(s) for that job (potential corruption), and stops</li> <li>Shell command returning non-zero status</li> <li>Missing output file(s) after the commands have run</li> <li>Reference to a <code>$shell_variable</code> before it was set</li> <li>Use of a wrong/unknown placeholder inside <code>{ }</code></li> </ul> </li> </ol>"},{"location":"course_material/day2/4_decorating_workflow/","title":"Decorating and optimising a Snakemake workflow","text":""},{"location":"course_material/day2/4_decorating_workflow/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Optimise a workflow by multi-threading</li> <li>Use non-file parameters and config files in rules</li> <li>Create rules with non-conventional outputs</li> <li>Modularise a workflow</li> <li>Make a workflow process a list of files rather than one file at a time</li> </ul>"},{"location":"course_material/day2/4_decorating_workflow/#exercises","title":"Exercises","text":"<p>In this series of exercises, we will create only one new rule to add to our workflow, because this part aims mainly to show how to improve and \u2018decorate\u2019 the rules we previously wrote.</p> <p>Development and back-up</p> <p>During this session, we will modify our Snakefile quite heavily, so it may be a good idea to start by making a back-up: <code>cp worklow/Snakefile worklow/Snakefile_backup</code>. As a general rule, if you have a doubt on the code you are developing, do not hesitate to make a back-up.</p>"},{"location":"course_material/day2/4_decorating_workflow/#optimising-a-workflow-by-multi-threading","title":"Optimising a workflow by multi-threading","text":"<p>When working with real datasets, most processes are very long and computationally expensive. Fortunately, they can be parallelised very efficiently to decrease the computation time by using several threads for a single job.</p> <p>Exercise: Parallelise as much processes as possible using the <code>threads</code> directive and test its effect:</p> <ol> <li>Identify which software can make use of parallelisation</li> <li>Identify in each software the parameter that controls multi-threading</li> <li>Implement the multi-threading</li> </ol> <p>Hint</p> <ul> <li>Check the software documentation and parameters with the <code>-h/--help</code> flags</li> <li>Remember that multi-threading only applies to software that can make use of a threads parameters, Snakemake itself cannot parallelise a software automatically</li> <li>Remember that you need to add threads to the Snakemake rule but also to the commands! Just increasing the number of threads in Snakemake will not magically run a command with multiple threads</li> <li>Remember that you have 4 threads in total, so even if you ask for more in a rule, Snakemake will cap this value at 4. And if you use 4 threads in a rule, that means that no other job can run parallel!</li> </ul> Answer <p>It turns out that all the software except <code>samtools index</code> can handle multi-threading:</p> <ul> <li><code>atropos trim</code>, <code>hisat2</code>, <code>samtools view</code>, and <code>samtools sort</code> use the <code>--threads</code> option</li> <li><code>featureCounts</code> uses the <code>-T</code> option</li> </ul> <p>Let\u2019s use 4 threads for the mapping step and 2 for the other steps. Your Snakefile should look like this: <pre><code>rule fastq_trim:\n    '''\n    This rule trims paired-end reads to improve their quality. Specifically, it removes:\n    - Low quality bases\n    - A stretches longer than 20 bases\n    - N stretches\n    '''\n    input:\n        reads1 = 'data/{sample}_1.fastq',\n        reads2 = 'data/{sample}_2.fastq',\n    output:\n        trim1 = 'results/{sample}/{sample}_atropos_trimmed_1.fastq',\n        trim2 = 'results/{sample}/{sample}_atropos_trimmed_2.fastq'\n    log:\n        'logs/{sample}/{sample}_atropos_trimming.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_trimming.txt'\n    resources:\n        mem_mb = 500\n    threads: 2\n    shell:\n        '''\n        echo \"Trimming reads in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt; {log}\n        atropos trim -q 20,20 --minimum-length 25 --trim-n --preserve-order --max-n 10 \\\n        --no-cache-adapters -a \"A{{20}}\" -A \"A{{20}}\" --threads {threads} \\\n        -pe1 {input.reads1} -pe2 {input.reads2} -o {output.trim1} -p {output.trim2} &amp;&gt;&gt; {log}\n        echo \"Trimmed files saved in &lt;{output.trim1}&gt; and &lt;{output.trim2}&gt; respectively\" &gt;&gt; {log}\n        echo \"Trimming report saved in &lt;{log}&gt;\" &gt;&gt; {log}\n        '''\n\nrule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq on a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    resources:\n        mem_gb = 2\n    threads: 4\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x resources/genome_indices/Scerevisiae_index --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n\nrule sam_to_bam:\n    '''\n    This rule converts a sam file to bam format, sorts it and indexes it.\n    '''\n    input:\n        sam = rules.read_mapping.output.sam\n    output:\n        bam = 'results/{sample}/{sample}_mapped_reads.bam',\n        bam_sorted = 'results/{sample}/{sample}_mapped_reads_sorted.bam',\n        index = 'results/{sample}/{sample}_mapped_reads_sorted.bam.bai'\n    log:\n        'logs/{sample}/{sample}_mapping_sam_to_bam.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping_sam_to_bam.txt'\n    resources:\n        mem_mb = 250\n    threads: 2\n    shell:\n        '''\n        echo \"Converting &lt;{input.sam}&gt; to BAM format\" &gt; {log}\n        samtools view {input.sam} --threads {threads} -b -o {output.bam} 2&gt;&gt; {log}\n        echo \"Sorting BAM file\" &gt;&gt; {log}\n        samtools sort {output.bam} --threads {threads} -O bam -o {output.bam_sorted} 2&gt;&gt; {log}\n        echo \"Indexing the sorted BAM file\" &gt;&gt; {log}\n        samtools index -b {output.bam_sorted} -o {output.index} 2&gt;&gt; {log}\n        echo \"Sorted file saved in &lt;{output.bam_sorted}&gt;\" &gt;&gt; {log}\n        '''\n\nrule reads_quantification_genes:\n    '''\n    This rule quantifies the reads of a bam file mapping on genes and produces\n    a count table for all genes of the assembly. The strandedness parameter\n    is determined by get_strandedness().\n    '''\n    input:\n        bam_once_sorted = rules.sam_to_bam.output.bam_sorted,\n    output:\n        gene_level = 'results/{sample}/{sample}_genes_read_quantification.tsv',\n        gene_summary = 'results/{sample}/{sample}_genes_read_quantification.summary'\n    log:\n        'logs/{sample}/{sample}_genes_read_quantification.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_genes_read_quantification.txt'\n    resources:\n        mem_mb = 500\n    threads: 2\n    shell:\n        '''\n        echo \"Counting reads mapping on genes in &lt;{input.bam_once_sorted}&gt;\" &gt; {log}\n        featureCounts -t exon -g gene_id -s 2 -p -B -C --largestOverlap --verbose -F GTF \\\n        -a resources/Scerevisiae.gtf -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}\n        echo \"Renaming output files\" &gt;&gt; {log}\n        mv {output.gene_level}.summary {output.gene_summary}\n        echo \"Results saved in &lt;{output.gene_level}&gt;\" &gt;&gt; {log}\n        echo \"Report saved in &lt;{output.gene_summary}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre></p> <p>Exercise: Finally, test the effect of the number of threads on the workflow\u2019s runtime. What command will you use to run the workflow? Does the workflow run faster?</p> Answer <p>The command to use is:</p> <p><code>snakemake --cores 4 -F -r -p results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv</code></p> <p>Do not forget to provide additional cores to Snakemake in the execution command with <code>--cores 4</code>. Note that the number of threads allocated to all jobs running at a given time cannot exceed the value specified with <code>--cores</code>. Therefore, if you leave this number at 1, Snakemake will not be able to use multiple threads. Also note that increasing <code>--cores</code> allows Snakemake to run multiple jobs in parallel (for example, running 2 jobs using 2 threads each). The workflow now takes ~6 min to run, compared to ~10 min before (i.e. a 40% decrease!). This gives you an idea of how powerful multi-threading is when the datasets and computing power get bigger!</p> <p>Explicit is better than implicit</p> <p>Even if a software cannot multi-thread, it is useful to add <code>threads: 1</code> in the rule to keep the rule consistency and clearly state that the software works with a single thread.</p> <p>Things to keep in mind when using parallel execution</p> <ul> <li>Parallel jobs will use more RAM. If you run out then either your OS will swap data to disk, or a process will crash</li> <li>The on-screen output from parallel jobs will be mixed, so save any output to log files instead</li> </ul>"},{"location":"course_material/day2/4_decorating_workflow/#using-non-file-parameters-and-config-files","title":"Using non-file parameters and config files","text":""},{"location":"course_material/day2/4_decorating_workflow/#non-file-parameters","title":"Non-file parameters","text":"<p>As we have seen, Snakemake\u2019s execution is based around inputs and outputs of each step of the workflow. However, a lot of software rely on additional non-file parameters. In the previous presentation and series of exercises, we advocated (rightfully so!) against using hard-coded filepaths. Yet, if you look back at the rules we have implemented, you will find 2 occurrences of this behaviour in the shell command:</p> <ul> <li>In the <code>rule read_mapping</code>, the index parameter <code>-x resources/genome_indices/Scerevisiae_index</code></li> <li>In the <code>rule reads_quantification_genes</code>, the annotation parameter <code>-a resources/Scerevisiae.gtf</code></li> </ul> <p>This reduces readability and also makes it very hard to change the value of these parameters.</p> <p>The <code>params</code> directive was designed for this purpose: it allows to specify additional parameters that can also depend on the wildcard values and use input functions (see Session 4 for more information on this). <code>params</code> values can be of any type (integer, string, list  etc\u2026) and similarly to the <code>{input}</code> and <code>{output}</code> placeholders, they can also be accessed from the shell command with the placeholder <code>{params}</code>. Just like for the <code>input</code> and <code>output</code> directives, you can define multiple parameters (in this case, do not forget the comma between each entry!) and they can be named (in practice, unknown parameters are unexplicit and easily confusing, so parameters should always be named!).</p> <p>It also helps readability and clarity to use the <code>params</code> section to name and assign parameters and variables for your shell command.</p> <p>Here is an example on how to use <code>params</code>:</p> <pre><code>rule example:\n    input:\n        'data/example.tsv'\n    output:\n        'results/example.txt'\n    params:\n        lines = 5\n    shell:\n        'head -n {params.lines} {input} &gt; {output}'\n</code></pre> <p>Parameters arguments</p> <p>In contrast to the <code>input</code> directive, the <code>params</code> directive can optionally take more arguments than only <code>wildcards</code>, namely <code>input</code>, <code>output</code>, <code>threads</code>, and <code>resources</code>.</p> <p>Exercise: Replace the two hard-coded paths mentioned earlier by <code>params</code>.</p> <p>Hint</p> <p>Add a <code>params</code> directive to the rules, name the parameter and replace the path by the placeholder in the shell command.</p> Answer <p>Note: for clarity, only the lines that changed are shown below.</p> <ul> <li><code>rule read_mapping</code></li> </ul> <pre><code>params:\n    index = 'resources/genome_indices/Scerevisiae_index'\nshell:\n    'hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n    -x {params.index} --threads {threads} \\\n    -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}'\n</code></pre> <ul> <li><code>rule reads_quantification_genes</code></li> </ul> <pre><code>params:\n    annotations = 'resources/Scerevisiae.gtf'\nshell:\n    'featureCounts -t exon -g gene_id -s 2 -p -B -C --largestOverlap --verbose -F GTF \\\n    -a {params.annotations} -T {threads} -o {output.gene_level} {input.bam_once_sorted} &amp;&gt;&gt; {log}'\n</code></pre> <p>Snakemake re-run behaviour</p> <p>If you try to re-run only the last rule with <code>snakemake --cores 4 -r -p -f results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv</code>, Snakemake will actually try to re-run 3 rules in total.</p> <p>This is because the code changed in 2 rules (see <code>reason</code> field in Snakemake\u2019s log), which triggered an update of the inputs in the 3rd rule (<code>sam_to_bam</code>). To avoid this, first <code>touch</code> the files with <code>snakemake --cores 1 --touch -F results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv</code> then re-run the last rule.</p>"},{"location":"course_material/day2/4_decorating_workflow/#config-files","title":"Config files","text":"<p>That being said, there is an even better way to handle parameters like the we just modified: instead of hard-coding parameter values in the Snakefile, Snakemake allows to define parameters and their values in config files. The config files will be parsed by Snakemake when executing the workflow, and parameters and their values will be stored in a Python dictionary named <code>config</code>. The path to the config file can be specified either in the Snakefile with the line <code>configfile: &lt;path/to/file.yaml&gt;</code> at the top of the file, or it can be specified at runtime with the execution parameter <code>--configfile &lt;path/to/file.yaml&gt;</code>.</p> <p>Config files are stored in the <code>config</code> subfolder and written in the JSON or YAML format. We will use the latter for this course as it is the most user-friendly and the recommended one. Briefly, in the YAML format, parameters are defined with the syntax <code>&lt;name&gt;: &lt;value&gt;</code>. Values can be strings, integers, floating points, booleans \u2026 For a complete overview of available value types, see this list. A parameter can have multiple values, which are then each listed on an indented single line starting with \u201c-\u201d. These values will be stored in a Python list when Snakemake parses the config file. Finally, parameters can be nested on indented single lines, and they will be stored as a dictionary when Snakemake parses the config file.</p> <p>The example below shows a parameter with a single value (<code>lines_number</code>), a parameter with multiple values (<code>samples</code>), and an example of nested parameters (<code>resources</code>):</p> <pre><code># Parameter with a single value (string, int, float, bool ...)\nlines_number: 5\n# Parameter with multiple values\nsamples:\n    - sample1\n    - sample2\n# Nested parameters\nresources:\n    threads: 4\n    memory: 4G\n</code></pre> <p>Then, each parameter can be accessed in Snakefile with the following syntax:</p> <pre><code>config['lines_number']  # --&gt; 5\nconfig['samples']  # --&gt; ['sample1', 'sample2']  # Lists of parameters become list\nconfig['resources']  # --&gt; {'threads': 4, 'memory': '4G'}  # Lists of named parameters become dictionaries\nconfig['resources']['threads']  # --&gt; 4\n</code></pre> <p>Accessing config values in <code>shell</code></p> <p>Values stored in the <code>config</code> dictionary cannot be accessed directly within the <code>shell</code> directive. If you need to use a parameter value in <code>shell</code>, define the parameter in <code>params</code> and assign its value from the <code>config</code> dictionary.</p> <p>Exercise: Create a config file in YAML format and fill it with adapted variables and values to replace the 2 hard-coded parameters in rules <code>read_mapping</code> and <code>reads_quantification_genes</code>. Then replace the hard-coded parameters by values from the config file and add its path on top of your Snakefile.</p> Answer <p>Note: for clarity, only the lines that changed are shown below. The first step is to create the subfolder and an empty config file:</p> <pre><code>mkdir config  # Create a new folder\ntouch config/config.yaml  # Create an empty config file\n</code></pre> <p>Then, fill the config file with the desired values:</p> <pre><code># Configuration options of RNAseq-analysis workflow\n\n# Location of the genome indices\nindex: 'resources/genome_indices/Scerevisiae_index'\n\n# Location of the annotation file\nannotations: 'resources/Scerevisiae.gtf'\n</code></pre> <p>Then, replace the <code>params</code> values in the Snakefile:</p> <ul> <li><code>rule read_mapping</code></li> </ul> <pre><code>params:\n    index = config['index']\n</code></pre> <ul> <li><code>rule reads_quantification_genes</code></li> </ul> <pre><code>params:\n    annotations = config['annotations']\n</code></pre> <p>Finally, add the file path on top of the Snakefile: <code>configfile: 'config/config.yaml'</code></p> <p>Now, if we need to change these values, we can easily do it in the config file instead of modifying the code!</p>"},{"location":"course_material/day2/4_decorating_workflow/#using-non-conventional-outputs","title":"Using non-conventional outputs","text":"<p>Snakemake has several built-in utilities to assign properties to outputs that are deemed \u2018special\u2019. These properties are listed in the table below:</p> Property Syntax Function Temporary <code>temp('path/to/file.txt')</code> File is deleted as soon as it is not required by any future jobs Protected <code>protected('path/to/file.txt')</code> File cannot be overwritten after the job ends (useful to prevent erasing a file by mistake) Ancient <code>ancient('path/to/file.txt')</code> File will not be re-created when running the pipeline (useful for files that require heavy computation) Directory <code>directory('path/to/directory')</code> Output is a directory instead of a file (use \u2018touch\u2019 instead if possible) Touch <code>touch('path/to/file.txt')</code> Create an empty flag file \u2018file.txt\u2019 regardless of the shell command (if the command finished without errors) <p>The next paragraphs will show how to use some of these properties.</p>"},{"location":"course_material/day2/4_decorating_workflow/#use-case-of-the-temp-command","title":"Use-case of the <code>temp()</code> command","text":"<p>Exercise: Can you think of a convenient use of <code>temp()</code> command?</p> Answer <p>The <code>temp()</code> command is extremely useful to automatically remove intermediary outputs that are no longer needed.</p> <p>Exercise: In your workflow, identify outputs that are intermediary and mark them as temporary with <code>temp()</code>.</p> Answer <p>The unsorted .bam and the .sam outputs seem like great candidates to be marked as temporary. One could also argue that the trimmed FASTQ files are also temporary, but we will keep them for now. Note: for clarity, only the lines that changed are shown below.</p> <ul> <li><code>rule read_mapping</code></li> </ul> <pre><code>output:\n    sam = temp('results/{sample}/{sample}_mapped_reads.sam'),\n</code></pre> <ul> <li><code>rule sam_to_bam</code></li> </ul> <pre><code>output:\n    bam = temp('results/{sample}/{sample}_mapped_reads.bam'),\n</code></pre> <p>Consequences of using <code>temp()</code></p> <p>Removing temporary outputs is a great way to save a lot of storage space. If you look at the size of your current <code>results/</code> folder (<code>du -bchd0 results/</code>), you will notice that it drastically. Just removing these two files would allow to save ~1 GB. While it may not seem a lot, remember that you usually have much bigger files and many more samples! On the other hand, using temporary outputs might force you to re-run more jobs than necessary if an input changes, so carefully think about it before using it.</p> <p>Exercise: On the contrary, is there a file of your workflow that you would like to protect with <code>protected()</code></p> Answer <p>This is debatable, but one could argue that the sorted .bam file is a good candidate for protection.</p> <ul> <li><code>rule sam_to_bam</code></li> </ul> <pre><code>output:\n    bam_sorted = protected('results/{sample}/{sample}_mapped_reads_sorted.bam'),\n</code></pre> <p>If you set this output as protected, be careful when you want to re-run your workflow and recreate the file!</p>"},{"location":"course_material/day2/4_decorating_workflow/#use-case-of-the-directory-command-the-fastqc-example","title":"Use-case of the <code>directory()</code> command: the FastQC example","text":"<p>FastQC is a program designed to spot potential problems in high-througput sequencing datasets. It is a very popular tool, notably because it runs quickly and does not require a lot of configuration. It runs a set of analyses on one or more raw sequence files in FASTQ or BAM format and produces a report with quality plots that summarises the results. It will highlight any areas where a dataset looks unusual and might require a closer look. As such, it would be interesting to run FastQC on the original .fastq files and the trimmed .fastq files to check whether trimminga actually improved the read quality. FastQC can be run interactively or in batch mode, during which it saves results as an HTML file and a ZIP file. We will soon see that running FastQC in batch mode presents a little problem.</p> <p>Data types and FastQC</p> <p>FastQC does not differentiate between sequencing techniques and as such can be used to look at libraries coming from a large number of experiments (Genomic Sequencing, ChIP-Seq, RNAseq, BS-Seq etc\u2026).</p> <p>If you run <code>fastqc -h</code>, you will notice something a bit surprising (but not unusual in bioinformatics):</p> <pre><code>    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the\n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n\n   -d --dir         Selects a directory to be used for temporary files written when\n                    generating report images. Defaults to system temp directory if\n                    not specified.\n</code></pre> <p>Two files are produced for each FASTQ file and these files appear in the same directory as the input file: FastQC does not allow to specify the names of the output files! However, we can set an alternative output directory, even though it needs to be created before FastQC is run.</p> <p>There are different solutions to deal with this problem:</p> <ol> <li>Work with the default file names produced by FastQC and leave the reports in the same directory than the input files</li> <li>Create the outputs in a new directory and leave the reports with their default name</li> <li>Create the outputs in a new directory and tell Snakemake that the directory itself is the output</li> <li>Force a naming convention by renaming the FastQC output files within the rule</li> </ol> <p>For the sake of time, we will not test all 4 solutions, but rather try to apply the 3<sup>rd</sup> or the 4<sup>th</sup> solution. We\u2019ll briefly summarise solutions 1 and 2 here:</p> <ol> <li>This could work, but it\u2019s better not to put the reports in the same directory than the input sequences. As a general principle, when writing Snakemake rules, we prefer to be in charge of the output names and to have all the files linked to a sample in the same directory</li> <li>This involves manually constructing the output directory path to use with the <code>-o</code> option, which works but isn\u2019t very convenient</li> </ol> <p>The base of the FastQC command is the following: <code>fastqc --format fastq --threads 2 &lt;input_fastq1&gt; &lt;input_fastq2&gt;</code></p> <ul> <li><code>-t/--threads</code>: specify the number of files which can be processed simultaneously. Here, it will be 2 because the inputs are paired-end files</li> <li>The <code>-o</code> and <code>-d</code> will be used in the last 2 solutions that we will now see in details</li> <li>We will create a single rule to run FastQC on both the original and the trimmed FASTQ files</li> </ul> <p>Choose only one solution to implement:</p> Solution 3Solution 4 <p>This option amounts to tell Snakemake not to worry about individual files at all and consider the output of the rule as an entire directory.</p> <p>Exercise: Implement a single rule to run FastQC on both the original and the trimmed FASTQ files (4 files in total) using directories as ouputs with the <code>directory()</code> command.</p> Answer <p>This makes the rule definition quite \u2018simple\u2019:</p> <pre><code>rule fastq_qc_sol3:\n    '''\n    This rule performs a QC on paired-end fastq files before and after trimming.\n    '''\n    input:\n        reads1 = rules.fastq_trim.input.reads1,\n        reads2 = rules.fastq_trim.input.reads2,\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        before_trim = directory('results/{sample}/fastqc_reports/before_trim/'),\n        after_trim = directory('results/{sample}/fastqc_reports/after_trim/')\n    log:\n        'logs/{sample}/{sample}_fastqc.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_fastqc.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    shell:\n        '''\n        echo \"Creating output directory &lt;{output.before_trim}&gt;\" &gt; {log}\n        mkdir -p {output.before_trim} 2&gt;&gt; {log}\n        echo \"Performing QC of reads before trimming in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {output.before_trim} \\\n        --dir {output.before_trim} {input.reads1} {input.reads2} &amp;&gt;&gt; {log}\n        echo \"Results saved in &lt;{output.before_trim}&gt;\" &gt;&gt; {log}\n        echo \"Creating output directory &lt;{output.after_trim}&gt;\" &gt;&gt; {log}\n        mkdir -p {output.after_trim} 2&gt;&gt; {log}\n        echo \"Performing QC of reads after trimming in &lt;{input.trim1}&gt; and &lt;{input.trim2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {output.after_trim} \\\n        --dir {output.after_trim} {input.trim1} {input.trim2} &amp;&gt;&gt; {log}\n        echo \"Results saved in &lt;{output.after_trim}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre> <p> <code>.snakemake_timestamp</code></p> <p>When <code>directory()</code> is used, Snakemake creates an empty file called <code>.snakemake_timestamp</code> in the output directory. This is the marker it uses to know if it needs to re-run the rule producing the directory.</p> <p>Overall, this rule works well and allows for an easy rule definition. However, in this case, individual files are not explicitly named as outputs and this may cause problems to chain rules later. Also, remember that some applications won\u2019t give you any control at all over the outputs, which is why you need a back-up plan, i.e. solution 4: the most powerful solution is to use shell commands to move and/or rename the files to the names you want. Also, the Snakemake developers advise to use <code>directory()</code> as a last resort and to rather use the <code>touch()</code> flag instead.</p> <p>This option amounts to let FastQC follows its default behaviour but force the renaming of the files afterwards to obtain the exact outputs we require.</p> <p>Exercise: Implement a single rule to run FastQC on both the original and the trimmed FASTQ files (4 files in total) and rename the files created by FastQC to precise output names using the <code>mv</code> command.</p> Answer <p>This makes the rule definition (much) more complicated than the other solution:</p> <pre><code>rule fastq_qc_sol4:\n    '''\n    This rule performs a QC on paired-end fastq files before and after trimming.\n    '''\n    input:\n        reads1 = rules.fastq_trim.input.reads1,\n        reads2 = rules.fastq_trim.input.reads2,\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2\n    output:\n        # QC before trimming\n        html1_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_1.html',\n        zipfile1_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_1.zip',\n        html2_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_2.html',\n        zipfile2_before = 'results/{sample}/fastqc_reports/{sample}_before_trim_2.zip',\n        # QC after trimming\n        html1_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_1.html',\n        zipfile1_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_1.zip',\n        html2_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_2.html',\n        zipfile2_after = 'results/{sample}/fastqc_reports/{sample}_after_trim_2.zip'\n    params:\n        wd = 'results/{sample}/fastqc_reports/',\n        # QC before trimming\n        html1_before = 'results/{sample}/fastqc_reports/{sample}_1_fastqc.html',\n        zipfile1_before = 'results/{sample}/fastqc_reports/{sample}_1_fastqc.zip',\n        html2_before = 'results/{sample}/fastqc_reports/{sample}_2_fastqc.html',\n        zipfile2_before = 'results/{sample}/fastqc_reports/{sample}_2_fastqc.zip',\n        # QC after trimming\n        html1_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_1_fastqc.html',\n        zipfile1_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_1_fastqc.zip',\n        html2_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_2_fastqc.html',\n        zipfile2_after = 'results/{sample}/fastqc_reports/{sample}_atropos_trimmed_2_fastqc.zip'\n    log:\n        'logs/{sample}/{sample}_fastqc.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_atropos_fastqc.txt'\n    resources:\n        mem_gb = 1\n    threads: 2\n    shell:\n        '''\n        echo \"Performing QC of reads before trimming in &lt;{input.reads1}&gt; and &lt;{input.reads2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {params.wd} \\\n        --dir {params.wd} {input.reads1} {input.reads2} &amp;&gt;&gt; {log}\n        echo \"Renaming results from original fastq analysis\" &gt;&gt; {log}  # Renames files because we can't choose fastqc output\n        mv {params.html1_before} {output.html1_before} 2&gt;&gt; {log}\n        mv {params.zipfile1_before} {output.zipfile1_before} 2&gt;&gt; {log}\n        mv {params.html2_before} {output.html2_before} 2&gt;&gt; {log}\n        mv {params.zipfile2_before} {output.zipfile2_before} 2&gt;&gt; {log}\n        echo \"Performing QC of reads after trimming in &lt;{input.trim1}&gt; and &lt;{input.trim2}&gt;\" &gt;&gt; {log}\n        fastqc --format fastq --threads {threads} --outdir {params.wd} \\\n        --dir {params.wd} {input.trim1} {input.trim2} &amp;&gt;&gt; {log}\n        echo \"Renaming results from trimmed fastq analysis\" &gt;&gt; {log}  # Renames files because we can't choose fastqc output\n        mv {params.html1_after} {output.html1_after} 2&gt;&gt; {log}\n        mv {params.zipfile1_after} {output.zipfile1_after} 2&gt;&gt; {log}\n        mv {params.html2_after} {output.html2_after} 2&gt;&gt; {log}\n        mv {params.zipfile2_after} {output.zipfile2_after} 2&gt;&gt; {log}\n        echo \"Results saved in &lt;results/{wildcards.sample}/fastqc_reports/&gt;\" &gt;&gt; {log}\n        '''\n</code></pre> <p>This solution is very long and much more complicated than the other one. However, it makes up for the complexity by allowing a total control on what is happening: with this method, we can choose where the temporary files are saved and the names of the outputs. It could have been shortened by using <code>-o .</code> to tell FastQC to create the files in the current working directory instead of a specific one, but this would have created another problem: if we run multiple jobs in parallel, then Snakemake may potentially try to produce files from different jobs but with the same temporary destination. In this case, the different instances would be trying to write to the same temporary files at the same time, overwriting each other and corrupting the output files.</p> <p>Several interesting things are happening in both versions of this rule:</p> <ul> <li>Much like for the outputs, it is possible to refer to the inputs of a rule directly in another rule with the syntax <code>rules.&lt;rule_name&gt;.input.&lt;input_name&gt;</code></li> <li>FastQC doesn\u2019t create the output directory by itself (other programs might insist that the output directory does not already exist), so we have to create it manually  with <code>mkdir</code> in the shell command before running FastQC</li> <li>The <code>-p</code> flag of <code>mkdir</code> make parent directories as needed and does not return an error if the directory already exists</li> </ul> <p>Directory creation</p> <p>Remember that in most cases it is not necessary to manually create directories because Snakemake will do it for you. Even when using a <code>directory(</code>) output, Snakemake will not create the directory itself but most applications will make the directory for you; FastQC is an exception.</p> <p>Hint</p> <p>If you want to make sure that a certain rule is executed before another, you can write the outputs of the first rule as inputs of the second one, even if you don\u2019t use them in the rule. For example, we could force the execution of FastQC before mapping the reads with only a few modifications to <code>rule read_mapping</code>:</p> <pre><code>rule read_mapping:\n    '''\n    This rule maps trimmed reads of a fastq on a reference assembly.\n    '''\n    input:\n        trim1 = rules.fastq_trim.output.trim1,\n        trim2 = rules.fastq_trim.output.trim2,  # Do not forget to add a comma here\n        fastqc = rules.fastq_qc_sol4.output.html1_before  # This single line will force the execution of FASTQC before read mapping\n    output:\n        sam = 'results/{sample}/{sample}_mapped_reads.sam',\n        report = 'results/{sample}/{sample}_mapping_report.txt'\n    params:\n        index = 'resources/genome_indices/Scerevisiae_index'\n    log:\n        'logs/{sample}/{sample}_mapping.log'\n    benchmark:\n        'benchmarks/{sample}/{sample}_mapping.txt'\n    resources:\n        mem_gb = 2\n    threads: 4\n    shell:\n        '''\n        echo \"Mapping the reads\" &gt; {log}\n        hisat2 --dta --fr --no-mixed --no-discordant --time --new-summary --no-unal \\\n        -x {params.index} --threads {threads} \\\n        -1 {input.trim1} -2 {input.trim2} -S {output.sam} --summary-file {output.report} 2&gt;&gt; {log}\n        echo \"Mapped reads saved in &lt;{output.sam}&gt;\" &gt;&gt; {log}\n        echo \"Mapping report saved in &lt;{output.report}&gt;\" &gt;&gt; {log}\n        '''\n</code></pre>"},{"location":"course_material/day2/4_decorating_workflow/#modularising-a-workflow","title":"Modularising a workflow","text":"<p>If you keep developing a workflow long enough, you are bound to encounter some cluttering problems. Have a look at your current Snakefile: with only 5 rules, it is already almost 200 lines long. Imagine what happens when your workflow comprises dozens of rules?! The <code>Snakefile</code> may become messy and harder to maintain and edit. This is why it quickly becomes crucial to modularise your workflow; this is a common practice in programming in general. This approach also makes it easier to re-use pieces of workflow in the future. Modularisation comes at 4 different levels:</p> <ol> <li>The most fine-grained level are wrappers. Wrappers allow to quickly use popular tools and libraries in Snakemake workflows, thanks to the <code>wrapper</code> directive. Wrappers are automatically downloaded and deploy a conda environment when running the workflow, which increases reproducibility, however their implementation can sometimes be \u2018rigid\u2019 and you may have to write your own rule. See the official documentation for more explanations</li> <li>For larger, reusable parts belonging to the same workflow, it is recommended to write smaller snakefiles and include them into a main Snakefile with the <code>include</code> statement. Note that in this case, all rules share a common config file. See the official documentation for more explanations</li> <li>The next level of modularisation is provided via the <code>module</code> statement, which enables arbitrary combination and re-use of rules in the same workflow and between workflows. See the official documentation for more explanations</li> <li>Finally, Snakemake also provides a syntax to define subworkflows, but this syntax is currently being deprecated in favor of the <code>module</code> statement. See the official documentation for more explanations</li> </ol> <p>In this course, we will only use the 2<sup>nd</sup> level of modularisation. In more details, the idea is to write a main Snakefile in <code>workflow/Snakefile</code>, to place the other snakefiles containing the rules in the subfolder <code>workflow/rules</code> (these \u2018sub-Snakefile\u2019 should end with <code>.smk</code>, the recommended file extension of Snakemake) and to tell Snakemake to import the modular snakefiles in the main Snakefile with the <code>include: &lt;path/to/snakefile.smk&gt;</code> syntax.</p> <p>Rules organisation</p> <p>How to organize rules is up to you, but a common approach would be to create \u201cthematic\u201d modules, i.e. regroup rules involved in the same general step of the workflow.</p> <p>Exercise: Move your current Snakefile into the subfolder <code>workflow/rules</code> and rename it to <code>read_mapping.smk</code>. Then create a new Snakefile in <code>workflow/</code> and import <code>read_mapping.smk</code> in it using the <code>include</code> syntax. You should also move the importation of the config file from the modular Snakefile to the main one.</p> Answer <p>We will solve this problem step by step. First, create the new file structure:</p> <pre><code>mkdir workflow/rules  # Create a new folder\nmv workflow/Snakefile workflow/rules/read_mapping.smk  # Move and rename the modular snakefile\ntouch workflow/Snakefile  # Recreate the main Snakefile\n</code></pre> <p>Then, fill the main Snakefile with <code>include</code> and <code>configfile</code>:</p> <pre><code>'''\nMain Snakefile of the RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Path of the config file\nconfigfile: 'config/config.yaml'\n\n# Rules to execute the workflow\ninclude: 'rules/read_mapping.smk'\n</code></pre> <p>Finally, do not forget to remove the config file import (<code>configfile: 'config/config.yaml'</code>) from the snakefiles (<code>workflow/rules/read_mapping.smk</code>)</p> <p>Relative paths</p> <ul> <li>Includes are relative to the directory of the Snakefile in which they occur. For example, if the Snakefile resides in <code>workflow</code>, then Snakemake will search for the included snakefiles in <code>workflow/path/to/other/snakefile</code>, regardless of the working directory</li> <li>You can place snakefiles in a sub-directory without changing input and output paths, as these paths are relative to the working directory. However, you will need to edit paths to external scripts and conda environments, as these paths are relative to the snakefile from which they are called (this will be discussed in the last series of exercises)</li> </ul> <p>In practice, you can imagine that the line <code>include: &lt;path/to/snakefile.smk&gt;</code> is replaced by the entire content of <code>snakefile.smk</code> in <code>Snakefile</code>. This means that syntaxes like <code>rules.&lt;rule_name&gt;.output.&lt;output_name&gt;</code> can still be used in snakefiles, even if the rule <code>&lt;rule_name&gt;</code> was defined in another snakefile, as long as the snakefile in which <code>&lt;rule_name&gt;</code> is defined is included before the snakefile that uses <code>rules.&lt;rule_name&gt;.output</code>. This also works for input and output functions.</p>"},{"location":"course_material/day2/4_decorating_workflow/#using-a-target-rule-and-aggregating-outputs","title":"Using a target rule and aggregating outputs","text":""},{"location":"course_material/day2/4_decorating_workflow/#creating-a-target-rule","title":"Creating a target rule","text":"<p>Modularisation also offers a great opportunity to facilitate the execution of the workflow. By default, if no target is given at the command line, Snakemake executes the first rule in the Snakefile. Hence, we have always executed the workflow by specifying a target file in the command line to avoid this behaviour. But we can actually use this property to make the execution easier by writing a pseudo-rule (also called target-rule and usually named <code>rule all</code>) in the Snakefile which has all the desired outputs (or a particular subsets of them) files as input files. This rule will look like this:</p> <pre><code>rule all:\n    input:\n        'path/to/ouput1',\n        'path/to/ouput2'\n</code></pre> <p>Order of rules in Snakefile/snakefiles</p> <p>Apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile/snakefiles is arbitrary and does not influence the DAG of jobs.</p> <p>Exercise: Implement a special rule in the Snakefile so that the final output is generated by default when running <code>snakemake</code> without specifying a target, then test your workflow with a dry-run.</p> <p>Hint</p> <ul> <li>Remember that a rule is not required to have an output nor a shell command</li> <li>The inputs of <code>rule all</code> should be the final outputs that you want to generate (those from the last rule you wrote)</li> </ul> Answer <p>If we consider that the last outputs are the ones produced by <code>rule reads_quantification_genes</code>, we can write the target rule like this:</p> <pre><code># Master rule that launches the workflow\nrule all:\n    '''\n    Dummy rule to automatically generate the required outputs.\n    '''\n    input:\n        'results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv'\n</code></pre> <p>Note that we used only one of the two outputs of <code>rule reads_quantification_genes</code>. We do this because it is enough to trigger the execution and if the rule didn\u2019t produce both outputs, Snakemake would crash and report it this error.</p> <p>Now, let\u2019s try to do a dry-run with this new rule: <code>snakemake --cores 4 -F -r -p -n</code>. You should see all the rules appearing thanks to the <code>-F</code> flag, including:</p> <pre><code>localrule all:\n    input: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n    jobid: 0\n    reason: Input files updated by another job: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv\n    resources: tmpdir=/tmp\n\nJob stats:\njob                           count\n--------------------------  -------\nall                               1\nfastq_qc_sol4                     1\nfastq_trim                        1\nread_mapping                      1\nreads_quantification_genes        1\nsam_to_bam                        1\ntotal                             6\n</code></pre>"},{"location":"course_material/day2/4_decorating_workflow/#aggregating-outputs","title":"Aggregating outputs","text":"<p>Using a target rule like the one presented in the previous paragraph gives another opportunity to make things easier. In the rule we just created, we used a hard-coded input and by now, you should know that this is not an optimal solution and that we should avoid this as much as possible, especially if you have many samples to process. To solve this problem, we will rely on the expand function.</p> <p>Exercise: Write an <code>expand()</code> syntax to generate a list of outputs from <code>rule reads_quantification_genes</code> with all the RNAseq samples. What do you need to write this?</p> Answer <p>The output of <code>rule reads_quantification_genes</code> has the following syntax: <code>'results/{sample}/{sample}_genes_read_quantification.tsv'</code>.</p> <p>First, we need to create a Python list containing all the values that the <code>{sample}</code> wildcards can take:</p> <p><code>SAMPLES = ['highCO2_sample1', 'highCO2_sample2', 'highCO2_sample3', 'lowCO2_sample1', 'lowCO2_sample2', 'lowCO2_sample3']</code></p> <p>Then, we can transform the output syntax with <code>expand()</code>:</p> <p><code>expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=SAMPLES)</code></p> <p>Exercise: Use these two elements (the list of samples and the <code>expand()</code> syntax) in the target rule to ask Snakemake to generate all the outputs.</p> Answer <p>You need to add the sample list to the Snakefile before the <code>rule all</code> and replace the value of the <code>input</code> directive:</p> <pre><code># Sample list\nSAMPLES = ['highCO2_sample1', 'highCO2_sample2', 'highCO2_sample3', 'lowCO2_sample1', 'lowCO2_sample2', 'lowCO2_sample3']\n\n# Master rule that launches the workflow\nrule all:\n    '''\n    Dummy rule to automatically generate the required outputs.\n    '''\n    input:\n        expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=SAMPLES)\n</code></pre> <p>If you launch the workflow in dry-run mode with this new rule: <code>snakemake --cores 4 -F -r -p -n</code>. You should see all the rules appearing 5 times (1 for each sample that hasn\u2019t been processed yet):</p> <pre><code>localrule all:\n    input: results/highCO2_sample1/highCO2_sample1_genes_read_quantification.tsv, results/highCO2_sample2/highCO2_sample2_genes_read_quantification.tsv, results/highCO2_sample3/highCO2_sample3_genes_read_quantification.tsv, results/lowCO2_sample1/lowCO2_sample1_genes_read_quantification.tsv, results/lowCO2_sample2/lowCO2_sample2_genes_read_quantification.tsv, results/lowCO2_sample3/lowCO2_sample3_genes_read_quantification.tsv\n    jobid: 0\n    reason: Input files updated by another job: results/lowCO2_sample1/lowCO2_sample1_genes_read_quantification.tsv, results/lowCO2_sample2/lowCO2_sample2_genes_read_quantification.tsv, results/lowCO2_sample3/lowCO2_sample3_genes_read_quantification.tsv, results/highCO2_sample3/highCO2_sample3_genes_read_quantification.tsv, results/highCO2_sample2/highCO2_sample2_genes_read_quantification.tsv\n    resources: tmpdir=/tmp\n\nJob stats:\njob                           count\n--------------------------  -------\nall                               1\nfastq_qc_sol4                     5\nfastq_trim                        5\nread_mapping                      5\nreads_quantification_genes        5\nsam_to_bam                        5\ntotal                            26\n</code></pre> <p>But we can do even better! At the moment, samples are defined in a list at the top of the Snakefile. To further improve the workflow\u2019s usability, we can define samples in the config file, so they can easily be added, removed, or modified by the user.</p> <p>Exercise: Implement a parameter in the config file to specify sample names and modify <code>rule all</code> to use this parameter in the <code>expand()</code> syntax.</p> Answer <p>First, we need to modify the config file:</p> <pre><code># Configuration options of RNAseq-analysis workflow\n\n# Location of the genome indices\nindex: 'resources/genome_indices/Scerevisiae_index'\n\n# Location of the annotation file\nannotations: 'resources/Scerevisiae.gtf'\n\n# Sample names\nsamples:\n  - highCO2_sample1\n  - highCO2_sample2\n  - highCO2_sample3\n  - lowCO2_sample1\n  - lowCO2_sample2\n  - lowCO2_sample3\n</code></pre> <p>Then, we need to use the config file in the <code>expand()</code> syntax (and remove <code>SAMPLES</code> from the Snakefile, because we don\u2019t need this variable anymore):</p> <pre><code># Master rule that launches the workflow\nrule all:\n    '''\n    Dummy rule to automatically generate the required outputs.\n    '''\n    input:\n        expand('results/{sample}/{sample}_genes_read_quantification.tsv', sample=config['samples'])\n</code></pre> <p>Here, <code>config['samples']</code> is a Python list containing strings, each string being a sample name. This is because a list of parameters become a list during the config file parsing.</p> <p>An even more Snakemake-idiomatic solution</p> <p>There is an even better and more Snakemake-idiomatic version of the <code>expand()</code> syntax:</p> <p><code>expand(rules.reads_quantification_genes.output.gene_level, sample=config['samples'])</code>.</p> <p>While it may not seem easy to use and understand, this entirely removes the need to write the output paths!</p>"},{"location":"course_material/day2/4_decorating_workflow/#running-the-other-samples-of-the-workflow","title":"Running the other samples of the workflow","text":"<p>Exercise: Touch the files already present in your workflow to avoid re-creating them and then run your workflow on the 5 other samples.</p> Answer <ul> <li>Touch the existing files: <code>snakemake --cores 1 --touch</code></li> <li>Run the workflow <code>snakemake --cores 4 -r -p</code></li> </ul> <p>Thanks to the parallelisation, the workflow execution should take less than 10 min in total to process all the samples!</p> <p>Exercise: Generate the workflow DAG and filegraph.</p> Answer <ul> <li>Generate the DAG: <code>snakemake --cores 1 -F -r -p --rulegraph | dot -Tpng &gt; images/all_samples_rulegraph.png</code></li> <li>Generate the filegraph: <code>snakemake --cores 1 -F -r -p --filegraph | dot -Tpng &gt; images/all_samples_filegraph.png</code></li> </ul> <p>Your DAG should resemble this:</p> <p>And this should be your filegraph (open the picture in a new tab to zoom in):</p> <p></p>"},{"location":"course_material/day2/5_reproducibility_snakemake/","title":"Being reproducible with Snakemake","text":""},{"location":"course_material/day2/5_reproducibility_snakemake/#learning-outcomes","title":"Learning outcomes","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Create and use an input function</li> <li>Deploy a conda environment and run scripts within it</li> <li>Deploy a Docker/Singularity container and run scripts within it</li> </ul>"},{"location":"course_material/day2/5_reproducibility_snakemake/#material","title":"Material","text":"<p>Reproducibility in Snakemake:</p> <p> Download the presentation</p> <p>Additional advanced concepts:</p> <p> Download the presentation</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#exercises","title":"Exercises","text":"<p>In this series of exercises, we will create the last two rules of the workflow. Each rule will execute a script, one in Python and one in R, and both rules will have dedicated environment that you will need to take into account in the snakefiles. These last two rules are quite different from the previous ones, so it would be a good idea to implement them in a new snakefile in <code>workflow/rules</code>, for example called <code>analysis.smk</code>.</p> <p>Development and back-up</p> <p>During this session, we will modify our snakefiles quite heavily, so it may be a good idea to start by making a back-up: <code>cp -r worklow/ worklow_backup</code>. As a general rule, if you have a doubt on the code you are developing, do not hesitate to make a back-up.</p> <p>Hint</p> <p>This is not a programming course, so you won\u2019t need to write the scripts: they were already prepared for you!</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#creating-a-rule-to-gather-read-count-files","title":"Creating a rule to gather read count files","text":"<p>To perform a Differential Expression Analysis (DEA), it is easier to have a single file gathering all the read counts of the different samples.</p> <p>Exercise: Implement a rule to list and merge read count files (coming from <code>rule reads_quantification_genes</code>) into a single file using the Python script provided [here]https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/scripts/solutions/day2/session4/workflow/scripts/count_table.py).</p> <p>Information on the script to compute the table</p> <ul> <li>You can download the script with <code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/scripts/solutions/day2/session4/workflow/scripts/count_table.py</code></li> <li>It goes in a special directory in your workflow</li> <li>It is written in Python</li> <li>It takes a list of files as input, each file being the read count output of featureCounts</li> <li>It produces one output, a tab-separated table containing all the read counts of the different sample gathered by gene</li> </ul> <p>Hint</p> <p>While the goal of this rule is quite easy to grasp, setting it up requires using several advanced notions of Snakemake, so here is a little outline of the steps you should take:</p> <ol> <li>Build the basic structure of your rule: name, output, log, benchmark<ul> <li>Memory should be set at 500 MB</li> <li>Threads should be set at 1</li> <li>Don\u2019t focus on the inputs for now</li> </ul> </li> <li>Think about the directive you want to use to run the script<ul> <li>Looking at the script length with <code>wc -l &lt;path/to/script&gt;</code> could help you decide</li> </ul> </li> <li>Think about the location/path of the script</li> <li>Check the beginning of the script to see if you need any special Python packages. You can do that with <code>head &lt;path/to/script&gt;</code>. If you see lines containing <code>import &lt;package_name&gt;</code>, it means that the script is using external Python packages<ul> <li>If the script is using external packages, think on how can you provide them</li> </ul> </li> <li>Finally, identify the inputs your rule needs<ul> <li>Think on how you can easily list and provide the inputs. Does it remind you of something you saw in the course?</li> </ul> </li> </ol> <p>Now, let\u2019s solve these problems one by one!</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#building-the-rule-structure","title":"Building the rule structure","text":"<p>You have done that a few times already, so it should not be too difficult. </p> <p>Exercise: Set the output, log, benchmark, resources and thread values.</p> Answer <pre><code>rule count_table:\n    '''\n    This rule merges all the gene count tables of an assembly into one table.\n    '''\n    input:\n        ?\n    output:\n        count_table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    benchmark:\n        'benchmarks/total_count_table.txt'\n    resources:\n        mem_mb = 500\n    threads: 1\n    ?:\n        ?\n</code></pre> <p>Note that we do not need to use <code>wildcards</code> in the output name: only one file will be created, and its name will not change depending on the sample name, because we use all the samples to create it.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#getting-the-python-script-and-running-it","title":"Getting the Python script and running it","text":"<p>Exercise: Download the script and place it the proper folder. Remember that per the official documentation, scripts should be stored in a subfolder <code>workflow/scripts</code>.</p> Answer <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/scripts/solutions/day2/session4/workflow/scripts/count_table.py  # Download the script\nmkdir -p workflow/scripts  # Create the appropriate folder\nmv count_table.py workflow/scripts  # Move the script in the newly created folder\n</code></pre> <p>Exercise: Check how long the script is.</p> Answer <p>If you run <code>wc -l workflow/scripts/count_table.py</code>, you will see that the script is 67 lines long. This is too much to use a <code>run</code> directive, so we will use the <code>script</code> directive instead. This means that we need to add the following to our <code>rule count_table</code>: <pre><code>script:\n    '../scripts/count_table.py'\n</code></pre></p> <p>Script path</p> <p>If you placed included files in subfolders (like <code>rules/analysis.smk</code>), you need to change relative paths for external script files, hence the <code>../</code> in the script path.</p> <p>Hint</p> <p>In many cases, it would be nice to have a script that can be called by Snakemake but also work with standard Python, so that the code can be reused in other projects. There are several ways to do that:</p> <ul> <li>You could implement most of the functionalities in a module and use this module in a simple script called by Snakemake</li> <li>You could test for the existence of a <code>snakemake</code> object and handle parameter values differently (e.g. command-line arguments) if the object does not exist</li> </ul> <p>Inside the script, you have access to an object <code>snakemake</code> that provides access to the same objects that are available in the <code>run</code> and <code>shell</code> directives (input, output, params, wildcards, log, threads, resources, config), e.g. you can use <code>snakemake.input[0]</code> to access the first input file of a rule, or <code>snakemake.input.input_name</code> to access a specific named input.</p> <p>Exercise: Check the script content to see whether it requires specific packages.</p> Answer <p>If you run <code>head workflow/scripts/count_table.py</code>, you will see several <code>import</code> commands at the start of the script, including <code>import pandas as pd</code>. <code>pandas</code> is a great package, but it is not part of the default packages natively shipped with Python. This means that we need to find a solution to provide it to the rule. The easiest way to do that is to create a conda environment dedicated to the rule. Conda environments should be stored in a subfolder <code>workflow/envs</code>.</p> <p>Create the appropriate folder: <code>mkdir -p workflow/envs</code>. Then, write the following configuration in the environment file, <code>workflow/envs/py.yaml</code>: <pre><code># Environment file to perform data processing with python\nname : python\nchannels :\n    - conda-forge\n    - bioconda\ndependencies :\n    - python &gt;= 3.10\n    - pandas == 1.4.3\n</code></pre></p> <p>This means that you need to add the following to your <code>rule count_table</code>: <pre><code>conda:\n    '../envs/py.yaml'\n</code></pre></p> <p>Environment file path</p> <p>If you placed included files in subfolders (like <code>rules/analysis.smk</code>), you need to change relative paths for conda environments files as well, hence the <code>../</code> in the environment file path.</p> <p>Using conda environments improves reproducibility for many reasons, including version control and the fact that users do not need to manually manage software dependencies. Note: the first execution of the workflow after adding Conda environments will take some time, because Conda will have to download and install all the software.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#identifying-and-listing-the-input-files","title":"Identifying and listing the input files","text":"<p>The before-last step is the most complex one: identifying all the inputs of the rule and gathering them in a list. Here, there are only 6 samples, so in theory, we could list them directly\u2026 However, by now you should that is not a good solution. Fortunately, there is a much more elegant and convenient way to do this: an input function, which provides the added benefit of scaling up very well if the number of samples increase.</p> <p>We already wrote the input function for you:</p> <pre><code># Input function used in rule count_table\ndef get_gene_counts(wildcards):\n    '''\n    This function lists all the gene count tables of samples in the config file\n    '''\n    # Note that here {sample} is not a wildcard, it is an f-string variable!\n    return [f\"results/{sample}/{sample}_genes_read_quantification.tsv\"\n            for sample in config['samples']]\n</code></pre> <p>This function will loop over the list of samples in the config file, replace \u2018{sample}\u2019 with the current sample name of the iteration to create a string which is the output path from the <code>rule reads_quantification_genes</code> of said sample. Then, it will aggregate all the paths in a list and return this list.</p> <p>Details on input functions</p> <ul> <li>Input functions take the <code>wildcards</code> global object as single argument</li> <li>You can access wildcard values inside an input function with the syntax <code>{wildcards.wildcards_name}</code></li> <li>Input and output functions can return a list of files, which will then be automatically handled like multiple inputs or outputs by Snakemake. These functions can also return a dictionary; in this case, the function should be called with the syntax <code>input: unpack(&lt;function_name&gt;)</code>. The dictionary\u2019s keys will be interpreted as input/output names and the dictionary\u2019s values will be interpreted as input/output values</li> <li>Functions are evaluated before the workflow is executed. As a consequence, they cannot be used to list the content of an output directory, since the directory does not exist before the workflow is executed!</li> </ul> <p>Exercise: Insert the function in the proper Snakefile and adapt the input value of the rule accordingly.</p> Answer <p>There are two things to do: * Insert the input function in <code>workflow/rules/analysis.smk</code>, before the rule, otherwise you will get a <code>name 'get_gene_counts' is not defined</code> error (the function needs to be defined before Snakemake looks for it when it parses the rule input). * Use the function name as value for the input directive</p> <p>Your rule and function should resemble this: <pre><code># Input function used in rule count_table\ndef get_gene_counts(wildcards):\n    '''\n    This function lists all the gene count tables of samples in the config file\n    '''\n    return [f\"results/{sample}/{sample}_genes_read_quantification.tsv\"\n            for sample in config['samples']]\n</code></pre></p> <pre><code>rule count_table:\n    '''\n    This rule merges all the gene count tables of an assembly into one table.\n    '''\n    input:\n        get_gene_counts\n    output:\n        count_table = 'results/total_count_table.tsv'\n    log:\n        'logs/total_count_table.log'\n    benchmark:\n        'benchmarks/total_count_table.txt'\n    conda:\n        '../envs/py.yaml'\n    resources:\n        mem_mb = 500\n    threads: 1\n    script:\n        '../scripts/count_table.py'\n</code></pre> <p>Input functions usage</p> <p>You don\u2019t need to use parentheses or specify any argument when you call an input function in the <code>input</code> directive.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#adapting-the-snakefile-and-running-the-rule","title":"Adapting the Snakefile and running the rule","text":"<p>Now, all that is left is to run the rule to create the table.</p> <p>Exercise: Which command should you use to create the output? Is there anything else to do beforehand?</p> Answer <p>It turns out that we cannnot launch the workflow directly: we need to include the new rule file in the Snakefile and adapt the output of the <code>rule all</code>! Your Snakefile should now resemble this:</p> <pre><code>'''\nMain Snakefile of the RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Path of the config file\nconfigfile: 'config/config.yaml'\n\n# Rules to execute the workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule that launches the workflow\nrule all:\n    '''\n    Dummy rule to automatically generate the required outputs.\n    '''\n    input:\n        'results/total_count_table.tsv'\n</code></pre> <p>Finally, run the workflow with <code>snakemake --cores 1 -r -p --use-conda</code></p> <p>\u2013use-conda</p> <p>Do not forget <code>--use-conda</code> otherwise Snakemake will not use the environments!!!</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#creating-a-rule-to-detect-differentially-expressed-genes","title":"Creating a rule to detect Differentially Expressed Genes","text":"<p>It is now time to write the final rule of the workflow. This rule will perform the DEA using the global count table we previously created.</p> <p>Exercise: Implement a rule to perform DEA using the R script provided here.</p> <p>Information on the script to compute the table</p> <ul> <li>You can download the script with <code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/scripts/solutions/day2/session4/workflow/scripts/DESeq2.R</code></li> <li>It goes in a special directory in your workflow</li> <li>It is written in R</li> <li>It takes a global read counts table as input</li> <li>It produces two outputs:<ol> <li>A tab-separated table containing the DEG and the associated statistical results</li> <li>A pdf file containing control plots of the analysis</li> </ol> </li> </ul> <p>Hint</p> <p>While not being trivial, this rule is much easier than the previous one and some things work similarly. Still, here is a little outline of the steps you should take:</p> <ol> <li>Build the basic structure of your rule: name, input, outputs, log, benchmark<ul> <li>Memory should be set at 1 GB</li> <li>Threads should be set at 2</li> </ul> </li> <li>Think about the directive you want to use to run the script<ul> <li>Remember than there is only one way to run an R script</li> </ul> </li> <li>Think about the location/path of the script</li> <li>This script is using a lot of external packages. Fortunately, all these packages are available in a certain Docker image you worked with yesterday</li> </ol> <p>Now, let\u2019s write this last rule!</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#building-the-rule-structure_1","title":"Building the rule structure","text":"<p>You have done that a few times already, so it should not be too difficult. </p> <p>Exercise: Set the input, outputs, log, benchmark, resources and thread values.</p> Answer <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots associated visual control graphs (PCA,\n    heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    benchmark:\n        'benchmarks/differential_expression.txt'\n    ?:\n        ?\n    resources:\n        mem_gb = 1\n    threads: 2\n    ?:\n        ?\n</code></pre> <p>Note that we do not need to use <code>wildcards</code> in this rule, because all the files are precisely defined.</p>"},{"location":"course_material/day2/5_reproducibility_snakemake/#getting-the-r-script-and-running-it","title":"Getting the R script and running it","text":"<p>Exercise: Download the script and place it the proper folder. Remember that per the official documentation, scripts should be stored in a subfolder <code>workflow/scripts</code>.</p> Answer <pre><code>wget https://raw.githubusercontent.com/sib-swiss/containers-snakemake-training/main/scripts/solutions/day2/session4/workflow/scripts/DESeq2.R  # Download the script\nmv DESeq2.R workflow/scripts  # Move the script in the newly created folder\n</code></pre> <p>Exercise: Find a way to run the script.</p> Answer <p>There is only one way to run an R script: use the <code>script</code> directive. This means that we need to add the following to our <code>rule differential_expression</code>: <pre><code>script:\n    '../scripts/DESeq2.R'\n</code></pre></p> <p>Script path</p> <p>If you placed included files in subfolders (like <code>rules/analysis.smk</code>), you need to change relative paths for external script files, hence the <code>../</code> in the script path.</p> <p>Hint</p> <p>Inside the script, an S4 object named <code>snakemake</code> analogous to the Python case available and allows access to input and output files and other parameters. Here the syntax follows that of S4 classes with attributes that are R lists, e.g. you can access the first input file with <code>snakemake@input[[1]]</code> (note that the first file does not have index 0 here, because R starts counting from 1). Named input and output files can be accessed in the same way, by just providing the name instead of an index, e.g. <code>snakemake@input[[\"myfile\"]]</code>.</p> <p>Exercise: Find an efficient way to create a computing environment for the rule.</p> <p>Hint</p> <p>Remember what you did during Day 1, session 3 \u201cWorking with Dockerfiles\u201d!</p> Answer <p>During Day 1, you built your own docker image, called deseq2. This image actually contains everything we need to run DEA, so let\u2019s use it again, but with Snakemake this time! This means that you need to add the following to your <code>rule differential_expression</code>: <pre><code>container:\n    'docker://geertvangeest/deseq2:v1'\n</code></pre></p> <p>Your own Docker image</p> <p>First try with your own image. If it doesn\u2019t work, then you can use Geert\u2019s image: <code>geertvangeest/deseq2:v1</code>.</p> <p>After all these modifcations, this is what your final rule should look like:</p> Answer <pre><code>rule differential_expression:\n    '''\n    This rule detects DEGs and plots associated visual control graphs (PCA,\n    heatmaps...).\n    '''\n    input:\n        table = rules.count_table.output.table\n    output:\n        deg = 'results/deg_list.tsv',\n        pdf = 'results/deg_plots.pdf'\n    log:\n        'logs/differential_expression.log'\n    benchmark:\n        'benchmarks/differential_expression.txt'\n    container:\n        'docker://geertvangeest/deseq2:v1'\n    resources:\n        mem_gb = 1\n    threads: 2\n    script:\n        '../scripts/DESeq2.R'\n</code></pre>"},{"location":"course_material/day2/5_reproducibility_snakemake/#adapting-the-snakefile-and-running-the-rule_1","title":"Adapting the Snakefile and running the rule","text":"<p>Now, all that is left is to run the rule to create the DEG list.</p> <p>Exercise: Which command should you use to create the output? Is there anything else to do beforehand?</p> Answer <p>It turns out that we cannnot launch the workflow directly: we need to include the new rule file in the Snakefile and adapt the output of the <code>rule all</code>! Your Snakefile should now resemble this:</p> <pre><code>'''\nMain Snakefile of the RNAseq analysis workflow. This workflow can clean and\nmap reads, and perform Differential Expression Analyses.\n'''\n\n# Path of the config file\nconfigfile: 'config/config.yaml'\n\n# Rules to execute the workflow\ninclude: 'rules/read_mapping.smk'\ninclude: 'rules/analyses.smk'\n\n# Master rule that launches the workflow\nrule all:\n    '''\n    Dummy rule to automatically generate the required outputs.\n    '''\n    input:\n        'results/deg_list.tsv'\n</code></pre> <p>Finally, run the workflow with <code>snakemake --cores 1 -r -p --use-singularity</code>. You should see new Snakemake information messages: <pre><code>Rscript --vanilla /path/to/snakemake_rnaseq/.snakemake/scripts/tmpge97d_lz.DESeq2.R\nActivating singularity image /path/to/snakemake_rnaseq/.snakemake/singularity/8bfdbe93244feb95887ab5d33a705017.simg\nINFO:    squashfuse not found, will not be able to mount SIF\nINFO:    fuse2fs not found, will not be able to mount EXT3 filesystems\nINFO:    gocryptfs not found, will not be able to use gocryptfs\nINFO:    Converting SIF file to temporary sandbox...\nINFO:    Cleaning up image... \n</code></pre></p> <p>\u2013use-singularity</p> <p>Do not forget <code>--use-singularity</code> otherwise Snakemake will not pull the image!!!</p> <p>Hint</p> <p>If you want to see how a Snakemake-generated Dockerfile looks like, use: <code>snakemake --cores 1 --containerize &gt; Dockerfile</code></p> <p>Exercise: How many DEG were detected?</p> Answer <p>Have a look at the list that was just created: <code>cat results/deg_list.tsv</code>. 8 genes are differentially expressed!</p> <p>Exercise: If you had to re-run the entire workflow from scratch, what command would you use?</p> Answer <p>You would need to execute <code>snakemake --cores 4 -r -p --use-conda --use-singularity -F</code>. * <code>-F</code> is to force the execution of the entire workflow * Don\u2019t forget <code>--use-conda --use-singularity</code>! Otherwise, you will lack some software and packages and the workflow will crash!</p> <p>Exercise: Visualise the DAG of the entire workflow.</p> Answer <p>You should now be used to this. <code>snakemake --cores 1 -r -p -F --dag | dot -T png &gt; images/total_dag.png</code></p> <p>This is the DAG you should see:</p> <p>Congratulations, you are now able to create a Snakemake workflow and make it reproducible thanks to conda/mamba and Docker/Singularity! To make things even better, have a look at Snakemake\u2019s best practices!</p>"}]}